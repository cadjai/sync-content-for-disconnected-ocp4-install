apiVersion: v1
items:
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Microcks.io
      provider-url: ""
    name: microcks
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/microcks
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: microcks-operator.v0.2.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "microcks.github.io/v1alpha1",
                "kind": "MicrocksInstall",
                "metadata": {
                  "name": "my-microcksinstall"
                },
                "spec": {
                  "name": "my-microcksinstall",
                  "version": "0.7.1",
                  "microcks": {
                    "replicas": 2
                  },
                  "postman": {
                    "replicas": 2
                  },
                  "keycloak": {
                    "install": true,
                    "persistent": true,
                    "volumeSize": "1Gi"
                  },
                  "mongodb": {
                    "install": true,
                    "persistent": true,
                    "volumeSize": "2Gi"
                  }
                }
              },
              {
                "apiVersion": "microcks.github.io/v1alpha1",
                "kind": "MicrocksInstall",
                "metadata": {
                  "name": "my-microcksinstall-minikube"
                },
                "spec": {
                  "name": "my-microcksinstall-minikube",
                  "version": "0.7.1",
                  "microcks": {
                    "replicas": 2,
                    "url": "microcks.192.168.99.100.nip.io"
                  },
                  "postman": {
                    "replicas": 2
                  },
                  "keycloak": {
                    "install": true,
                    "persistent": true,
                    "volumeSize": "1Gi",
                    "url": "keycloak.192.168.99.100.nip.io"
                  },
                  "mongodb": {
                    "install": true,
                    "persistent": true,
                    "volumeSize": "2Gi"
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/microcks/microcks-ansible-operator:0.2.1
          createdAt: "2019-11-04T09:01:00Z"
          description: Open Source mocking and testing platform for API and microservices
          repository: https://github.com/microcks/microcks-ansible-operator
          support: microcks.github.io
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Microcks installation
            displayName: MicrocksInstall
            kind: MicrocksInstall
            name: microcksinstalls.microcks.github.io
            version: v1alpha1
        description: |
          Microcks is an open source project those goal is to provide a platform for referencing, deploying mocks and allow contract-testing of Services and APIs. It can also be considered as a Service Virtualization solution because it will allow you to provide fake API or Service implementation before development being actually done. It supports both REST API and SOAP WebServices and perfectly integrates into an iterative,contract-first delivery process.
          Microcks tries not reinventing the wheel and let you capitalize on already known tools of standards. It supports OpenAPI Specification 3.x out-of-the-box and can also relies on popular tools like [SoapUI](http://www.soapui.com) or [Postman](http://www.getpostman.io) to edit your API request / response samples as well as your test scripts.
          * **Create/Destroy**: Easily launch Microcks installations for your kubernetes namespace.
        displayName: Microcks Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Microcks.io
        version: 0.2.1
      name: alpha
    defaultChannel: alpha
    packageName: microcks
    provider:
      name: Microcks.io
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: IBM
      provider-url: ""
    name: esindex-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/esindex-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: esindex-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"kind": "EsIndex", "spec": {"bindingFrom": {"name": "myes-binding"},
            "numberOfShards": 2, "numberOfReplicas": 1, "indexName": "myindex"}, "apiVersion":
            "ibmcloud.ibm.com/v1alpha1", "metadata": {"name": "myindex"}}]'
          capabilities: Basic Install
          categories: Cloud Provider
          certified: "false"
          containerImage: cloudoperators/esindex-operator:0.1.0
          createdAt: "2019-08-07T11:11:07Z"
          description: An operator for managing Elastic Search indices
          repository: https://github.com/IBM/esindex-operator
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an Elasticsearch index
            displayName: EsIndex
            kind: EsIndex
            name: esindices.ibmcloud.ibm.com
            version: v1alpha1
        description: "The IBM Cloud Operator for Elastic Search Indices, as part of
          IBM Cloud operators, provides  a Kubernetes CRD-Based API to manage the
          lifecycle of Elastic Search indices. It allows to  provision elasticsearch
          indices from your Kubernetes cluster, using the EsIndex CRD. \nThe Elastic
          Search access credentials can be specified in requests via reference to
          a Binding,  a Secret, or a ConfigMap resource. The Binding resource is managed
          by IBM Cloud Binding Operator  in conjuction with IBM Cloud Service Operator.
          Details can be found at https://github.com/IBM/cloud-operators.  \nThe IBM
          Cloud Operator for Elasticsearch Indices is currently in preview. It will
          get updated as we release new versions of the [upstream repository](https://github.com/IBM/esindex-operator).\n##
          Features\n* **Creation and Deletion** - Creates, deletes and monitors indices
          on Elastic Search service.\n* **Credential Source Options** - Elasticsearch
          access credentials can be provided by refrence  to a IBMCloud Binding, a
          Secret, or a ConfigMap resource.\n## Requirements\nThe operator can be installed
          on any OLM-enabled Kubernetes cluster with version >= 1.11. \nIf using IBMCloud
          Binding for access credentialï¼Œit requires the [IBM Cloud Operator](https://operatorhub.io/operator/ibmcloud-operator)
          \ installed on your cluster.\n## Using the IBM Cloud Operator\nYou can create
          an instance of an IBM Elastic Search Index using the following custom resources:\n\n
          \   apiVersion: ibmcloud.ibm.com/v1alpha1\n    kind: EsIndex\n    metadata:\n
          \     name: myindex\n    spec:\n      bindingFrom: \n        name: <BINDING>\n
          \     indexName: myindex\n      numberOfShards: 2\n      numberOfReplicas:
          1\n\nwhere `<BINDING>` is the binding for an instance of an IBM Elasticsearch
          service created and bound with the IBM Cloud Operator.\n\n    apiVersion:
          ibmcloud.ibm.com/v1alpha1\n    kind: EsIndex\n    metadata:\n      name:
          myindex\n    spec:\n      esURIComposed:\n        secretKeyRef: \n          name:
          <SECRET>\n          key: <KEY> \n      indexName: myindex\n      numberOfShards:
          2\n      numberOfReplicas: 1\n\nwhere `<SECRET>` is the name of a secret
          that holds the elasticesearch credentrials, `<KEY>` is the key for the value
          of composed elasticsearch URI.\nFor additional configuration options, samples
          and more information on using the operator, consult  the [IBM Cloud Operator
          for Elastic Search Indices documentation](https://github.com/IBM/esindex-operator).\n"
        displayName: IBM Cloud Operator for Elasticsearch Indices
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: IBM
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: esindex-operator
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: special-resource-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/special-resource-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: special-resource-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"sro.openshift.io/v1alpha1","kind":"SpecialResource","metadata":{"name":"nvidia-gpu"},"spec":{"size":3},
            "status":{"state": "N/A"}}]'
          capabilities: Seamless Upgrades
          categories: AI/Machine Learning
          certified: "false"
          containerImage: ""
          createdAt: ""
          description: 'The special resource operator is an orchestrator for resources
            in a cluster specifically designed to enable hardware accelerators that
            need extra management. '
          repository: ""
          support: ""
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Special Resource
            displayName: SpecialResource
            kind: SpecialResource
            name: specialresources.sro.openshift.io
            version: v1alpha1
          required:
          - description: Discovers special resources and labels the nodes with the
              PCI vendor ID.
            displayName: Node Feature Discovery
            kind: NodeFeatureDiscovery
            name: nodefeaturediscoveries.nfd.openshift.io
            version: v1alpha1
        description: "## About the managed application\nThe SRO will deploy the complete
          stack for enabling the accelerator in a cluster. \nIt manages the drivers,
          device-plugin, node-exporter, grafana and feature discovery.\n## About this
          Operator\n README: [https://github.com/openshift-psap/special-resource-operator](https://github.com/openshift-psap/special-resource-operator)\n##
          Prerequisites for enabling this Operator\nThe SRO has a dependency on NFD
          (Node Feature Discovery)\n"
        displayName: Special Resource Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: special-resource-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat, Inc.
      provider-url: ""
    name: codeready-toolchain-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/codeready-toolchain-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: codeready-toolchain-operator.v0.1.2
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "toolchain.openshift.dev/v1alpha1",
                "kind": "CheInstallation",
                "metadata": {
                  "name": "toolchain-cheinstallation"
                },
                "spec": {
                  "cheOperatorSpec": {
                    "namespace": "toolchain-workspaces"
                  }
                },
                "status": {
                  "conditions": [
                    {
                      "lastTransitionTime": "2019-12-16T09:37:11Z",
                      "reason": "Installed",
                      "status": "True",
                      "type": "CheReady"
                    }
                  ]
                }
              },
              {
                "apiVersion": "toolchain.openshift.dev/v1alpha1",
                "kind": "TektonInstallation",
                "metadata": {
                  "name": "tekton-installation"
                },
                "spec": {},
                "status": {
                  "conditions": [
                    {
                      "lastTransitionTime": "2019-12-16T09:20:19Z",
                      "reason": "Installed",
                      "status": "True",
                      "type": "TektonReady"
                    }
                  ]
                }
              }
            ]
          capabilities: Basic Install
          categories: Developer Tools, Integration & Delivery, OpenShift Optional
          certified: "false"
          containerImage: quay.io/codeready-toolchain/toolchain-operator@sha256:efcc3423ae9c4c8de729ec7455ab5e72defaf17b9c423c6f3141794110571fd6
          createdAt: "2020-02-12T13:50:15Z"
          description: CodeReady Toolchain is a suite of dev tools and runtimes for
            development and deployment of cloud-native applications on OpenShift.
            CodeReady Toolchain provides an easy way to deploy and configure the set
            of Red Hat curated developer tools and runtimes to the OpenShift cluster.
            The developer tools can be accessed from the Dev Perspective of the OpenShift
            console.
          repository: https://github.com/codeready-toolchain/toolchain-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: CheInstallation defines how CodeReady Workspaces (Che) operator
              should be installed
            displayName: CodeReady Workspaces Installation
            kind: CheInstallation
            name: cheinstallations.toolchain.openshift.dev
            version: v1alpha1
          - description: TektonInstallation defines how OpenShift Pipelines (Tekton)
              operator should be installed
            displayName: OpenShift Pipelines Installation
            kind: TektonInstallation
            name: tektoninstallations.toolchain.openshift.dev
            version: v1alpha1
        description: |
          # CodeReady Toolchain
          CodeReady Toolchain is a suite of dev tools and runtimes for development and deployment of cloud-native applications on OpenShift. CodeReady Toolchain provides an easy way to deploy and configure the set of Red Hat curated developer tools and runtimes to the OpenShift cluster. The developer tools can be accessed from the Dev Perspective of the OpenShift console.


          ## Bundled Developer Tools
          CodeReady Toolchain automatically installs and configures the following developer tool operators:
          * Red Hat CodeReady Workspaces
          * OpenShift Pipelines


          ## Installation
          CodeReady Toolchain Operator is a cluster-wide operator. It installs and configures the operators for the bundled developer tools to be available across all namespaces in the cluster. CodeReady Toolchain Operator and OpenShift Pipelines get installed in default `openshift-operators` namespace and Red Hat CodeReady Workspaces in single `toolchain-workspaces` namespace.


          ### Installation Pre-requisites
          * OAuth-based authentication has been enabled and configured for the OpenShift cluster.
          * One regular user (not the default kubeadmin) exists in the OpenShift cluster.

          You can find information about configuring the cluster OAuth in [OpenShift Container Platform documentation](https://docs.openshift.com/container-platform/4.2/authentication/understanding-identity-provider.html).


          ##### _**Warning**: This operator should be installed in OpenShift Container Platform clusters only. Installing on OKD or other platforms will result in errors._
        displayName: Red Hat CodeReady Toolchain
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 0.1.2
      name: alpha
    defaultChannel: alpha
    packageName: codeready-toolchain-operator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: kogito-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kogito-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kogito-operator.v0.7.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "app.kiegroup.org/v1alpha1",
                "kind": "KogitoJobsService",
                "metadata": {
                  "name": "jobs-service"
                },
                "spec": {
                  "replicas": 1
                }
              },
              {
                "apiVersion": "app.kiegroup.org/v1alpha1",
                "kind": "KogitoApp",
                "metadata": {
                  "name": "example-quarkus"
                },
                "spec": {
                  "build": {
                    "gitSource": {
                      "contextDir": "jbpm-quarkus-example",
                      "uri": "https://github.com/kiegroup/kogito-examples"
                    }
                  }
                }
              },
              {
                "apiVersion": "app.kiegroup.org/v1alpha1",
                "kind": "KogitoDataIndex",
                "metadata": {
                  "name": "kogito-data-index"
                },
                "spec": {
                  "cpuLimit": "",
                  "cpuRequest": "",
                  "image": "quay.io/kiegroup/kogito-data-index:latest",
                  "infinispan": {
                    "useKogitoInfra": true
                  },
                  "kafka": {
                    "useKogitoInfra": true
                  },
                  "memoryLimit": "",
                  "memoryRequest": "",
                  "replicas": 1
                }
              },
              {
                "apiVersion": "app.kiegroup.org/v1alpha1",
                "kind": "KogitoInfra",
                "metadata": {
                  "name": "infra"
                },
                "spec": {
                  "installInfinispan": false,
                  "installKafka": false,
                  "installKeycloak": false
                },
                "status": {}
              }
            ]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/kiegroup/kogito-cloud-operator:0.7.0
          createdAt: "2019-08-22T13:12:22Z"
          description: Kogito Operator for deployment and management of Kogito Services.
          repository: https://github.com/kiegroup/kogito-cloud-operator
          support: Red Hat
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A project prescription running a Kogito Runtime Service.
            displayName: Kogito Service
            kind: KogitoApp
            name: kogitoapps.app.kiegroup.org
            version: v1alpha1
          - description: The Kogito Data Index Service infrastructure deployment
            displayName: Kogito Data Index
            kind: KogitoDataIndex
            name: kogitodataindices.app.kiegroup.org
            version: v1alpha1
          - description: Will be managed automatically by the operator, don't need
              to create it manually. Kogito Infra is responsible to delegate the creation
              of each infrastructure dependency (such as Infinispan) to a third party
              operator. It holds the deployment status of each infrastructure dependency
              and custom resources needed to run Kogito Runtime and Kogito Data Index
              services.
            displayName: Kogito Infra
            kind: KogitoInfra
            name: kogitoinfras.app.kiegroup.org
            version: v1alpha1
          - description: The Kogito Jobs Service infrastructure deployment
            displayName: Kogito Jobs Service
            kind: KogitoJobsService
            name: kogitojobsservices.app.kiegroup.org
            version: v1alpha1
          required:
          - description: Represents a Infinispan cluster used internally by Kogito
              Data Index Service
            displayName: Infinispan Cluster
            kind: Infinispan
            name: infinispans.infinispan.org
            version: v1
          - description: Represents a Kafka cluster
            displayName: Kafka
            kind: Kafka
            name: kafkas.kafka.strimzi.io
            version: v1beta1
          - description: Represents a topic inside a Kafka cluster
            displayName: Kafka Topic
            kind: KafkaTopic
            name: kafkatopics.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Keycloak server to provide SSO for Kogito Services
            displayName: Keycloak
            kind: Keycloak
            name: keycloaks.keycloak.org
            version: v1alpha1
        description: |-
          Kogito Operator is designed to deploy Kogito Runtimes Services from source and every piece of infrastructure that the services might need:

          * Creates two kinds of build configuration: Native or JVM. For more information refer to [Native X JVM Builds](https://github.com/kiegroup/kogito-cloud-operator#native-x-jvm-builds) on our docs
          * Builds a Kogito Runtime Service from a remote git repository and creates a custom image for the given runtime: Quarkus or Springboot. [See more](https://github.com/kiegroup/kogito-cloud-operator#kogito-runtimes-service-deployment).
          * Installs the [Kogito Data Index Service](https://github.com/kiegroup/kogito-runtimes/wiki/Data-Index-Service) to enable data index feature for Kogito Runtimes. [See more](https://github.com/kiegroup/kogito-cloud-operator#kogito-data-index-service-installation).
          * Installs the [Kogito Jobs Service](https://github.com/kiegroup/kogito-runtimes/wiki/Jobs-Service) to enable jobs scheduling.

          **IMPORTANT!** Kogito Operator depends on Infinispan Operator and Strimzi to implement persistence and messaging use cases. If you need these features, bear in mind that those operators must be installed in the same namespace as the Kogito Operator.
        displayName: Kogito Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.7.0
      name: alpha
    - currentCSV: kogito-operator.v0.7.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "app.kiegroup.org/v1alpha1",
                "kind": "KogitoJobsService",
                "metadata": {
                  "name": "jobs-service"
                },
                "spec": {
                  "replicas": 1
                }
              },
              {
                "apiVersion": "app.kiegroup.org/v1alpha1",
                "kind": "KogitoApp",
                "metadata": {
                  "name": "example-quarkus"
                },
                "spec": {
                  "build": {
                    "gitSource": {
                      "contextDir": "jbpm-quarkus-example",
                      "uri": "https://github.com/kiegroup/kogito-examples"
                    }
                  }
                }
              },
              {
                "apiVersion": "app.kiegroup.org/v1alpha1",
                "kind": "KogitoDataIndex",
                "metadata": {
                  "name": "kogito-data-index"
                },
                "spec": {
                  "cpuLimit": "",
                  "cpuRequest": "",
                  "image": "quay.io/kiegroup/kogito-data-index:latest",
                  "infinispan": {
                    "useKogitoInfra": true
                  },
                  "kafka": {
                    "useKogitoInfra": true
                  },
                  "memoryLimit": "",
                  "memoryRequest": "",
                  "replicas": 1
                }
              },
              {
                "apiVersion": "app.kiegroup.org/v1alpha1",
                "kind": "KogitoInfra",
                "metadata": {
                  "name": "infra"
                },
                "spec": {
                  "installInfinispan": false,
                  "installKafka": false,
                  "installKeycloak": false
                },
                "status": {}
              }
            ]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/kiegroup/kogito-cloud-operator:0.7.0
          createdAt: "2019-08-22T13:12:22Z"
          description: Kogito Operator for deployment and management of Kogito Services.
          repository: https://github.com/kiegroup/kogito-cloud-operator
          support: Red Hat
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A project prescription running a Kogito Runtime Service.
            displayName: Kogito Service
            kind: KogitoApp
            name: kogitoapps.app.kiegroup.org
            version: v1alpha1
          - description: The Kogito Data Index Service infrastructure deployment
            displayName: Kogito Data Index
            kind: KogitoDataIndex
            name: kogitodataindices.app.kiegroup.org
            version: v1alpha1
          - description: Will be managed automatically by the operator, don't need
              to create it manually. Kogito Infra is responsible to delegate the creation
              of each infrastructure dependency (such as Infinispan) to a third party
              operator. It holds the deployment status of each infrastructure dependency
              and custom resources needed to run Kogito Runtime and Kogito Data Index
              services.
            displayName: Kogito Infra
            kind: KogitoInfra
            name: kogitoinfras.app.kiegroup.org
            version: v1alpha1
          - description: The Kogito Jobs Service infrastructure deployment
            displayName: Kogito Jobs Service
            kind: KogitoJobsService
            name: kogitojobsservices.app.kiegroup.org
            version: v1alpha1
          required:
          - description: Represents a Infinispan cluster used internally by Kogito
              Data Index Service
            displayName: Infinispan Cluster
            kind: Infinispan
            name: infinispans.infinispan.org
            version: v1
          - description: Represents a Kafka cluster
            displayName: Kafka
            kind: Kafka
            name: kafkas.kafka.strimzi.io
            version: v1beta1
          - description: Represents a topic inside a Kafka cluster
            displayName: Kafka Topic
            kind: KafkaTopic
            name: kafkatopics.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Keycloak server to provide SSO for Kogito Services
            displayName: Keycloak
            kind: Keycloak
            name: keycloaks.keycloak.org
            version: v1alpha1
        description: |-
          Kogito Operator is designed to deploy Kogito Runtimes Services from source and every piece of infrastructure that the services might need:

          * Creates two kinds of build configuration: Native or JVM. For more information refer to [Native X JVM Builds](https://github.com/kiegroup/kogito-cloud-operator#native-x-jvm-builds) on our docs
          * Builds a Kogito Runtime Service from a remote git repository and creates a custom image for the given runtime: Quarkus or Springboot. [See more](https://github.com/kiegroup/kogito-cloud-operator#kogito-runtimes-service-deployment).
          * Installs the [Kogito Data Index Service](https://github.com/kiegroup/kogito-runtimes/wiki/Data-Index-Service) to enable data index feature for Kogito Runtimes. [See more](https://github.com/kiegroup/kogito-cloud-operator#kogito-data-index-service-installation).
          * Installs the [Kogito Jobs Service](https://github.com/kiegroup/kogito-runtimes/wiki/Jobs-Service) to enable jobs scheduling.

          **IMPORTANT!** Kogito Operator depends on Infinispan Operator and Strimzi to implement persistence and messaging use cases. If you need these features, bear in mind that those operators must be installed in the same namespace as the Kogito Operator.
        displayName: Kogito Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.7.0
      name: dev-preview
    defaultChannel: alpha
    packageName: kogito-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: ptp-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ptp-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ptp-operator.4.3.2-202002112006
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "ptp.openshift.io/v1",
                "kind": "PtpConfig",
                "metadata": {
                  "name": "ptpconfig-1",
                  "namespace": "openshift-ptp"
                },
                "spec": {
                  "profile": [{
                      "name": "ptpconfig-profile-1",
                      "interface": "eth0",
                      "ptp4lOpts": "-s -2",
                      "phc2sysOpts": "-a -r"
                    }
                  ],
                  "recommend": [{
                      "profile": "ptpconfig-profile-1",
                      "priority": 50,
                      "match": [{
                          "nodeLabel": "node-role.kubernetes.io/worker=",
                          "nodeName": "kubernetes.io/hostname=node.example.com"
                        }
                      ]
                    }
                  ]
                }
              },
              {
                "apiVersion": "ptp.openshift.io/v1",
                "kind": "NodePtpDevice",
                "metadata": {
                  "name": "node.example.com",
                  "namespace": "openshift-ptp"
                },
                "spec": {}
              },
              {
                "apiVersion": "ptp.openshift.io/v1",
                "kind": "PtpOperatorConfig",
                "metadata": {
                  "name": "default",
                  "namespace": "openshift-ptp"
                },
                "spec": {
                  "daemonNodeSelector": {}
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-ptp-operator@sha256:dd1f0fa4df20a8c0e02c30a51904990e29944104a8cf207a4951ca82874b8614
          createdAt: "2019-10-14"
          description: This software enables configuration of Precision Time Protocol(PTP)
            on Kubernetes. It detects hardware capable PTP devices on each node, and
            configures linuxptp processes such as ptp4l, phc2sys and timemaster.
          olm.skipRange: '>=4.3.0-0 <4.3.2-202002112006'
          provider: Red Hat
          repository: https://github.com/openshift/ptp-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: |
              Represents PTP Configuration that will be applied to linuxptp process based on node name or label
            displayName: PTP Configuration
            kind: PtpConfig
            name: ptpconfigs.ptp.openshift.io
            version: v1
          - description: |
              Represents Configuration for Operator itself
            displayName: Operator Configuration
            kind: PtpOperatorConfig
            name: ptpoperatorconfigs.ptp.openshift.io
            version: v1
          - description: |
              Represents PTP capable devices on node
            displayName: Node PTP Devices
            kind: NodePtpDevice
            name: nodeptpdevices.ptp.openshift.io
            version: v1
        description: |
          # Precision Time Protocol (PTP) Operator for Openshift

          ## Introdution
          PTP Operator manages cluster wide PTP configuration.
          This operator has to run in namespace 'openshift-ptp'. An Operator Group is also required to install this operator:
          ```
          $ oc create -f - <<EOF
          apiVersion: v1
          kind: Namespace
          metadata:
            name: openshift-ptp
            labels:
              name: openshift-ptp
              openshift.io/run-level: "1"
          EOF

          $ oc create -f - <<EOF
          apiVersion: operators.coreos.com/v1
          kind: OperatorGroup
          metadata:
            name: ptp-operators
            namespace: openshift-ptp
          spec:
            targetNamespaces:
            - openshift-ptp
          EOF
          ```
        displayName: PTP Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.3.0
      name: "4.3"
    defaultChannel: "4.3"
    packageName: ptp-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: amq7-cert-manager
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/amq7-cert-manager
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: amq7-cert-manager.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"certmanager.k8s.io/v1alpha1","kind":"Certificate","metadata":{"name":"example-com"},"spec":{"secretName":"example-com-tls","issuerRef":{"name":"ca-issuer","kind":"Issuer"},"commonName":"example.com","organization":["Example
            CA"],"dnsNames":["example.com","www.example.com"]}},{"apiVersion": "certmanager.k8s.io/v1alpha1","kind":
            "Issuer", "metadata":{"name":"ca-issuer"},"spec":{"ca":{"secretName":
            "my-selfsigned-cert"}}},{"apiVersion": "certmanager.k8s.io/v1alpha1","kind":
            "ClusterIssuer", "metadata":{"name":"ca-issuer"},"spec":{"ca":{"secretName":
            "my-selfsigned-cert"}}},{"apiVersion": "certmanager.k8s.io/v1alpha1","kind":
            "Order", "metadata":{"name":"ignore-me"},"csr":"ignore-me","issuerRef":"ignore-me"},{"apiVersion":
            "certmanager.k8s.io/v1alpha1","kind": "Challenge", "metadata":{"name":"ignore-me"},"spec":{"issuerRef":"ignore-me"}}]'
          capabilities: Basic Install
          categories: Networking, Security
          certified: "false"
          containerImage: registry.redhat.io/amq7/amq-cert-manager:1.0
          createdAt: "2019-06-26 22:00:00"
          description: Certificate Creation and Update
          repository: https://github.com/jetstack/cert-manager
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A declaration of a required Certificate
            displayName: Certificate
            kind: Certificate
            name: certificates.certmanager.k8s.io
            version: v1alpha1
          - description: A declaration of a namespace specific issuer
            displayName: Issuer
            kind: Issuer
            name: issuers.certmanager.k8s.io
            version: v1alpha1
          - description: A declaration of a cluster-wide issuer
            displayName: ClusterIssuer
            kind: ClusterIssuer
            name: clusterissuers.certmanager.k8s.io
            version: v1alpha1
          - description: Not yet supported
            displayName: Order
            kind: Order
            name: orders.certmanager.k8s.io
            version: v1alpha1
          - description: Not yet supported
            displayName: Challenge
            kind: Challenge
            name: challenges.certmanager.k8s.io
            version: v1alpha1
        description: |
          cert-manager is a Kubernetes add-on to automate the management and issuance of
          TLS certificates from various issuing sources.

          It will ensure certificates are valid and up to date periodically, and attempt
          to renew certificates at an appropriate time before expiry.
        displayName: AMQ Certificate Manager
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: amq7-cert-manager
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: container-security-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/container-security-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: container-security-operator.v1.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "secscan.quay.redhat.com/v1alpha1",
                "kind": "ImageManifestVuln",
                "metadata": {
                  "name": "example"
                },
                "spec": {}
              }
            ]
          capabilities: Full Lifecycle
          categories: Security
          containerImage: quay.io/quay/container-security-operator@sha256:15a4b50d847512b5f404ec1cf72c30c98e073a7f26f1588213bd2e8b6331f016
          createdAt: "2019-11-16 01:03:00"
          description: Identify image vulnerabilities in Kubernetes pods
          repository: https://github.com/quay/container-security-operator
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a set of vulnerabilities in an image manifest.
            displayName: Image Manifest Vulnerability
            kind: ImageManifestVuln
            name: imagemanifestvulns.secscan.quay.redhat.com
            version: v1alpha1
        description: The Container Security Operator (CSO) brings Quay and Clair metadata
          to Kubernetes / OpenShift. Starting with vulnerability information the scope
          will get expanded over time. If it runs on OpenShift, the corresponding
          vulnerability information is shown inside the OCP Console. The Container
          Security Operator enables cluster administrators to monitor known container
          image vulnerabilities in pods running on their Kubernetes cluster. The controller
          sets up a watch on pods in the specified namespace(s) and queries the container
          registry for vulnerability information. If the container registry supports
          image scanning, such as [Quay](https://github.com/quay/quay) with [Clair](https://github.com/quay/clair),
          then the Operator will expose any vulnerabilities found via the Kubernetes
          API in an `ImageManifestVuln` object.  This Operator requires no additional
          configuration after deployment, and will begin watching pods and populating
          `ImageManifestVulns` immediately once installed.
        displayName: Container Security
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.0.1
      name: alpha
    defaultChannel: alpha
    packageName: container-security-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Lightbend, Inc.
      provider-url: ""
    name: akka-cluster-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/akka-cluster-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: akka-cluster-operator-certified.v0.2.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "app.lightbend.com/v1alpha1",
                "kind": "AkkaCluster",
                "metadata": {
                  "name": "akka-cluster-demo"
                },
                "spec": {
                  "replicas": 3,
                  "template": {
                    "spec": {
                      "containers": [
                        {
                          "name": "main",
                          "image": "lightbend-docker-registry.bintray.io/lightbend/akka-cluster-demo:1.0.2",
                          "readinessProbe": {
                            "httpGet": {
                              "path": "/ready",
                              "port": "management"
                            },
                            "periodSeconds": 10,
                            "failureThreshold": 10,
                            "initialDelaySeconds": 20
                          },
                          "livenessProbe": {
                            "httpGet": {
                              "path": "/alive",
                              "port": "management"
                            },
                            "periodSeconds": 10,
                            "failureThreshold": 10,
                            "initialDelaySeconds": 20
                          },
                          "ports": [
                            {
                              "name": "http",
                              "containerPort": 8080
                            },
                            {
                              "name": "remoting",
                              "containerPort": 2552
                            },
                            {
                              "name": "management",
                              "containerPort": 8558
                            }
                          ]
                        }
                      ]
                    }
                  }
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Application Runtime
          certified: "false"
          containerImage: registry.connect.redhat.com/lightbend/akka-cluster-operator-certified:0.2.0
          createdAt: "2019-06-28T15:23:00Z"
          description: Run Akka Cluster applications on Kubernetes.
          repository: https://github.com/lightbend/akka-cluster-operator
          support: Lightbend, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An example Akka Cluster app that provides cluster visualization.
            displayName: Akka Cluster
            kind: AkkaCluster
            name: akkaclusters.app.lightbend.com
            version: v1alpha1
        description: |
          The Akka Cluster Operator allows you to manage applications designed for
          [Akka Cluster](https://doc.akka.io/docs/akka/current/common/cluster.html).
          Clustering with [Akka](https://doc.akka.io/docs/akka/current/guide/introduction.html) provides a
          fault-tolerant, decentralized, peer-to-peer based cluster
          for building stateful, distributed applications with no single point of failure.
          Developers should use Akka Management v1.x or newer, with both Bootstrap and HTTP modules enabled.
          When deploying using the Akka Cluster Operator, only the `management port` needs to be defined.
          Defaults are provided by the Operator for all other required configuration.
          The Akka Cluster Operator provides scalability control and membership status information
          for deployed applications using Akka Cluster. As part of supervising membership of running clusters,
          this Operator creates a pod-listing ServiceAccount, Role, and RoleBinding suitable for
          each application. See the project [Readme](https://github.com/lightbend/akka-cluster-operator/blob/master/README.md)
          for more information and details.
          This is an incubating project in alpha version.
        displayName: Akka Cluster Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Lightbend, Inc.
        version: 0.2.0
      name: alpha
    defaultChannel: alpha
    packageName: akka-cluster-operator-certified
    provider:
      name: Lightbend, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: amq-streams
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/amq-streams
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: amqstreams.v1.3.0
      currentCSVDesc:
        annotations:
          alm-examples: "[  \n  {  \n      \"apiVersion\":\"kafka.strimzi.io/v1beta1\",\n
            \     \"kind\":\"Kafka\",\n      \"metadata\":{  \n        \"name\":\"my-cluster\"\n
            \     },\n      \"spec\":{  \n        \"kafka\":{  \n            \"version\":\"2.3.0\",\n
            \           \"replicas\":3,\n            \"listeners\":{  \n              \"plain\":{
            \ \n\n              },\n              \"tls\":{  \n\n              }\n
            \           },\n            \"config\":{  \n              \"offsets.topic.replication.factor\":3,\n
            \             \"transaction.state.log.replication.factor\":3,\n              \"transaction.state.log.min.isr\":2,\n
            \             \"log.message.format.version\":\"2.3\"\n            },\n
            \           \"storage\":{  \n              \"type\":\"ephemeral\"\n            }\n
            \       },\n        \"zookeeper\":{  \n            \"replicas\":3,\n            \"storage\":{
            \ \n              \"type\":\"ephemeral\"\n            }\n        },\n
            \       \"entityOperator\":{  \n            \"topicOperator\":{  \n\n
            \           },\n            \"userOperator\":{  \n\n            }\n        }\n
            \     }\n  },\n  {  \n      \"apiVersion\":\"kafka.strimzi.io/v1beta1\",\n
            \     \"kind\":\"KafkaConnect\",\n      \"metadata\":{  \n        \"name\":\"my-connect-cluster\"\n
            \     },\n      \"spec\":{  \n        \"version\":\"2.3.0\",\n        \"replicas\":1,\n
            \       \"bootstrapServers\":\"my-cluster-kafka-bootstrap:9093\",\n        \"tls\":{
            \ \n            \"trustedCertificates\":[  \n              {  \n                  \"secretName\":\"my-cluster-cluster-ca-cert\",\n
            \                 \"certificate\":\"ca.crt\"\n              }\n            ]\n
            \       }\n      }\n  },\n  {  \n      \"apiVersion\":\"kafka.strimzi.io/v1beta1\",\n
            \     \"kind\":\"KafkaConnectS2I\",\n      \"metadata\":{  \n        \"name\":\"my-connect-cluster\"\n
            \     },\n      \"spec\":{  \n        \"version\":\"2.3.0\",\n        \"replicas\":1,\n
            \       \"bootstrapServers\":\"my-cluster-kafka-bootstrap:9093\",\n        \"tls\":{
            \ \n            \"trustedCertificates\":[  \n              {  \n                  \"secretName\":\"my-cluster-cluster-ca-cert\",\n
            \                 \"certificate\":\"ca.crt\"\n              }\n            ]\n
            \       }\n      }\n  },\n  {  \n      \"apiVersion\":\"kafka.strimzi.io/v1beta1\",\n
            \     \"kind\":\"KafkaMirrorMaker\",\n      \"metadata\":{  \n        \"name\":\"my-mirror-maker\"\n
            \     },\n      \"spec\":{  \n        \"version\":\"2.3.0\",\n        \"replicas\":1,\n
            \       \"consumer\":{  \n            \"bootstrapServers\":\"my-source-cluster-kafka-bootstrap:9092\",\n
            \           \"groupId\":\"my-source-group-id\"\n        },\n        \"producer\":{
            \ \n            \"bootstrapServers\":\"my-target-cluster-kafka-bootstrap:9092\"\n
            \       },\n        \"whitelist\":\".*\"\n      }\n  },\n  {  \n      \"apiVersion\":\"kafka.strimzi.io/v1alpha1\",\n
            \     \"kind\":\"KafkaBridge\",\n      \"metadata\":{  \n        \"name\":\"my-bridge\"\n
            \     },\n      \"spec\":{  \n        \"replicas\":1,\n        \"bootstrapServers\":\"my-cluster-kafka-bootstrap:9092\",\n
            \       \"http\":{  \n            \"port\":8080\n        }\n      }\n
            \ },\n  {  \n      \"apiVersion\":\"kafka.strimzi.io/v1beta1\",\n      \"kind\":\"KafkaTopic\",\n
            \     \"metadata\":{  \n        \"name\":\"my-topic\",\n        \"labels\":{
            \ \n            \"strimzi.io/cluster\":\"my-cluster\"\n        }\n      },\n
            \     \"spec\":{  \n        \"partitions\":10,\n        \"replicas\":3,\n
            \       \"config\":{  \n            \"retention.ms\":604800000,\n            \"segment.bytes\":1073741824\n
            \       }\n      }\n  },\n  {  \n      \"apiVersion\":\"kafka.strimzi.io/v1beta1\",\n
            \     \"kind\":\"KafkaUser\",\n      \"metadata\":{  \n        \"name\":\"my-user\",\n
            \       \"labels\":{  \n            \"strimzi.io/cluster\":\"my-cluster\"\n
            \       }\n      },\n      \"spec\":{  \n        \"authentication\":{
            \ \n            \"type\":\"tls\"\n        },\n        \"authorization\":{
            \ \n            \"type\":\"simple\",\n            \"acls\":[  \n              {
            \ \n                  \"resource\":{  \n                    \"type\":\"topic\",\n
            \                   \"name\":\"my-topic\",\n                    \"patternType\":\"literal\"\n
            \                 },\n                  \"operation\":\"Read\",\n                  \"host\":\"*\"\n
            \             },\n              {  \n                  \"resource\":{
            \ \n                    \"type\":\"topic\",\n                    \"name\":\"my-topic\",\n
            \                   \"patternType\":\"literal\"\n                  },\n
            \                 \"operation\":\"Describe\",\n                  \"host\":\"*\"\n
            \             },\n              {  \n                  \"resource\":{
            \ \n                    \"type\":\"group\",\n                    \"name\":\"my-group\",\n
            \                   \"patternType\":\"literal\"\n                  },\n
            \                 \"operation\":\"Read\",\n                  \"host\":\"*\"\n
            \             },\n              {  \n                  \"resource\":{
            \ \n                    \"type\":\"topic\",\n                    \"name\":\"my-topic\",\n
            \                   \"patternType\":\"literal\"\n                  },\n
            \                 \"operation\":\"Write\",\n                  \"host\":\"*\"\n
            \             },\n              {  \n                  \"resource\":{
            \ \n                    \"type\":\"topic\",\n                    \"name\":\"my-topic\",\n
            \                   \"patternType\":\"literal\"\n                  },\n
            \                 \"operation\":\"Create\",\n                  \"host\":\"*\"\n
            \             },\n              {  \n                  \"resource\":{
            \ \n                    \"type\":\"topic\",\n                    \"name\":\"my-topic\",\n
            \                   \"patternType\":\"literal\"\n                  },\n
            \                 \"operation\":\"Describe\",\n                  \"host\":\"*\"\n
            \             }\n            ]\n        }\n      }\n  }\n]"
          capabilities: Deep Insights
          categories: Streaming & Messaging
          certified: "false"
          containerImage: registry.redhat.io/amq7/amq-streams-operator:1.3.0
          createdAt: "2019-09-24T22:00:00Z"
          description: Red Hat AMQ Streams is a massively scalable, distributed, and
            high performance data streaming platform based on Apache KafkaÂ®.
          repository: https://github.com/strimzi/strimzi-kafka-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Kafka cluster
            displayName: Kafka
            kind: Kafka
            name: kafkas.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Kafka Connect cluster
            displayName: Kafka Connect
            kind: KafkaConnect
            name: kafkaconnects.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Kafka Connect cluster with Source 2 Image support
            displayName: Kafka Connect S2I
            kind: KafkaConnectS2I
            name: kafkaconnects2is.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Kafka MirrorMaker cluster
            displayName: Kafka MirrorMaker
            kind: KafkaMirrorMaker
            name: kafkamirrormakers.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Kafka Bridge cluster
            displayName: Kafka Bridge
            kind: KafkaBridge
            name: kafkabridges.kafka.strimzi.io
            version: v1alpha1
          - description: Represents a topic inside a Kafka cluster
            displayName: Kafka Topic
            kind: KafkaTopic
            name: kafkatopics.kafka.strimzi.io
            version: v1beta1
          - description: Represents a user inside a Kafka cluster
            displayName: Kafka User
            kind: KafkaUser
            name: kafkausers.kafka.strimzi.io
            version: v1beta1
        description: "**Red Hat AMQ Streams** is a massively scalable, distributed,
          and high performance data streaming platform based on the Apache KafkaÂ®
          project. \nAMQ Streams provides an event streaming backbone that allows
          microservices and other application components to exchange data with extremely
          high throughput and low latency. \n\n**The core capabilities include:**\n\n*
          A pub/sub messaging model, similar to a traditional enterprise messaging
          system, in which application components publish and consume events to/from
          an ordered stream\n\n* The long term, fault-tolerant storage of events\n\n*
          The ability for a consumer to replay streams of events\n\n* The ability
          to partition topics for horizontal scalability\n\nRed Hat AMQ Streams provides
          a way to run an [Apache Kafka](https://kafka.apache.org) cluster on [OpenShift](https://www.openshift.com/)
          in various deployment configurations.\nSee the Red Hat AMQ [website](https://access.redhat.com/products/red-hat-amq)
          for more details about the project.\n\n### What is new in AMQ Streams 1.3.0\n\n*
          Authentication using OAUTHBEARER SASL mechanism\n\n* Built in support for
          Kafka Exporter for consumer lag monitoring\n\n* Support for tracing using
          OpenTracing and Jaeger projects\n\n* Status fields for all custom resources\n\n###
          Supported Features\n\n* **Manages the Kafka Cluster** - Deploys and manages
          all of the components of this complex application, including dependencies
          like Apache ZooKeeperÂ® that are traditionally hard to administer.\n\n* **Includes
          Kafka Connect** - Allows for configuration of common data sources and sinks
          to move data into and out of the Kafka cluster.\n\n* **Topic Management**
          - Creates and manages Kafka Topics within the cluster.\n\n* **User Management**
          - Creates and manages Kafka Users within the cluster.\n\n### Upgrading your
          Clusters\n\nThe AMQ Streams operator understands how to run and upgrade
          between a set of Kafka versions.\nWhen specifying a new version in your
          config, check to make sure you are not using any features that may have
          been removed.\nSee [the upgrade guide](https://access.redhat.com/documentation/en-us/red_hat_amq/7.3/html/using_amq_streams_on_openshift_container_platform/index)
          for more information.\n\n### Security\n\nAMQ Streams supports TLS encryption,
          authentication, and authorization out of the box.\nSupported authentication
          methods are TLS client certificates, SCRAM-SHA mechanism based on username
          and password, and OAuth authentication.\n\n### Monitoring\n\nAMQ Streams
          has support for Prometheus metrics.\nEnable metrics in the custom resources
          and configure your Prometheus server to scrape the metrics from all Kafka
          and Zookeeper pods.\nWith the integrated Kafka Exporter you have now also
          advanced metrics about topics, consumer groups and consumer group lag.\n\n###
          Storage\n\nAn efficient data storage infrastructure is essential to the
          optimal performance of AMQ Streams.\nAMQ Streams requires block storage.\nThe
          use of file storage (for example, NFS) is not recommended.\n\nAMQ Streams
          supports three types of data storage:\n\n* Ephemeral (Recommended for development
          only)\n\n* Persistent\n\n* JBOD (Just a Bunch of Disks, suitable for Kafka
          only. Not supported in Zookeeper.)\n\nAMQ Streams also supports advanced
          operations such as adding or removing disks in Kafka brokers or resizing
          the persistent volumes (where supported by the infrastructure).\n\n### Documentation\n\nDocumentation
          for the current release can be found on Red Hat [website](https://access.redhat.com/documentation/en-us/red_hat_amq/7.3/html/using_amq_streams_on_openshift_container_platform/index).\n"
        displayName: AMQ Streams
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.3.0
      name: stable
    defaultChannel: stable
    packageName: amq-streams
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: PingCAP
      provider-url: ""
    name: tidb-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/tidb-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: tidb-operator.1.0.0-beta1
      currentCSVDesc:
        annotations:
          alm-examples: "[  \n  {\n    \"apiVersion\": \"pingcap.com/v1alpha1\",\n
            \   \"kind\": \"TidbCluster\",\n    \"metadata\": {\n      \"name\": \"demo\",\n
            \     \"namespace\": \"tidb1\"\n      },\n    \"spec\": {\n      \"pd\":
            {\n          \"image\": \"pingcap/pd:v2.1.0\",\n          \"imagePullPolicy\":
            \"IfNotPresent\",\n          \"limits\": {},\n          \"nodeSelectorRequired\":
            true,\n          \"replicas\": 3,\n          \"requests\": {\n            \"storage\":
            \"5Gi\"\n          },\n          \"storageClassName\": \"pd-ssd\"\n        },\n
            \     \"pvReclaimPolicy\": \"Retain\",\n      \"schedulerName\": \"tidb-scheduler\",\n
            \     \"services\": [\n          {\n          \"name\": \"pd\",\n          \"type\":
            \"ClusterIP\"\n          }\n        ],\n      \"tidb\": {\n        \"image\":
            \"gcr.io/dbaas-beta/tidb:v2.1.3.pre-invalid-dates\",\n        \"imagePullPolicy\":
            \"IfNotPresent\",\n        \"limits\": {},\n        \"maxFailoverCount\":
            3,\n        \"nodeSelectorRequired\": true,\n        \"replicas\": 2,\n
            \       \"requests\": {\n          \"storage\": \"1Gi\"\n        }\n      },\n
            \     \"tikv\":{\n          \"image\": \"pingcap/tikv:v2.1.0\",\n          \"imagePullPolicy\":
            \"IfNotPresent\",\n          \"limits\": {},\n          \"nodeSelectorRequired\":
            true,\n          \"replicas\": 3,\n          \"requests\":{\n            \"storage\":
            \"50Gi\"\n          },\n          \"storageClassName\": \"pd-ssd\"\n        },\n
            \     \"tikvPromGateway\": {\n        \"image\": \"prom/pushgateway:v0.3.1\",\n
            \       \"imagePullPolicy\": \"IfNotPresent\",\n        \"limits\": {},\n
            \       \"requests\": {}\n        },\n      \"timezone\": \"UTC\"\n      }\n
            \ }\n]"
          capabilities: Deep Insights
          categories: Database,OpenShift Optional
          certified: "false"
          containerImage: registry.connect.redhat.com/pingcap/tidb-operator-1:v1.0.0-beta.1-p2.2
          createdAt: "2019-02-06 04:11:39"
          description: TiDB is a popular NewSQL MySQL-compatible database. The TiDB
            Operator manages the full lifecycle of using TiDB in any Kubernetes-enabled
            cloud environment.
          repository: https://github.com/pingcap/tidb-operator
          support: PingCAP
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A TiDB Cluster
            displayName: TiDBCluster
            kind: TidbCluster
            name: tidbclusters.pingcap.com
            version: v1alpha1
        description: "\nThe TiDB Operator automatically deploys, operates, and manages
          a TiDB cluster in any Kubernetes-enabled cloud environment. \n\n[TiDB](https://github.com/pingcap/tidb)
          is a popular open-source NewSQL database that's MySQL compatible. It is
          currently deployed in-production in more than 300 companies, spanning industries
          from e-commerce and ride-sharing, to banking and food delivery. \n\n\nAdditional
          documentation can be found [here](https://github.com/pingcap/tidb-operator).\n\n###
          Supported Features\n\n* **Horizontal Scaling** - Safely scale up or down
          each component of TiDB.\n\n* **Rolling Update** - Gracefully perform rolling
          updates in order with no downtime.\n\n* **Multi-tenant Support** - Users
          can deploy and manage multiple TiDB clusters on a single Kubernetes cluster
          using TiDB Operator.\n\n* **Automatic Failover** - Operator performs failover
          automatically when node failures occur.\n\n* **Simple Monitoring** - Operator
          installs Prometheus and Grafana by default for cluster monitoring. \n\n\n**For
          proper setup, deployment, and troubleshooting instructions, please see [TiDB
          Operator User Guide](https://github.com/pingcap/tidb-operator/blob/master/docs/user-guide.md).**
          \ "
        displayName: TiDB Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: PingCAP
        version: 1.0.0-beta1
      name: beta
    defaultChannel: beta
    packageName: tidb-operator-certified
    provider:
      name: PingCAP
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: VMware
      provider-url: ""
    name: wavefront-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/wavefront-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: wavefront-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: "[\n  {\n    \"apiVersion\": \"wavefront.com/v1alpha1\",\n
            \   \"kind\": \"WavefrontProxy\",\n    \"metadata\": {\n      \"name\":
            \"example-wavefrontproxy\",\n      \"namespace\": \"wavefront\"\n    },\n
            \   \"spec\": {\n      \"token\": \"\\u003cTOKEN\\u003e\",\n      \"url\":
            \"\\u003chttps://YOUR_CLUSTER.wavefront.com/api/\\u003e\",\n      \"image\":
            \"registry.connect.redhat.com/wavefronthq/proxy:5.7\",\n      \"openshift\":
            true,\n      \"storageClaimName\": \"wavefront-proxy-storage\",\n      \"enableAutoUpgrade\":
            false\n    }\n  },\n  {\n    \"apiVersion\": \"wavefront.com/v1alpha1\",\n
            \   \"kind\": \"WavefrontCollector\",\n    \"metadata\": {\n      \"name\":
            \"wavefront-collector\",\n      \"namespace\": \"wavefront\"\n    },\n
            \   \"spec\": {\n      \"daemon\": true,\n      \"openshift\": true,\n
            \     \"useOpenshiftDefaultConfig\": true,\n      \"enableAutoUpgrade\":
            false, \n      \"env\": [\n        {\n          \"name\": \"HOST_PROC\",\n
            \         \"value\": \"/host/proc\"\n        },\n        {\n          \"name\":
            \"POD_NODE_NAME\",\n          \"valueFrom\": {\n            \"fieldRef\":
            {\n              \"apiVersion\": \"v1\",\n              \"fieldPath\":
            \"spec.nodeName\"\n            }\n          }\n        },\n        {\n
            \         \"name\": \"POD_NAMESPACE_NAME\",\n          \"valueFrom\":
            {\n            \"fieldRef\": {\n              \"apiVersion\": \"v1\",\n
            \             \"fieldPath\": \"metadata.namespace\"\n            }\n          }\n
            \       }\n      ],\n      \"image\": \"registry.connect.redhat.com/wavefronthq/wavefront-kubernetes-collector:1.0.3-1\",\n
            \     \"tolerations\": [\n        {\n          \"effect\": \"NoSchedule\",\n
            \         \"key\": \"node.alpha.kubernetes.io/role\",\n          \"operator\":
            \"Exists\"\n        },\n        {\n          \"effect\": \"NoSchedule\",\n
            \         \"key\": \"node-role.kubernetes.io/master\",\n          \"operator\":
            \"Exists\"\n        }\n      ]\n    }\n  }\n]"
          capabilities: Basic Install
          categories: Monitoring
          certified: "true"
          containerImage: registry.connect.redhat.com/wavefronthq/wavefront-operator:0.1.0-1
          createdAt: "2019-11-26 01:09:59"
          description: Wavefront OpenShift Operator automatically deploys Wavefront
            Collector for Kubernetes and Wavefront Proxy.
          repository: https://github.com/wavefrontHQ/wavefront-operator
          support: Wavefront by VMware
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: The Wavefront Proxy is a light-weight Java application that
              can send your metrics, histograms, and trace data to Wavefront.
            displayName: Wavefront Proxy
            kind: WavefrontProxy
            name: wavefrontproxies.wavefront.com
            version: v1alpha1
          - description: The Wavefront Collector for Kubernetes auto-discovers and
              enables monitoring of Kubernetes, infrastructure, and Kubernetes workloads
              by sending metrics to Wavefront.
            displayName: Wavefront Collector
            kind: WavefrontCollector
            name: wavefrontcollectors.wavefront.com
            version: v1alpha1
        description: "Wavefront OpenShift Operator automatically deploys Wavefront
          Collector for Kubernetes and Wavefront Proxy, enabling developers and OpenShift
          operators to get automated observability across the OpenShift environment,
          including containerized applications, Kubernetes, and underlying infrastructure.
          \n\nThis Operator deploys two Custom Resources Definitions:\n\n* **Wavefront
          Proxy:-** A Wavefront proxy ingests metrics and forwards them to the Wavefront
          service in a secure, fast, and reliable manner. After you install a proxy
          in your environment, it can handle thousands of simultaneous clients. Your
          data collection agents or custom code send data to the proxy, which consolidates
          points into configurable batches and sends the data to the Wavefront service.\n\n*
          **Wavefront Collector:-** With zero-configuration installation, the Wavefront
          Collector for Kubernetes auto-discovers and enables monitoring of Kubernetes,
          infrastructure, and Kubernetes workloads by sending full-stack - Kubernetes
          clusters, worker nodes, system, Kubernetes and Docker KPI, RED/USE of each
          layer and custom applications metrics to Wavefront.\n\n### Prerequisite\n\n*
          A Wavefront API token is needed to configure Wavefront Proxy.  Refer [Generating
          an API Token] (https://docs.wavefront.com/users_account_managing.html#generating-an-api-token)
          for generating a token.\n\n### Installing Wavefront Operator using default
          values\n\n* Create project with name *wavefront* in Openshift and install
          Wavefront Operator into it.  \n* Create Wavefront Proxy by providing Wavefront
          API token and URL in the Wavefront Proxy definition. \n* Create Wavefront
          Collector.\n\n### Advanced Options\n\nRefer Wavefront Operator [docs] (https://github.com/wavefrontHQ/wavefront-operator/blob/master/docs/openshift-operator.md)
          for configuring Wavefront Proxy and Wavefront Collector using advanced configuration
          parameters.\n"
        displayName: Wavefront Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: VMware
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: wavefront-operator
    provider:
      name: VMware
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Turbonomic, Inc.
      provider-url: ""
    name: t8c
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/t8c
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: t8c-operator.v7.17.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"Xl","metadata":{"name":"xl-release"},"spec":{"global":{"repository":"turbonomic","tag":"7.17.1"}}}]'
          capabilities: Basic Install
          categories: Monitoring
          certified: "false"
          containerImage: turbonomic/t8c-operator:7.17
          createdAt: "2019-06-01T00:00:00.000Z"
          description: Turbonomic Workload Automation for Multicloud simultaneously
            optimizes performance, compliance, and cost in real-time. Workloads are
            precisely resourced, automatically, to perform while satisfying business
            constraints.
          repository: https://github.com/turbonomic/t8c-install/tree/master/operator
          support: Turbonomic, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Turbonomic Workload Automation for Multicloud simultaneously
              optimizes performance, compliance, and cost in real-time. Workloads
              are precisely resourced, automatically, to perform while satisfying
              business constraints.
            displayName: Turbonomic Platform Operator
            kind: Xl
            name: xls.charts.helm.k8s.io
            version: v1alpha1
        description: |-
          ### Realtime Decision Automation for Multicloud Applications
          Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:
          * Continuous placement of workload across multiple clouds both on-prem and public clouds providers.
          * Continuous scaling for applications and the underlying infrastructure.

          It assures application performance by giving workloads the resources they need when they need them.

          ### How does it work?
          Turbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.
          Turbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.
          Turbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.
        displayName: Turbonomic Platform Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Turbonomic, Inc.
        version: 7.17.0
      name: alpha
    defaultChannel: alpha
    packageName: t8c
    provider:
      name: Turbonomic, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: openshift-pipelines-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/openshift-pipelines-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: openshift-pipelines-operator.v0.10.6
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "operator.tekton.dev/v1alpha1",
                "kind": "Config",
                "metadata": {
                  "name": "name.must.be-cluster"
                },
                "spec": {
                  "targetNamespace": "openshift-pipelines"
                }
              }
            ]
          capabilities: Basic Install
          categories: Developer Tools, Integration & Delivery
          certified: "false"
          containerImage: quay.io/openshift-pipeline/openshift-pipelines-operator:v0.10.6
          createdAt: "2019-03-15T19:44:21Z"
          description: OpenShift Pipelines is a cloud-native CI/CD solution for building
            pipelines using Tekton concepts which run natively on OpenShift and Kubernetes.
          operators.operatorframework.io/internal-objects: '["config.operator.tekton.dev"]'
          repository: https://github.com/openshift/tektoncd-pipeline-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: OpenShift Pipelines is a cloud-native CI/CD solution for
              building pipelines using Tekton concepts which run natively on OpenShift
              and Kubernetes.
            displayName: OpenShift Pipelines Config
            kind: Config
            name: config.operator.tekton.dev
            version: v1alpha1
        description: |
          OpenShift Pipelines is a cloud-native continuous integration and delivery
          (CI/CD) solution for building pipelines using [Tekton](https://tekton.dev).
          Tekton is a flexible Kubernetes-native open-source CI/CD framework which
          enables automating deployments across multiple platforms (Kubernetes,
          serverless, VMs, etc) by abstracting away the underlying details.

          ## Features
          * Standard CI/CD pipelines definition
          * Build images with Kubernetes tools such as S2I, Buildah, Buildpacks, Kaniko, etc
          * Deploy applications to multiple platforms such as Kubernetes, serverless and VMs
          * Easy to extend and integrate with existing tools
          * Scale pipelines on-demand
          * Portable across any Kubernetes platform
          * Designed for microservices and decentralised team
          * Integrated with OpenShift Developer Console

          ## Installation
          _OpenShift Pipelines Operator_ gets installed into a single namespace which would then install _OpenShift Pipelines_ into the same namespace. _OpenShift Pipelines_ is however cluster-wide and can run pipelines created in any namespace.

          ## Getting Started
          In order to get familiar with _OpenShift Pipelines_ concepts and create your first pipeline, follow the [OpenShift Pipelines Tutorial](https://github.com/openshift/pipelines-tutorial).

          ### CLI
          Tekton Pipelines cli project provides a CLI for interacting with OpenShift Pipelines.

          [Download Tekton CLI](https://github.com/tektoncd/cli/releases/latest)
        displayName: OpenShift Pipelines Operator
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.10.6
      name: canary
    - currentCSV: openshift-pipelines-operator.v0.8.2
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"operator.tekton.dev/v1alpha1","kind":"Config","metadata":{"name":"name.must.be-cluster"},"spec":{"targetNamespace":"openshift-pipelines"}}]'
          capabilities: Basic Install
          categories: Developer Tools, Integration & Delivery
          certified: "false"
          containerImage: quay.io/openshift-pipeline/openshift-pipelines-operator:v0.8.2
          createdAt: "2019-03-15T19:44:21Z"
          description: OpenShift Pipelines is a cloud-native CI/CD solution for building
            pipelines using Tekton concepts which run natively on OpenShift and Kubernetes.
          operators.operatorframework.io/internal-objects: '["config.operator.tekton.dev"]'
          repository: https://github.com/openshift/tektoncd-pipeline-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: OpenShift Pipelines is a cloud-native CI/CD solution for
              building pipelines using Tekton concepts which run natively on OpenShift
              and Kubernetes.
            displayName: OpenShift Pipelines Config
            kind: Config
            name: config.operator.tekton.dev
            version: v1alpha1
        description: |
          # OpenShift Pipelines
          OpenShift Pipelines is a cloud-native continuous integration and delivery
          (CI/CD) solution for building pipelines using [Tekton](https://tekton.dev).
          Tekton is a flexible Kubernetes-native open-source CI/CD framework which
          enables automating deployments across multiple platforms (Kubernetes,
          serverless, VMs, etc) by abstracting away the underlying details.

          ## Features
          * Standard CI/CD pipelines definition
          * Build images with Kubernetes tools such as S2I, Buildah, Buildpacks, Kaniko, etc
          * Deploy applications to multiple platforms such as Kubernetes, serverless and VMs
          * Easy to extend and integrate with existing tools
          * Scale pipelines on-demand
          * Portable across any Kubernetes platform
          * Designed for microservices and decentralised team
          * Integrated with OpenShift Developer Console

          ## Installation
          _OpenShift Pipelines Operator_ gets installed into a single namespace which would then install _OpenShift Pipelines_ into the same namespace. _OpenShift Pipelines_ is however cluster-wide and can run pipelines created in any namespace.

          ## Getting Started
          In order to get familiar with _OpenShift Pipelines_ concepts and create your first pipeline, follow the [OpenShift Pipelines Tutorial](https://github.com/openshift/pipelines-tutorial).

          ### CLI
          Tekton Pipelines cli project provides a CLI for interacting with OpenShift Pipelines.

          [Download Tekton CLI](https://github.com/tektoncd/cli/releases/latest)
        displayName: OpenShift Pipelines Operator
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.8.2
      name: dev-preview
    defaultChannel: dev-preview
    packageName: openshift-pipelines-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: OpsMx
      provider-url: ""
    name: open-enterprise-spinnaker
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/open-enterprise-spinnaker
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: open-enterprise-spinnaker.v1.17.4
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "charts.helm.k8s.io/v1alpha1",
                "kind": "OpenEnterpriseSpinnakerOperator",
                "metadata": {
                  "name": "oes",
                  "namespace": "spinnaker"
                },
                "spec": {
                  "halyard": {
                    "spinnakerVersion": "1.17.4"
                  },
                  "dockerRegistries": [
                    {
                      "name": "dockerhub",
                      "address": "index.docker.io",
                      "repositories": [
                        "library/alpine",
                        "library/ubuntu",
                        "library/centos",
                        "library/nginx"
                      ]
                    }
                  ],
                  "spinnakerFeatureFlags": [
                    "artifacts",
                    "artifacts-rewrite",
                    "chaos",
                    "gremlin",
                    "infrastructure-stages",
                    "mine-canary",
                    "pipeline-templates",
                    "travis",
                    "wercker",
                    "managed-pipeline-templates-v2-ui"
                  ],
                  "minio": {
                    "enabled": true,
                    "serviceType": "ClusterIP",
                    "accessKey": "spinnakeradmin",
                    "secretKey": "spinnakeradmin",
                    "bucket": "spinnaker",
                    "nodeSelector": {},
                    "persistence": {
                      "enabled": false
                    }
                  },
                  "rbac": {
                    "create": false
                  },
                  "serviceAccount": {
                    "create": false,
                    "halyardName": "open-enterprise-spinnaker",
                    "spinnakerName": null
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "true"
          containerImage: registry.connect.redhat.com/opsmx/spinnaker-operator:1.17.4-11
          createdAt: "2020-01-29 14:15:35"
          description: Spinnaker is an Open Source, multi-cloud Continuous delivery
            platform to perform software releases with high velocity and confidence.
          support: OpsMx
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: |
              # Spinnaker as an Operator

              This Operator allows users to spin up Open Enterprise Spinnaker (OES) to manage deployments etc., using Openshift CRD&#39;s. With the help of Spinnaker Services Operator, users will have the convenience and confidence of simplified approach to execute CI/CD process with high velocity and quality deployments in all environments.

              ## What is Spinnaker?

              * Spinnaker is an Open Source, multi-cloud Continuous delivery platform to perform software releases with high velocity and confidence.
              * Spinnaker helps user to create deployment pipelines that run integration and system tests, spin up and down server groups, and monitor your rollouts.
              * OES has enterprise value-adds as described at www.opsmx.com

              ### List of Features

              * Multi-Cloud Deployment - Deploy your VM or Containers or functions across most public and private cloud including AWS EC2, ECS, EKS, Lambda, Kubernetes, Google Compute Engine, Google Kubernetes Engine, Google App Engine, Microsoft Azure, OpenStack, with Oracle Bare Metal and DC/OS.
              * Automated Releases with Pipelines - Create deployment pipelines that run integration and system tests, spin up and down server groups, and monitor your rollouts. Trigger pipelines via git events, Jenkins, Travis CI, Docker, CRON, or other Spinnaker pipelines
              * Pipeline-as-code - Manage the pipeline as code (JSON) or interact with pipeline using API or UI.
              * Safe Deployment Strategies - Deploy using Canary or Red/Black (Blue/Green) or Rolling update and enable automated Canary analysis to ensure safety of the new updates before full-rollout to production
              * 1-click Rollback - Rolling back new deployments is never been easier with a 1-click rollback of images/configurations.
              * See more spinnaker.io or docs.opsmx.com
            displayName: Open Enterprise Spinnaker Operator
            kind: OpenEnterpriseSpinnakerOperator
            name: openenterprisespinnakeroperators.charts.helm.k8s.io
            version: v1alpha1
        description: |
          # Spinnaker as an Operator

          This Operator allows users to spin up Open Enterprise Spinnaker (OES) to manage deployments etc., using Openshift CRD&#39;s. With the help of Spinnaker Services Operator, users will have the convenience and confidence of simplified approach to execute CI/CD process with high velocity and quality deployments in all environments.

          ## What is Spinnaker?

          * Spinnaker is an Open Source, multi-cloud Continuous delivery platform to perform software releases with high velocity and confidence.
          * Spinnaker helps user to create deployment pipelines that run integration and system tests, spin up and down server groups, and monitor your rollouts.
          * OES has enterprise value-adds as described at www.opsmx.com

          ### List of Features

          * Multi-Cloud Deployment - Deploy your VM or Containers or functions across most public and private cloud including AWS EC2, ECS, EKS, Lambda, Kubernetes, Google Compute Engine, Google Kubernetes Engine, Google App Engine, Microsoft Azure, OpenStack, with Oracle Bare Metal and DC/OS.
          * Automated Releases with Pipelines - Create deployment pipelines that run integration and system tests, spin up and down server groups, and monitor your rollouts. Trigger pipelines via git events, Jenkins, Travis CI, Docker, CRON, or other Spinnaker pipelines
          * Pipeline-as-code - Manage the pipeline as code (JSON) or interact with pipeline using API or UI.
          * Safe Deployment Strategies - Deploy using Canary or Red/Black (Blue/Green) or Rolling update and enable automated Canary analysis to ensure safety of the new updates before full-rollout to production
          * 1-click Rollback - Rolling back new deployments is never been easier with a 1-click rollback of images/configurations.
          * See more spinnaker.io or docs.opsmx.com
        displayName: Open Enterprise Spinnaker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: OpsMx
        version: 1.17.4
      name: alpha
    defaultChannel: alpha
    packageName: open-enterprise-spinnaker
    provider:
      name: OpsMx
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: eap
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/eap
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: eap-operator.v1.0.0
      currentCSVDesc:
        annotations:
          capabilities: Basic Install
          categories: Application Runtime
          certified: "false"
          containerImage: registry.redhat.io/jboss-eap-7/eap73-rhel8-operator:1.0
          createdAt: "2019-06-12T08:00:00Z"
          description: Operator that creates and manages Java applications running
            on JBoss EAP.
          repository: https://github.com/wildfly/wildfly-operator
          support: WildFlyServer
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An application running on EAP application runtime.
            displayName: WildFlyServer
            kind: WildFlyServer
            name: wildflyservers.wildfly.org
            version: v1alpha1
        description: |
          ### Optimized for cloud and containers
          JBoss EAP 7 is built to provide simplified deployment and full Javaâ„¢ EE performance for applications in any environment. Whether on-premise or in virtual, private, public, and hybrid clouds, JBoss EAP features a modular architecture that starts services only as they are required. The extreme low memory footprint and fast startup times mean that JBoss EAP is ideal for environments where efficient resource utilization is a priority, such as Red Hat OpenShift.

          ### Lightweight, flexible architecture
          JBoss EAP 7 is built for high performance and maximum flexibility in modern application environments. Its extremely modular architecture and services-driven set of components reduces scale-out times and provides flexibility for applications deployed in different environments. Well-suited for microservices and traditional applications, JBoss EAP offers the flexibility to build the applications your business requiresâ€”from traditional to new web-scale, microservices applications.

          ### More productive developers
          JBoss EAP allows developers to be more productive and responsive to line-of-business demands. Support for Java EE and a wide range of Java EE web-based frameworks, such as Spring, Spring Web Flow, Spring WS, Spring Security, Arquillian, AngularJS, jQuery, jQuery Mobile, and Google Web Toolkit (GWT) comes standard. Use JBoss EAP with common DevOps tools to maximize productivity, decrease quality issues, and get new apps out to market faster.

          ### Flexible management, configuration, and administration
          JBoss EAP 7 maximizes administrative productivity by making it easy to maintain and update your deployments. JBoss EAP 7 features an updated management console user interface with intuitive navigation and support for large-scale domain configurations. The updated command-line interface provides a quick, unified view into configurations and subsystems, while offering the ability to manage servers offline. This can be used with popular configuration management tools, such as Ansible Tower by Red Hat or Puppetâ€”making editing XML configuration files unnecessary.

          ### Flexible, future-friendly subscription model
          A subscription to JBoss EAP provides you both technical and business flexibility. Eliminate upfront, binding licensing decisions that lock you into specific deployment environments, hardware machines, infrastructure, or levels of enterprise support. With a Red Hat subscription, youâ€™re free to build your application and then decide where and how youâ€™ll deploy.
        displayName: JBoss EAP
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: eap
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Jens Reimann
      provider-url: ""
    name: iot-simulator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/iot-simulator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: iot-simulator.0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
                "apiVersion": "iot.dentrassi.de/v1alpha1",
                "kind": "Simulator",
                "metadata": {
                  "name": "hono1",
                    "namespace": "iot-simulator"
                },
                "spec": {
                    "endpoint": {
                        "messaging": {
                            "user": "foo",
                            "password": "bar",
                            "host": "messaging.host.hono.svc",
                            "port": 5671,
                            "caCertificate": ""
                        },
                        "registry": {
                            "url": "https://your-device-registry"
                        },
                        "adapters": {
                            "mqtt": {
                                "host": "mqtt-adapter.hono.svc",
                                "port": 8883
                            },
                            "http": {
                                "url": "https://http-adapter.hono.svc"
                            }
                        }
                    }
                }
            },
            {
              "apiVersion": "iot.dentrassi.de/v1alpha1",
              "kind": "SimulatorConsumer",
              "metadata": {
                "name": "consumer1",
                "namespace": "iot-simulator"
              },
              "spec": {
                "simulator": "hono1",
                "tenant": "myapp1.iot",
                "type": "telemetry",
                "replicas": 1
              }
            },
            {
              "apiVersion": "iot.dentrassi.de/v1alpha1",
              "kind": "SimulatorProducer",
              "metadata": {
                "name": "producer1",
                "namespace": "iot-simulator"
              },
              "spec": {
                "simulator": "hono1",
                "tenant": "myapp1.iot",
                "type": "telemetry",
                "protocol": "http",
                "replicas": 1,
                "numberOfDevices": 10
              }
            }
            ]
          capabilities: Seamless Upgrades
          categories: Streaming & Messaging
          certified: "false"
          containerImage: ctron/iot-simulator-operator:0.1
          createdAt: "2019-03-19 00:00:00"
          description: An IoT device simulator, simulating producers and consumers
            using the Eclipse Hono API
          repository: https://github.com/ctron/iot-simulator-operator
          support: none
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A new simulator instance
            displayName: Simulator
            kind: Simulator
            name: simulators.iot.dentrassi.de
            version: v1alpha1
          - description: A new consumer for an existing simulator instance
            displayName: Simulator Consumer
            kind: SimulatorConsumer
            name: simulatorconsumers.iot.dentrassi.de
            version: v1alpha1
          - description: A new producer for an existing simulator instance
            displayName: Simulator Producer
            kind: SimulatorProducer
            name: simulatorproducers.iot.dentrassi.de
            version: v1alpha1
          required:
          - description: A prometheus instance
            displayName: Prometheus
            kind: Prometheus
            name: prometheuses.monitoring.coreos.com
            version: v1
          - description: A prometheus service monitor
            displayName: ServiceMonitor
            kind: ServiceMonitor
            name: servicemonitors.monitoring.coreos.com
            version: v1
        description: |
          A scalable IoT simulator, which simulates producers (MQTT or HTTP) and consumers.

          ## Pre-requisites

          You will need to have access to a Hono instance, including the messaging backend
          and the HTTP endpoint of the device registry.

          As metrics information is stored in a Prometheus instance, this operator also
          requires the Prometheus operator to be installed.

          ## Creating an instance

          Once you have the operator deployed, you need to set up at least one `Simulator` resource.
          This will trigger the creation of the web console and deploy a Prometheus instance for
          gathering metrics. The `Simulator` instance also contains the endpoint information for the
          Hono instance.

          ## Producers & Consumer

          After a `Simulator` instance has been created, you can create `SimulatorProducer` and
          `SimulatorConsumer` instances as necessary. You will need to reference the `Simulator`
          instance.
        displayName: IoT simulator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Jens Reimann
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: iot-simulator
    provider:
      name: Jens Reimann
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Apicurio Project
      provider-url: ""
    name: apicurito
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/apicurito
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: apicuritooperator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "apicur.io/v1alpha1",
              "kind": "Apicurito",
              "metadata": {
                "name": "apicurito-service"
              },
              "spec": {
                "size": 3,
                "image": "apicurio/apicurito-ui:latest"
              }
            }]
          capabilities: Seamless Upgrades
          categories: Integration & Delivery
          certified: "false"
          containerImage: apicurio/apicurito-ui:1.0.1
          createdAt: "2019-05-08 16:12:00"
          description: Manages the installation and upgrades of apicurito, a small/minimal
            version of Apicurio
          repository: https://github.com/Apicurio/apicurio-operators/tree/master/apicurito
          support: Apicurio Project
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: CRD for Apicurito
            displayName: Apicurito CRD
            kind: Apicurito
            name: apicuritos.apicur.io
            version: v1alpha1
        description: |
          Apicurito is a small/minimal version of Apicurio, a standalone API design studio that can be used to create new or edit existing API designs (using the OpenAPI specification).

          This operator supports the installation and upgrade of apicurito. Apicurito components are:
            - apicurito-ui (apicurito application)
            - apicurito route (to access apicurito from outside openshift)

          ### How to install
          When the operator is installed (you have created a subscription and the operator is running in the selected namespace) create a new CR of Kind Apicurito (click the Create New button). The CR spec contains all defaults.

          At the moment, following fields are supported as part of the CR:
            - size: how many pods your the apicurito operand will have.
            - image: the apicurito image, this can be found [here](https://hub.docker.com/r/apicurio/apicurito-ui/tags). Changing this image in an existing installation will trigger an upgrade of the operand.

          ### How to upgrade
          Upgrades are trigered by updating the image field in the CR. This can be done manually via the Openshift console, or with kubeclt:
          ```
          $ cat apicurito_cr.yaml
          apiVersion: apicur.io/v1alpha1
            kind: Apicurito
            metadata:
              name: apicurito-service
            spec:
              size: 3
              image: apicurio/apicurito-ui:newversion

          $ kubectl apply -f apicurito_cr.yaml
          ```
        displayName: Apicurito Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Apicurio Project
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: apicurito
    provider:
      name: Apicurio Project
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Sematext
      provider-url: ""
    name: sematext
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/sematext
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: sematext-operator.v1.0.9
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "sematext.com/v1alpha1",
              "kind": "SematextAgent",
              "metadata": {
                "name": "basic-agent-deployment"
              },
              "spec": {
                "region": "EU"
              }
            }]
          capabilities: Basic Install
          categories: Monitoring, Logging & Tracing
          certified: "false"
          containerImage: registry.connect.redhat.com/sematext/sematext-operator:1.0.9
          createdAt: "2019-06-13 08:00:00"
          description: Full stack observability with Sematext Monitoring Platform!
          repository: https://github.com/sematext/sematext-operator
          support: Sematext, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Sematext Agent running on each node of your
              cluster.
            displayName: Sematext Agent DaemonSet
            kind: SematextAgent
            name: sematextagents.sematext.com
            version: v1alpha1
        description: |-
          The Sematext Operator for Kubernetes provides an easy way to deploy Sematext Agent.

          Sematext Agent collects metrics about hosts (CPU, memory, disk, network, processes), containers (both Docker and rkt) and orchestrator platforms and ships that to [Sematext Cloud](https://sematext.com/cloud). Sematext Cloud is available in the US and EU regions.

          It installs the Sematext Agent to all nodes in your cluster via a `DaemonSet` resource.

          ## Settings

          This operator uses all the same options as the [Sematext Agent Helm Chart](https://hub.helm.sh/charts/stable/sematext-agent), please take a look to all the options in the following table:

          |             Parameter            |            Description            |                  Default                  |
          |----------------------------------|-----------------------------------|-------------------------------------------|
          | `containerToken`                 | Sematext Container token          | `Nil` Provide your Container token        |
          | `logsToken`                      | Sematext Logs token               | `Nil` Provide your Logs token             |
          | `infraToken`                     | Sematext Infra token              | `Nil` Provide your Infra token            |
          | `region`                         | Sematext region                   | `US` Sematext US or EU region             |
          | `agent.image.repository`         | The image repository              | `sematext/agent`                          |
          | `agent.image.tag`                | The image tag                     | `latest`                                  |
          | `agent.image.pullPolicy`         | Image pull policy                 | `Always`                                  |
          | `agent.service.port`             | Service port                      | `80`                                      |
          | `agent.service.type`             | Service type                      | `ClusterIP`                               |
          | `agent.resources`                | Agent resources                   | `{}`                                      |
          | `logagent.image.repository`      | The image repository              | `sematext/logagent`                       |
          | `logagent.image.tag`             | The image tag                     | `latest`                                  |
          | `logagent.image.pullPolicy`      | Image pull policy                 | `Always`                                  |
          | `logagent.resources`             | Logagent resources                | `{}`                                      |
          | `logagent.extraHostVolumeMounts` | Extra mounts                      | `{}`                                      |
          | `customUrl.serverBaseUrl`        | Custom Base URL                   | `Nil`                                     |
          | `customUrl.logsReceiverUrl`      | Custom Logs receiver URL          | `Nil`                                     |
          | `customUrl.eventsRecieverUrl`    | Custom Event receiver URL         | `Nil`                                     |
          | `serviceAccount.create`          | Create a service account          | `true`                                    |
          | `serviceAccount.name`            | Service account name              | `Nil` Defaults to chart name              |
          | `rbac.create`                    | RBAC enabled                      | `true`                                    |
          | `tolerations`                    | Tolerations                       | `[]`                                      |
          | `nodeSelector`                   | Node selector                     | `{}`                                      |

          For example, if you want to deploy Sematext Agent in EU region, create the following resource:

          ```yaml
          apiVersion: sematext.com/v1alpha1
          kind: SematextAgent
          metadata:
            name: test-sematextagent
          spec:
            region: "EU"
            containerToken: YOUR_CONTAINER_TOKEN
            logsToken: YOUR_LOGS_TOKEN
            infraToken: YOUR_INFRA_TOKEN
          ```

          **NOTE:** You need to create [a new Docker app in Sematext Cloud](https://apps.sematext.com/ui/integrations/create/docker) to get relevant tokens.

          Once you have created the above resource, you can apply this file with `kubectl apply -f`.
        displayName: Sematext Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Sematext
        version: 1.0.9
      name: stable
    defaultChannel: stable
    packageName: sematext
    provider:
      name: Sematext
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: IBM
      provider-url: ""
    name: open-liberty-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/open-liberty-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: open-liberty-operator.0.3.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"openliberty.io/v1beta1","kind":"OpenLibertyApplication","metadata":{"name":"demo-app"},"spec":{"replicas":1,"applicationImage":"openliberty/open-liberty:full-java8-openj9-ubi","expose":true}},{"apiVersion":
            "openliberty.io/v1beta1","kind": "OpenLibertyDump","metadata": {"name":
            "example-dump"},"spec": {"podName": "Specify_Pod_Name_Here","include":
            ["heap","thread"]}},{"apiVersion": "openliberty.io/v1beta1","kind": "OpenLibertyTrace","metadata":
            {"name": "example-trace"},"spec": {"podName": "Specify_Pod_Name_Here","traceSpecification":
            "*=info:com.ibm.ws.webcontainer*=all"}}]'
          capabilities: Seamless Upgrades
          categories: Application Runtime
          certified: "true"
          containerImage: registry.connect.redhat.com/ibm/open-liberty-operator:0.3.0
          createdAt: "2020-01-09 09:00:00"
          description: Deploy and manage applications running on Open Liberty
          repository: https://github.com/OpenLiberty/open-liberty-operator
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Describe application deployment
            displayName: Open Liberty Application
            kind: OpenLibertyApplication
            name: openlibertyapplications.openliberty.io
            version: v1beta1
          - description: Day-2 operation for gathering server traces
            displayName: Open Liberty Trace
            kind: OpenLibertyTrace
            name: openlibertytraces.openliberty.io
            version: v1beta1
          - description: Day-2 operation for generating server dumps
            displayName: Open Liberty Dump
            kind: OpenLibertyDump
            name: openlibertydumps.openliberty.io
            version: v1beta1
        description: |-
          The Open Liberty Operator can be used to deploy and manage applications running on Open Liberty. You can also perform Day-2 operations such as gathering traces and dumps using the operator. The operator provides the following key features:

          #### Routing

          Expose your application to external users via a single toggle.

          #### High Availability

          Run multiple instances of your application for high availability. Either specify a static number of replicas or easily configure auto scaling to create (and delete) instances based on resource consumption.

          #### Persistence

          Enable persistence for your application by specifying storage requirements.

          #### Serviceability

          Easily use a single storage for serviceability related operations, such as gatherig server traces or dumps.

          #### Service Binding

          Easily bind to available services in your cluster.

          #### Knative

          Deploy your serverless application on [Knative](https://knative.dev) using a single toggle.

          #### Kubernetes Application Navigator (kAppNav)

          Automatically configures the Kubernetes resources for integration with [kAppNav](https://kappnav.io/).

          See our [**documentation**](https://github.com/OpenLiberty/open-liberty-operator/tree/master/doc/) for more information.
        displayName: Open Liberty Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: IBM
        version: 0.3.0
      name: beta
    defaultChannel: beta
    packageName: open-liberty-certified
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: NeuVector
      provider-url: ""
    name: neuvector-certified-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/neuvector-certified-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: neuvector-operator.v0.9.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "apm.neuvector.com/v1alpha1",
                "kind": "Neuvector",
                "metadata": {
                  "name": "example-neuvector"
                },
                "spec": {
                  "admissionwebhook": {
                    "type": "ClusterIP"
                  },
                  "containerd": {
                    "enabled": false,
                    "path": "/var/run/containerd/containerd.sock"
                  },
                  "controller": {
                    "apisvc": {
                      "type": null
                    },
                    "azureFileShare": {
                      "enabled": false,
                      "secretName": null,
                      "shareName": null
                    },
                    "configmap": {
                      "data": null,
                      "enabled": false
                    },
                    "enabled": true,
                    "federation": {
                      "managedsvc": {
                        "type": null
                      },
                      "mastersvc": {
                        "type": null
                      }
                    },
                    "image": {
                      "repository": "neuvector/controller"
                    },
                    "ingress": {
                      "annotations": {
                        "ingress.kubernetes.io/protocol": "https"
                      },
                      "enabled": false,
                      "host": null,
                      "path": "/",
                      "secretName": null,
                      "tls": false
                    },
                    "pvc": {
                      "accessModes": [
                        "ReadWriteMany"
                      ],
                      "enabled": false,
                      "storageClass": null
                    },
                    "replicas": 3,
                    "strategy": {
                      "rollingUpdate": {
                        "maxSurge": 1,
                        "maxUnavailable": 0
                      },
                      "type": "RollingUpdate"
                    }
                  },
                  "crdwebhook": {
                    "type": "ClusterIP"
                  },
                  "crio": {
                    "enabled": true,
                    "path": "/var/run/crio/crio.sock"
                  },
                  "cve": {
                    "updater": {
                      "enabled": false,
                      "image": {
                        "repository": "neuvector/updater",
                        "tag": "latest"
                      },
                      "schedule": "0 0 * * *"
                    }
                  },
                  "docker": {
                    "enabled": false,
                    "path": "/var/run/docker.sock"
                  },
                  "enforcer": {
                    "enabled": true,
                    "image": {
                      "repository": "neuvector/enforcer"
                    },
                    "tolerations": [
                      {
                        "effect": "NoSchedule",
                        "key": "node-role.kubernetes.io/master"
                      }
                    ]
                  },
                  "exporter": {
                    "CTRL_PASSWORD": "admin",
                    "CTRL_USERNAME": "admin",
                    "enabled": false,
                    "image": {
                      "repository": "neuvector/prometheus-exporter",
                      "tag": "0.9.0"
                    },
                    "scrapping": true
                  },
                  "imagePullSecrets": "regsecret",
                  "manager": {
                    "enabled": true,
                    "env": {
                      "ssl": true
                    },
                    "image": {
                      "repository": "neuvector/manager"
                    },
                    "ingress": {
                      "annotations": {},
                      "enabled": false,
                      "host": null,
                      "path": "/",
                      "secretName": null,
                      "tls": false
                    },
                    "svc": {
                      "type": "NodePort"
                    }
                  },
                  "openshift": true,
                  "registry": "docker.io",
                  "resources": {},
                  "tag": "latest"
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Monitoring, Networking, Security
          certified: "false"
          containerImage: registry.connect.redhat.com/neuvector/neuvector-operator:v0.0.1
          createdAt: "2019-11-18 02:09:59"
          description: NeuVector delivers the only cloud-native Kubernetes security
            platform with uncompromising end-to-end protection from DevOps vulnerability
            protection to automated run-time security, and featuring a true Layer
            7 container firewall.
          repository: https://github.com/neuvector/neuvector-operator
          support: support@neuvector.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Full LifeCycle Container Security Platform
            displayName: Neuvector
            kind: Neuvector
            name: neuvectors.apm.neuvector.com
            version: v1alpha1
        description: "NeuVector delivers the only cloud-native Kubernetes security
          platform with uncompromising end-to-end protection from DevOps vulnerability
          protection to automated run-time security, and featuring a true Layer 7
          container firewall.\n\nThe NeuVector Operator runs  in the openshift container
          platform to deploy and manage the NeuVector Security cluster components.
          The NeuVector operator contains all necessary information to deploy NeuVector
          using helm charts. You simply need to install the NeuVector operator from
          the OpenShift embeded operator hub and create NeuVector instance. You can
          modify the NeuVector installation configuration by modifying yaml while
          creating the NeuVector instance such as imagePullSecrets, tag version, etc.
          Please refer to https://github.com/neuvector/neuvector-helm for the values
          that can be modifed during installation. To upgrade to a newer version of
          NeuVector, just reapply the NeuVector instance with desired tag , which
          in turn pulls the specified NeuVector image tags and upgrades as per upgrade
          plan configured on the helm chart. \n\n**Complete below steps to create
          secret for accessing Docker or similar registry and Grant Service Account
          Access to the Privileged SCC before installation.**\n\nCreate the NeuVector
          namespace\n\n         oc new-project  neuvector\nConfigure OpenShift to
          pull images from the private NeuVector registry on Docker Hub\n\n         oc
          create secret docker-registry regsecret -n neuvector --docker-server=https://index.docker.io/v1/
          --docker-username=your-name --docker-password=your-pword --docker-email=your-email\n\t\t\t\t
          \n\nWhere â€™your-nameâ€™ is your Docker username, â€™your-pwordâ€™ is your Docker
          password, â€™your-emailâ€™ is your Docker email.\n\nLogin as system:admin account\n\n
          \        oc login -u system:admin\n\nGrant Service Account Access to the
          Privileged SCC\n\n         oc -n neuvector adm policy add-scc-to-user privileged
          -z default\n\nThe following info will be added in the Privileged SCC users:\n\n
          \        - system:serviceaccount:neuvector:default\n\n\n**Add Neuvector
          license from NeuVector WebUI->setting**"
        displayName: Neuvector Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: NeuVector
        version: 0.9.0
      name: beta
    defaultChannel: beta
    packageName: neuvector-certified-operator
    provider:
      name: NeuVector
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: StorageOS, Inc
      provider-url: ""
    name: storageos-1tb
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/storageos-1tb
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: storageosoperator-rhm-1tb.v1.5.3
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "storageos.com/v1",
                "kind": "StorageOSCluster",
                "metadata": {
                  "name": "example-storageos",
                  "namespace": "openshift-operators"
                },
                "spec": {
                  "namespace": "kube-system",
                  "secretRefName": "storageos-api",
                  "secretRefNamespace": "openshift-operators",
                  "k8sDistro": "openshift",
                  "csi": {
                    "enable": true,
                    "deploymentStrategy": "deployment"
                  }
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "Job",
                "metadata": {
                  "name": "example-job",
                  "namespace": "default"
                },
                "spec": {
                  "image": "registry.connect.redhat.com/storageos/cluster-operator:latest",
                  "args": ["/var/lib/storageos"],
                  "mountPath": "/var/lib",
                  "hostPath": "/var/lib",
                  "completionWord": "done"
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "StorageOSUpgrade",
                "metadata": {
                  "name": "example-upgrade",
                  "namespace": "default"
                },
                "spec": {
                  "newImage": "registry.connect.redhat.com/storageos/node:latest"
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "NFSServer",
                "metadata": {
                  "name": "example-nfsserver",
                  "namespace": "default"
                },
                "spec": {
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                }
              }
            ]
          capabilities: Deep Insights
          categories: Storage
          certified: "true"
          containerImage: registry.connect.redhat.com/storageos/cluster-operator:1.5.3
          createdAt: "2020-02-13T17:27:53Z"
          description: Cloud-native, persistent storage for containers.
          repository: https://github.com/storageos/cluster-operator
          support: StorageOS, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: StorageOS Cluster installs StorageOS in the cluster. It contains
              all the configuration for setting up a StorageOS cluster and also shows
              the status of the running StorageOS cluster.
            displayName: StorageOS Cluster
            kind: StorageOSCluster
            name: storageosclusters.storageos.com
            version: v1
          - description: StorageOS Job creates special pods that run on all the node
              and perform an administrative task. This could be used for cluster maintenance
              tasks.
            displayName: StorageOS Job
            kind: Job
            name: jobs.storageos.com
            version: v1
          - description: StorageOS Upgrade automatically upgrades an existing StorageOS
              cluster as per the upgrade configuration.
            displayName: StorageOS Upgrade
            kind: StorageOSUpgrade
            name: storageosupgrades.storageos.com
            version: v1
          - description: StorageOS NFS Server provides support for shared volumes.
              The StorageOS control plane will automatically create and manage NFS
              Server instances when a PersistentVolumeClaim requests a volume with
              AccessMode=ReadWriteMany.
            displayName: NFS Server
            kind: NFSServer
            name: nfsservers.storageos.com
            version: v1
        description: |
          StorageOS is a cloud native, software-defined storage platform that
          transforms commodity server or cloud based disk capacity into
          enterprise-class persistent storage for containers. StorageOS is ideal for
          deploying databases, message busses, and other mission-critical stateful
          solutions, where rapid recovery and fault tolerance are essential.

          The StorageOS Operator installs and manages StorageOS within a cluster.
          Cluster nodes may contribute local or attached disk-based storage into a
          distributed pool, which is then available to all cluster members via a
          global namespace.

          Volumes are available across the cluster so if a container gets moved to
          another node it has immediate access to re-attach its data. Data can be
          protected with synchronous replication. Compression, caching, and QoS are
          enabled by default, and all volumes are thinly-provisioned.

          No other hardware or software is required.

          StorageOS is free to use up to 50GB of presented storage, increasing to
          500GB after registration.  For additional capacity and support plans contact
          sales@storageos.com.

          ## Supported Features

          * **Rapid volume failover** - If a container gets re-scheduled to another
          node, any StorageOS volumes can be re-attached immediately, irrespective of
          where the data is located within the cluster.

          * **Data protection** - Data is replicated synchronously, up to 6 copies.

          * **Block checksums** - Each block is protected by a checksum which
          automatically detects any corruption of data in the underlying storage
          media.

          * **Thin provisioning** - Only consume the space you need in a storage pool.

          * **Data reduction** - Transparent inline data compression to reduce the
          amount of storage used in a backing store as well as reducing the network
          bandwidth requirements for replication.

          * **In-memory caching** - Speed up access to data even when accessed
          remotely.

          * **Quality of service** - Prioritize data traffic and address the â€œnoisy
          neighborsâ€ problem.

          * **Flexible configuration** - Use labels to automate data placement and
          enforce data policy such as replication. Ideal for compliance and infosec
          teams to enforce policies and rules while still enabling self-service
          storage by developers and DevOps teams.

          * **Access control** - Support multiple namespace â€“ individual users are
          permissioned to specific storage namespaces.

          * **Observability & instrumentation** - Log streams for observability and
          Prometheus support for instrumentation.

          * **Deployment flexibility** - Scale up or scale out storage based on
          application requirements.  Works with any infrastructure â€“ on-premises, VM,
          bare metal or cloud.

          * **Small footprint** - Lightweight container requires minimum 1 core with
          2GB of RAM.  Runs in user-space on any 64-bit Linux with no custom kernel
          modules.

          ## Prerequisites

          * Ensure port 5705 is open on the worker nodes.

          ## Required Parameters

          * `secretRefName` - the name of a secret that contains two keys for the
          `apiUsername` and `apiPassword` to be used as api credentials
          ([documentation](https://docs.storageos.com/docs/reference/cluster-operator/examples))
          * `secretRefNamespace` - the namespace where the api credentials secret is
          stored

          ## About StorageOS

          StorageOS gives users total control of their own storage environment â€“
          whether in the cloud or on-premises. Our customers take advantage of storage
          on any platform while maintaining full control of business requirements
          around availability, data mobility, performance, security, data residency
          compliance and business continuity.
        displayName: StorageOS
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: StorageOS, Inc
        version: 1.5.3
      name: stable
    defaultChannel: stable
    packageName: storageos-1tb
    provider:
      name: StorageOS, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: IBM
      provider-url: ""
    name: couchdb-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/couchdb-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: couchdb-operator.v0.2.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "couchdb.databases.cloud.ibm.com/v1",
                "kind": "CouchDBCluster",
                "metadata": {
                    "name": "example-couchdbcluster"
                },
                "spec": {
                    "cpu": "1",
                    "disk": "1Gi",
                    "memory": "1Gi",
                    "size": 3,
                    "version": "2.3.1",
                    "environment": {
                        "adminPassword": "changeme"
                    }
                }
              },
              {
                  "apiVersion": "couchdb.databases.cloud.ibm.com/v1",
                  "kind": "Formation",
                  "metadata": {
                      "name": "example-formation"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "couchdb.databases.cloud.ibm.com/v1",
                  "kind": "FormationLock",
                  "metadata": {
                      "name": "example-formationlock"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "couchdb.databases.cloud.ibm.com/v1",
                  "kind": "Recipe",
                  "metadata": {
                      "name": "example-recipe"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "couchdb.databases.cloud.ibm.com/v1",
                  "kind": "RecipeTemplate",
                  "metadata": {
                      "name": "example-recipetemplate"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "couchdb.databases.cloud.ibm.com/v1",
                  "kind": "ServiceEndpointACL",
                  "metadata": {
                      "name": "example-serviceendpointacl"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "couchdb.databases.cloud.ibm.com/v1",
                  "kind": "Backup",
                  "metadata": {
                      "name": "example-backup"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "couchdb.databases.cloud.ibm.com/v1",
                  "kind": "Bucket",
                  "metadata": {
                      "name": "example-bucket"
                  },
                  "spec": {}
              }
            ]
          capabilities: Basic Install
          categories: Database
          certified: "true"
          containerImage: registry.connect.redhat.com/ibm/couchdb-operator
          createdAt: "2019-09-12T12:00:00Z"
          description: Apache CouchDBâ„¢ is a highly available database for web and
            mobile applications
          minKubeVersion: 1.11.0
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a deployment of an Apache CouchDBâ„¢ cluster
            displayName: CouchDB Cluster
            kind: CouchDBCluster
            name: couchdbclusters.couchdb.databases.cloud.ibm.com
            version: v1
          - description: Internal representation of a CouchDB Cluster. Used internally
              by the Operator for Apache CouchDB.
            displayName: (Internal) CouchDB Formation
            kind: Formation
            name: formations.couchdb.databases.cloud.ibm.com
            version: v1
          - description: Internal representation of a CouchDB Backup. Used internally
              by the Operator for Apache CouchDB.
            displayName: (Internal) CouchDB Backup
            kind: Backup
            name: backups.couchdb.databases.cloud.ibm.com
            version: v1
          - description: Internal representation of a COS Bucket. Used internally
              by the Operator for Apache CouchDB.
            displayName: (Internal) CouchDB Formation
            kind: Bucket
            name: buckets.couchdb.databases.cloud.ibm.com
            version: v1
          - description: Internal lock on a CouchDB Formation. Used internally by
              the Operator for Apache CouchDB.
            displayName: (Internal) CouchDB Formation Lock
            kind: FormationLock
            name: formationlocks.couchdb.databases.cloud.ibm.com
            version: v1
          - description: Internal recipe for CouchDB. Used internally by the Operator
              for Apache CouchDB.
            displayName: (Internal) CouchDB Recipe
            kind: Recipe
            name: recipes.couchdb.databases.cloud.ibm.com
            version: v1
          - description: Internal recipe template for CouchDB. Used internally by
              the Operator for Apache CouchDB.
            displayName: (Internal) CouchDB Recipe Template
            kind: RecipeTemplate
            name: recipetemplates.couchdb.databases.cloud.ibm.com
            version: v1
        description: "The Operator for Apache CouchDB facilitates deploying and managing
          an Apache CouchDB cluster on RedHat OpenShift. \n\n### Operator Features\n*
          Fully automated deployment and configuration of Apache CouchDB clusters.\n*
          Single Operator deployment can support single, multiple, or all namespaces.\n\n###
          Apache CouchDB Features\nThe Operator for Apache CouchDB uses the following
          defaults when deploying a new Apache CouchDB cluster to ensure a secure
          and highly-available configuration:\n    \n#### Security\n* TLS - The Operator
          leverages the Red Hat OpenShift cert-manager to create certificates and
          enable HTTPS/SSL on the cluster\n* Authentication - The parameter require_valid_user
          is set to true, which means that no requests are allowed from anonymous
          users. Every request must be authenticated.\n* Authorization - Databases
          are initially accessible by Apache CouchDB admins only.\n\n#### High Availability\n*
          Nodes - Each database node in an Apache CouchDB cluster requires its own
          Kubernetes node. It's recommended that you run it with a minimum of three
          nodes for any production deployment.\n* Zones - The Apache CouchDB cluster
          database nodes are spread across available Kubernetes fault zones where
          available.\n* Replicas - The default configuration for each database is
          eight shards (Q=8) and three shard copies (N=3), where each shard copy is
          deployed on a separate node in the cluster.\n"
        displayName: Operator for Apache CouchDB
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: IBM
        version: 0.2.0
      name: beta
    defaultChannel: beta
    packageName: couchdb-operator-certified
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Twistlock
      provider-url: ""
    name: twistlock
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/twistlock
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: twistlock-console-operator.v0.0.9
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"TwistlockConsole","metadata":{"name":"example-twistlockconsole"},"spec":{"COMMUNICATION_PORT":"8084","CONSOLE_CN":"","DATA_FOLDER":"/var/lib/twistlock","DATA_RECOVERY_ENABLED":"true","DATA_RECOVERY_VOLUME":"/var/lib/twistlock-backup","DEFENDER_CN":"","DEFENDER_LISTENER_TYPE":"","DOCKER_SOCKET":"","DOCKER_TWISTLOCK_TAG":"_19_03_317","HIGH_AVAILABILITY_ENABLED":"false","HIGH_AVAILABILITY_PORT":"8086","HIGH_AVAILABILITY_STATE":"PRIMARY","MANAGEMENT_PORT_HTTP":"8081","MANAGEMENT_PORT_HTTPS":"8083","READ_ONLY_FS":"true","RUN_CONSOLE_AS_ROOT":"","SCAP_ENABLED":"","SELINUX_LABEL":"disable","SYSTEMD_ENABLED":"","consoleImageName":"registry-auth.twistlock.com/tw_<REPLACE_TWISTLOCK_TOKEN>/twistlock/console:console_19_03_317","namespace":"twistlock","openshift":false,"persistentVolumeDataFolder":"var/lib/twistlock","persistentVolumeDataRecoveryFolder":"var/lib/twistlock-backup","persistentVolumeStorage":"100Gi","privileged":false,"secrets":[],"selinux-type":"","serviceType":"LoadBalancer","twistlock_cfg":"\"#  _____          _     _   _            _    \\n#
            |_   _|_      _(_)___| |_| | ___   ___| | __  \\n#   | | \\\\ \\\\ /\\\\
            / / / __| __| |/ _ \\\\ / __| |/ /      \\n#   | |  \\\\ V  V /| \\\\__
            \\\\ |_| | (_) | (__|   \u003c       \\n#   |_|   \\\\_/\\\\_/ |_|___/\\\\__|_|\\\\___/
            \\\\___|_|\\\\_\\\\\\\\     \\n\\n# This configuration file contains the
            setup parameters for Twistlock\\n# This file is typically stored in the
            same directory as the installation script (twistlock.sh)\\n# To reconfigure
            settings, update this configuration file and re-run twistlock.sh; state
            and unchanged settings will persist\\n\\n\\n\\n#############################################\\n#     Network
            configuration\\n#############################################\\n# Each
            port must be set to a unique value (multiple services cannot share the
            same port)\\n###### Management console ports #####\\n# Sets the ports
            that the Twistlock management website listens on\\n# The system that you
            use to configure Twistlock must be able to connect to the Twistlock Console
            on these ports\\n# To disable the HTTP listner, leave the value empty
            (e.g. MANAGEMENT_PORT_HTTP=)\\nMANAGEMENT_PORT_HTTP=${MANAGEMENT_PORT_HTTP-8081}\\nMANAGEMENT_PORT_HTTPS=8083\\n\\n#####
            Inter-system communication port ##### \\n# Sets the port for communication
            between the Defender(s) and the Console\\nCOMMUNICATION_PORT=8084\\n\\n#####
            Certificate common names (optional) #####\\n# Determines how to construct
            the CN in the Console''s certificate\\n# This value should not be modified
            unless instructed to by Twistlock Support\\nCONSOLE_CN=$(hostname --fqdn
            2\u003e/dev/null); if [[ $? == 1 ]]; then CONSOLE_CN=$(hostname); fi\\n#
            Determines how to construct the CN in the Defenders'' certificates\\n#
            Each Defender authenticates to the Console with this certificate and each
            cert must have a unique CN\\n# These values should not be modified unless
            instructed to by Twistlock Support\\nDEFENDER_CN=${DEFENDER_CN:-}\\n\\n#############################################\\n#     Twistlock
            system configuration\\n#############################################\\n######
            Data recovery #####\\n# Data recovery automatically exports the full Twistlock
            configuration to the specified path every 24 hours\\n# Daily, weekly,
            and monthly snapshots are retained\\n# The exported configuration can
            be stored on durable storage or backed up remotely with other tools\\n#
            Sets data recovery state (enabled or disabled)\\nDATA_RECOVERY_ENABLED=true\\n#
            Sets the directory to which Twistlock data is exported\\nDATA_RECOVERY_VOLUME=/var/lib/twistlock-backup\\n\\n#####
            Read only containers #####\\n# Sets Twistlock containers file-system to
            read-only\\nREAD_ONLY_FS=true\\n\\n##### Storage paths #####\\n# Sets
            the base directory to store Twistlock local data (db and log files)\\nDATA_FOLDER=/var/lib/twistlock\\n\\n#####
            Docker socket #####\\n# Sets the location of the Docker socket file\\nDOCKER_SOCKET=${DOCKER_SOCKET:-/var/run/docker.sock}\\n#
            Sets the type of the docker listener (TCP or NONE)\\nDEFENDER_LISTENER_TYPE=${DEFENDER_LISTENER_TYPE:-NONE}\\n\\n####
            SCAP (XCCDF) configuration ####\\n# Sets SCAP state (enabled or disabled)\\nSCAP_ENABLED=${SCAP_ENABLED:-false}\\n\\n####
            systemd configuration ####\\n# Installs Twistlock as systemd service\\nSYSTEMD_ENABLED=${SYSTEMD_ENABLED:-false}\\n\\n####
            userid configuration ####\\n# Run Twistlock console processes as root
            (default, twistlock user account)\\nRUN_CONSOLE_AS_ROOT=${RUN_CONSOLE_AS_ROOT:-false}\\n\\n####
            selinux configuration ####\\n# If SELINUX is enabled in dockerd, enable
            running Twistlock console and defender with a dedicated SELINUX label\\n#
            See https://docs.docker.com/engine/reference/run/#security-configuration\\nSELINUX_LABEL=disable\\n\\n#############################################\\n#      High
            availability settings\\n#############################################\\n#
            Only to be used when the Console is deployed outside of a Kubernetes cluster\\n#
            This native HA capability uses Mongo clustering and requires 3 or more
            instances\\nHIGH_AVAILABILITY_ENABLED=false\\nHIGH_AVAILABILITY_STATE=PRIMARY\\nHIGH_AVAILABILITY_PORT=8086\\n\\n\\n\\n#############################################\\n#      Twistlock
            repository configuration\\n#############################################\\n#
            Sets the version tag of the Twistlock containers\\n# Do not modify unless
            instructed to by Twistlock Support\\nDOCKER_TWISTLOCK_TAG=_19_03_317\\n\""}}]'
          capabilities: Basic Install
          categories: Security
          certified: "false"
          containerImage: docker.io/twistlock/console-operator:0.0.9
          createdAt: "2019-04-01 00:00:00"
          description: Deploy Twistlock cloud native security in Kubernetes.
          repository: https://github.com/twistlock/sample-code/tree/master/operators/twistlock-console-helm-operator
          support: Twistlock
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Twistlock Console is installed first and provides policy,
              API endpoints, GUI, and makes install of Defenders on each node easy
              through a daemonset.
            displayName: Twistlock Console
            kind: TwistlockConsole
            name: twistlockconsoles.charts.helm.k8s.io
            version: v1alpha1
        description: "This guide walks through using the Twistlock Console operator
          \npowered by Helm using tools and libraries provided by the Operator SDK.\n\nThe
          operator runs as a container that has the Twistlock Console helm charts
          on board\nand can watch to see if the components are present. If they're
          not, the\noperator will use Helm to install the Twistlock Console correctly
          without\nany user interaction!\n\nAll you have to to do is tell your cluster
          about the new TwistlockConsole custom resource by\napplying the included
          Custom Resource Definition (CRD). Then deploy the operator\ncontainer into
          the cluster and let it do its thing.\n\nLet's get down to it. All we need
          from you is the Twistlock registry/docs token\nthat came with your license.
          All of the yaml files you need to deploy are listed below.\n\n##  Here's
          the plan:\n\n```sh\n# Let the cluster know about our new custom resource,
          TwistlockConsole\nkubectl apply -f deploy/crds/charts_v1alpha1_twistlockconsole_crd.yaml\n#
          Create the 'twistlock' namespace\nkubectl apply -f deploy/namespace.yaml\n#
          Create necessary user and permissions to make things happen\nkubectl apply
          -f deploy/service_account.yaml \nkubectl apply -f deploy/role.yaml\nkubectl
          apply -f deploy/role_binding.yaml\n# Deploy the operator container as a
          pod\nkubectl apply -f deploy/operator.yaml\n# Add your token and apply.
          For more detail, before you apply the CR, read the note below\nkubectl apply
          -f deploy/crds/charts_v1alpha1_twistlockconsole_cr.yaml\n``` \n\n\n### Understanding
          the Twistlock Customer Resource (CR) spec\n\nHelm uses a concept called
          [values][helm_values] to provide customizations\nto a Helm chart's defaults,
          which are defined in the Helm chart's `values.yaml`\nfile.\n\nOverriding
          these defaults is as simple as setting the desired values in the CR\nspec.
          With a Twistlock token from your license you can replace this placeholder
          \n`<REPLACE_TWISTLOCK_TOKEN>` with your token.\n\n```sh\nconsoleImageName:
          registry-auth.twistlock.com/tw_<REPLACE_TWISTLOCK_TOKEN>/twistlock/console:console_19_03_317\n```
          \n\nThe other option to choose is whether we're on OpenShift, since Kubernetes
          is the default. If on OpenShift, be sure \nto add these lines to your CR
          spec as well:\n\n```sh\nopenshift: true\nserviceType: NodePort\n```"
        displayName: Twistlock Console Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Twistlock
        version: 0.0.9
      name: alpha
    defaultChannel: alpha
    packageName: twistlock
    provider:
      name: Twistlock
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: ember-csi-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ember-csi-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ember-csi-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "ember-csi.io/v1alpha1",
                "kind": "EmberCSI",
                "metadata": {
                  "name": "my-ceph"
                },
                "spec": {
                  "config": {
                    "envVars": {
                      "X_CSI_PERSISTENCE_CONFIG": {
                        "storage": "crd"
                      },
                      "X_CSI_EMBER_CONFIG": {
                        "plugin_name": "my-ceph"
                      },
                      "X_CSI_BACKEND_CONFIG": {
                        "name": "rbd",
                        "driver": "RBD",
                        "rbd_user": "admin",
                        "rbd_pool": "cephfs_data",
                        "rbd_ceph_conf": "/etc/ceph/ceph.conf",
                        "rbd_keyring_conf": "/etc/ceph/ceph.client.admin.keyring"
                      }
                    },
                    "sysFiles": {
                      "name": "sysfiles-secret",
                      "key": "system-files.tar"
                    }
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: Storage
          certified: "false"
          containerImage: docker.io/embercsi/ember-csi-operator:latest
          createdAt: 2019-08-12:22:09:00
          description: Operator to deploy Ember-CSI, a multi-vendor CSI plugin driver
            supporting over 80 storage drivers in a single plugin to provide block
            and mount storage to container orchestration systems
          repository: https://github.com/embercsi/ember-csi-operator
          support: http://readthedocs.org/projects/ember-csi/
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a deployment of EmberCSI driver
            displayName: Deployments
            kind: EmberCSI
            name: embercsis.ember-csi.io
            version: v1alpha1
        description: Operator to deploy Ember-CSI, a multi-vendor CSI plugin driver
          supporting over 80 storage drivers in a single plugin to provide block and
          mount storage to container orchestration systems.
        displayName: Ember CSI Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: ember-csi-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Dotscience
      provider-url: ""
    name: dotscience-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/dotscience-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: dotscience-operator.v0.2.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "deployer.dotscience.com/v1",
                "kind": "DeployerService",
                "metadata": {
                  "name": "example-deployerservice"
                },
                "spec": {
                  "gatewayAddress": "cloud.dotscience.com:8800",
                  "image": "quay.io/dotmesh/dotscience-deployer:latest",
                  "name": "model-deployer",
                  "serviceAccountName": "dotscience-operator",
                  "token": "3ZZ7SY27OFICJHJWY57RHNXSMS7DUASWNCXBMPHX5QKX7KDZULKA===="
                }
              }
            ]
          capabilities: Auto Pilot
          categories: Big Data,  AI/Machine Learning, Developer Tools
          certified: "false"
          containerImage: ""
          createdAt: ""
          description: ML model deployment operator
          repository: ""
          support: ""
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: DeployerService is the Schema for the deployerservices API
            displayName: Dotscience Operator
            kind: DeployerService
            name: deployerservices.deployer.dotscience.com
            version: v1
        description: |
          ## About the managed application

          Easy and fast model deployment to Kubernetes.
          ## About this Operator

          This operator allows you to deploy models to a Kubernetes cluster straight from your Jupyter notebook or via Dotscience dashboard.
          ## Prerequisites for enabling this Operator

          Dotscience account
        displayName: Dotscience Deployment Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Dotscience
        version: 0.2.0
      name: alpha
    defaultChannel: alpha
    packageName: dotscience-operator
    provider:
      name: Dotscience
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: metering
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/metering
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: metering-operator.v4.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "metering.openshift.io/v1alpha1",
                "kind": "Metering",
                "metadata": {
                  "name": "operator-metering"
                },
                "spec": {}
              },
              {
                "apiVersion": "metering.openshift.io/v1alpha1",
                "kind": "Report",
                "metadata": {
                  "name": "unready-deployment-replicas-hourly"
                },
                "spec": {
                  "query": "unready-deployment-replicas",
                  "schedule": {
                    "period": "hourly"
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1alpha1",
                "kind": "ReportQuery",
                "metadata": {
                  "name": "unready-deployment-replicas"
                },
                "spec": {
                  "columns": [
                    {
                      "name": "period_start",
                      "type": "timestamp"
                    },
                    {
                      "name": "period_end",
                      "type": "timestamp"
                    },
                    {
                      "name": "namespace",
                      "type": "varchar"
                    },
                    {
                      "name": "deployment",
                      "type": "varchar"
                    },
                    {
                      "name": "total_replica_unready_seconds",
                      "type": "double"
                    },
                    {
                      "name": "avg_replica_unready_seconds",
                      "type": "double"
                    }
                  ],
                  "inputs": [
                    {
                      "name": "ReportingStart",
                      "type": "time"
                    },
                    {
                      "name": "ReportingEnd",
                      "type": "time"
                    },
                    {
                      "default": "unready-deployment-replicas",
                      "name": "UnreadyDeploymentReplicasDataSourceName",
                      "type": "ReportDataSource"
                    }
                  ],
                  "query": "SELECT\n    timestamp '{| default .Report.ReportingStart .Report.Inputs.ReportingStart | prestoTimestamp |}' AS period_start,\n    timestamp '{| default .Report.ReportingEnd .Report.Inputs.ReportingEnd | prestoTimestamp |}' AS period_end,\n    labels['namespace'] AS namespace,\n    labels['deployment'] AS deployment,\n    sum(amount * \"timeprecision\") AS total_replica_unready_seconds,\n    avg(amount * \"timeprecision\") AS avg_replica_unready_seconds\nFROM {| dataSourceTableName .Report.Inputs.UnreadyDeploymentReplicasDataSourceName |}\nWHERE \"timestamp\" >= timestamp '{| default .Report.ReportingStart .Report.Inputs.ReportingStart | prestoTimestamp |}'\nAND \"timestamp\" < timestamp '{| default .Report.ReportingEnd .Report.Inputs.ReportingEnd | prestoTimestamp |}'\nGROUP BY labels['namespace'], labels['deployment']\nORDER BY total_replica_unready_seconds DESC, avg_replica_unready_seconds DESC, namespace ASC, deployment ASC\n"
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1alpha1",
                "kind": "ReportDataSource",
                "metadata": {
                  "name": "unready-deployment-replicas"
                },
                "spec": {
                  "prometheusMetricsImporter": {
                    "query": "sum(kube_deployment_status_replicas_unavailable) by (namespace, deployment)\n"
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1alpha1",
                "kind": "StorageLocation",
                "metadata": {
                  "name": "s3-storage-example"
                },
                "spec": {
                  "hive": {
                    "databaseName": "metering-s3",
                    "location": "s3a://bucketName/pathInBucket",
                    "unmanagedDatabase": true
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1alpha1",
                "kind": "PrestoTable",
                "metadata": {
                  "name": "example-baremetal-cost"
                },
                "spec": {
                  "catalog": "hive",
                  "columns": [
                    {
                      "name": "cost_per_gigabyte_hour",
                      "type": "double"
                    },
                    {
                      "name": "cost_per_cpu_hour",
                      "type": "double"
                    },
                    {
                      "name": "currency",
                      "type": "varchar"
                    }
                  ],
                  "createTableAs": true,
                  "query": "SELECT * FROM (\n  VALUES (10.00, 50.00, 'USD')\n) AS t (cost_per_gigabyte_hour, cost_per_cpu_hour, currency)\n",
                  "schema": "default",
                  "tableName": "example_baremetal_cost"
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1alpha1",
                "kind": "HiveTable",
                "metadata": {
                  "name": "apache-log",
                  "annotations": {
                    "reference": "based on the RegEx example from https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&SerDe"
                  }
                },
                "spec": {
                  "columns": [
                    {
                      "name": "host",
                      "type": "string"
                    },
                    {
                      "name": "identity",
                      "type": "string"
                    },
                    {
                      "name": "user",
                      "type": "string"
                    },
                    {
                      "name": "time",
                      "type": "string"
                    },
                    {
                      "name": "request",
                      "type": "string"
                    },
                    {
                      "name": "status",
                      "type": "string"
                    },
                    {
                      "name": "size",
                      "type": "string"
                    },
                    {
                      "name": "referer",
                      "type": "string"
                    },
                    {
                      "name": "agent",
                      "type": "string"
                    }
                  ],
                  "databaseName": "default",
                  "external": true,
                  "fileFormat": "TEXTFILE",
                  "location": "s3a://my-bucket/apache_logs",
                  "rowFormat": "SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\nWITH SERDEPROPERTIES (\n  \"input.regex\" = \"([^ ]*) ([^ ]*) ([^ ]*) (-|\\\\[[^\\\\]]*\\\\]) ([^ \\\"]*|\\\"[^\\\"]*\\\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\"]*|\\\"[^\\\"]*\\\") ([^ \\\"]*|\\\"[^\\\"]*\\\"))?\"\n)\n",
                  "tableName": "apache_log"
                }
              }
            ]
          capabilities: Basic Install
          categories: OpenShift Optional, Monitoring
          certified: "false"
          containerImage: quay.io/openshift/origin-metering-helm-operator:latest
          createdAt: "2019-01-01 11:59:59"
          description: The community version of metering is deprecated. Please update
            to using the metering-ocp package from the Red Hat Catalog within OperatorHub
            instead.
          repository: https://github.com/operator-framework/operator-metering
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An instance of Metering with high-level configuration
            displayName: Metering
            kind: Metering
            name: meterings.metering.openshift.io
            version: v1alpha1
          - description: A scheduled or on-off Metering Report summarizes data based
              on the query specified.
            displayName: Metering Report
            kind: Report
            name: reports.metering.openshift.io
            version: v1alpha1
          - description: A SQL query used by Metering to generate reports.
            displayName: Metering Report Query
            kind: ReportQuery
            name: reportqueries.metering.openshift.io
            version: v1alpha1
          - description: Used under-the-hood. A resource a database table in Presto.
              Use by ReportQueries to determine what tables exist, and by the HTTP
              API to determine how to query a table.
            displayName: Metering Data Source
            kind: ReportDataSource
            name: reportdatasources.metering.openshift.io
            version: v1alpha1
          - description: Represents a configurable storage location for Metering to
              store metering and report data.
            displayName: Metering Storage Location
            kind: StorageLocation
            name: storagelocations.metering.openshift.io
            version: v1alpha1
          - description: Used under-the-hood. A resource describing a source of data
              for usage by Report Queries.
            displayName: Metering Presto Table
            kind: PrestoTable
            name: prestotables.metering.openshift.io
            version: v1alpha1
          - description: Used under-the-hood. A resource a database table in Hive.
            displayName: Metering Hive Table
            kind: HiveTable
            name: hivetables.metering.openshift.io
            version: v1alpha1
        description: |
          # Deprecation Notice

          The community version of metering is **deprecated**.
          Please update to using the **metering-ocp** package from the Red Hat Catalog within OperatorHub instead.

          In the future, this version of the Metering package may be removed, so please plan accordingly.

          *Tip*: To find the correct package from the OperatorHub page: filter by setting `Provider Type` to `Red Hat`  and search for `metering-ocp`.

          ------------------

          ### Summary

          Operator Metering is a chargeback and reporting tool to provide accountability for how resources are used across a cluster. Cluster admins can schedule reports based on historical usage data by Pod, Namespace, and Cluster wide. Operator Metering is part of the [Operator Framework](https://coreos.com/blog/introducing-operator-framework-metering).

          Read the user guide for more details on [running and viewing your first report](https://github.com/operator-framework/operator-metering/blob/release-4.1/Documentation/using-metering.md).

          ### Core capabilities

          * **Chargeback/Showback** - Break down the reserved and utlized resources requested by applications.

          * **Pod, Namespace & Cluster Reports** - Built in reports exist to break down CPU and RAM in any way you desire.

          * **Scheduled Reports** - Schedule reports to run on a standard interval, eg. daily or monthly

          * **Post-Processing** - Reports are generated in CSV format and stored in persistent storage for further post-processing. Use this to send reminder emails, integrate into your ERP system, or graph on a dashboard.

          * **HTTP API** - Reports can be queried from an in-cluster HTTP API in addition to reading from persistent storage.

          ### Before you start

          Metering runs a big data stack to crunch your data and requires at least 4GB of RAM and 4 CPU cores. At least one Node should have 2GB of RAM and 2 CPU cores. Memory and CPU consumption may often be lower, but will spike when running reports, or collecting data for larger clusters.

          ### Common Configurations

          Metering works out of the box without any customizations or configuration. Read the [full documentation](https://github.com/operator-framework/operator-metering/blob/release-4.1/Documentation/metering-config.md) for more details.

          * **Use a specific StorageClass** - Follow the example to [use your own StorageClass](https://github.com/operator-framework/operator-metering/blob/release-4.1/manifests/metering-config/custom-storage.yaml) instead of the Dynamic Provisioner.

          * **Store data in S3 instead of PV** - Store your report output [in an S3 bucket](https://github.com/operator-framework/operator-metering/blob/release-4.1/Documentation/configuring-storage.md#storing-data-in-s3) instead of a PersistentVolume.

          * **Configure AWS Billing Data Source** - To assign Pod $$ costs on AWS, create a [read-only IAM user](https://github.com/operator-framework/operator-metering/blob/release-4.1/Documentation/configuring-aws-billing.md) ([example-policy](https://github.com/operator-framework/operator-metering/blob/release-4.1/Documentation/aws/read-only.json)) and [configure Metering](https://github.com/operator-framework/operator-metering/blob/release-4.1/Documentation/configuring-aws-billing.md) to use it.
        displayName: Metering (deprecated)
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.1.0
      name: preview
    defaultChannel: preview
    packageName: metering
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: lib-bucket-provisioner
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/lib-bucket-provisioner
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: lib-bucket-provisioner.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "objectbucket.io/v1alpha1",
                "kind": "ObjectBucketClaim",
                "metadata": {
                  "name": "my-obc",
                  "namespace": "my-app"
                },
                "spec": {
                  "storageClassName": "object-bucket-class",
                  "generateBucketName": "my-obc",
                  "additionalConfig": {}
                },
                "status": {}
              },
              {
                "apiVersion": "objectbucket.io/v1alpha1",
                "kind": "ObjectBucket",
                "metadata": {
                  "name": "my-obc"
                },
                "spec": {
                  "storageClassName": "object-bucket-class",
                  "reclaimPolicy": "Delete",
                  "claimRef": {
                    "name": "my-obc",
                    "namespace": "my-app"
                  },
                  "endpoint": {
                    "bucketHost": "xxx",
                    "bucketPort": 80,
                    "bucketName": "my-obc-1234-5678-1234-5678",
                    "region": "xxx",
                    "subRegion": "xxx",
                    "additionalConfig": {}
                  },
                  "additionalState": {}
                },
                "status": {}
              }
            ]
          capabilities: Basic Install
          categories: Storage,Big Data
          certified: "false"
          containerImage: kubernetes/pause
          createdAt: "2014-07-19T07:02:32.267701596Z"
          description: Library for the dynamic provisioning of object store buckets
            to be used by object store providers.
          repository: https://github.com/kube-object-storage/lib-bucket-provisioner
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Claim a bucket just like claiming a PV. Automate you app
              bucket provisioning by creating OBC with your app deployment. A secret
              and configmap (name=claim) will be created with access details for the
              app pods.
            displayName: ObjectBucketClaim
            kind: ObjectBucketClaim
            name: objectbucketclaims.objectbucket.io
            version: v1alpha1
          - description: Used under-the-hood. Created per ObjectBucketClaim and keeps
              provisioning information
            displayName: ObjectBucket
            kind: ObjectBucket
            name: objectbuckets.objectbucket.io
            version: v1alpha1
        description: "### CRD-only Operator\n\nThis operator package is **CRD-only**
          and the operator is a no-op operator.\n\nInstead, bucket provisioners using
          this library are using these CRD's and using CSV [required-crds](https://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/building-your-csv.md#required-crds)
          them so that OLM can install it as a dependency.\n\n### Generic Bucket Provisioning\n\nKubernetes
          natively supports dynamic provisioning for many types of file and block
          storage, but lacks support for object bucket provisioning. \n\nThis repo
          is a placeholder for an object store bucket provisioning library, very similar
          to the Kubernetes [sig-storage-lib-external-provisioner](https://github.com/kubernetes-sigs/sig-storage-lib-external-provisioner/blob/master/controller/controller.go)
          library.\n\n### Known Provisioners\n- https://github.com/noobaa/noobaa-operator
          (NooBaa)\n- https://github.com/rook/rook (Rook-Ceph)\n- https://github.com/yard-turkey/aws-s3-provisioner
          (AWS-S3)\n"
        displayName: lib-bucket-provisioner
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: lib-bucket-provisioner
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Portshift
      provider-url: ""
    name: portshift-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/portshift-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: portshift-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "portshift.io/v1",
                "kind": "PortshiftInstaller",
                "metadata": {
                  "name": "example-portshiftinstaller"
                },
                "spec": {
                  "managementUrl": "console.portshift.io",
                  "portshiftClusterId": "CLUSTER_ID"
                }
              }
            ]
          capabilities: Deep Insights
          categories: Monitoring, Networking, Security, OpenShift Optional
          certified: "true"
          containerImage: gcr.io/development-infra-208909/k8s_operator:v0.1.0
          createdAt: "2020-02-11 01:00:00"
          description: The operator will deploy portshift agent on the cluster
          support: Portshift
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: PortshiftInstaller is the Schema for the portshiftinstallers
              API.
            displayName: Portshift Operator Crd
            kind: PortshiftInstaller
            name: portshiftinstallers.portshift.io
            version: v1
        description: TBD
        displayName: Portshift Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Portshift
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: portshift-operator
    provider:
      name: Portshift
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Robin.io
      provider-url: ""
    name: robin-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/robin-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: robin-operator.v5.1.1
      currentCSVDesc:
        annotations:
          alm-examples: '[{ "apiVersion": "robin.io/v1alpha1", "kind": "RobinCluster",
            "metadata": { "name": "robin", "namespace": "placeholder" }, "spec": {
            "host_type": "physical", "image_robin": "registry.connect.redhat.com/robinio/robin-storage:5.1.1-9",
            "k8s_provider": "openshift", "source": "operatorhub", "image_provisioner_v04":
            "registry.connect.redhat.com/robinio/csi-provisioner:v0.4.1_robin-2",
            "image_provisioner_v10": "registry.connect.redhat.com/robinio/csi-provisioner:v1.0.0_robin-2"}}]'
          capabilities: Full Lifecycle
          categories: Database,Big Data,Storage
          certified: "true"
          containerImage: registry.connect.redhat.com/robinio/robin-operator:5.1.1-9
          createdAt: "2019-05-06 14:00:00"
          description: Robin Storage operator enables advanced data management capabilities
            to Kubernetes apps like snapshot,clone,rollback,backup,restore,import,etc.
          support: https://robin.io/support/
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Robin storage operator brings advanced data management capabilities
              for all apps including helm charts.
            displayName: Robin Cluster
            kind: RobinCluster
            name: robinclusters.robin.io
            version: v1alpha1
        description: |-
          ROBIN Storage is an application-aware container storage that offers advanced data management capabilities and runs natively on any Kubernetes distribution including OpenShift, GKE, PKS, EKS, and AKS. ROBIN Storage delivers bare-metal performance and enables you to Protect (via Snapshots, Backups), Secure (via encryption), Collaborate (via Clones and git like push/pull workflows), and make Portable (via Cloud-sync) any Stateful application that is deployed using Helm Charts or Operators.

          The ROBIN Storage Operator provides virtualized storage backed by the pool to all cluster members in a global namespace. ROBIN Storage is available FREE for up to 30 days, with no restrictions on nodes or storage capacity. For more information visit [get.robin.io](https://get.robin.io)
          ### Supported Features

          * **Enterprise-class data services** - such as inline compression, thin provisioning and data-at-rest/at-motion encryption
          * **Day 2 management capabilities** - such as point-in-time snapshots, backup & recovery, app cloning, & multi-cloud migration
          * **Data protection** - Replicate ROBIN volumes synchronously up to 3 copies
          * **Quality of service** - Set maximum IO bandwidth for each volume to address the â€œnoisy neighborsâ€ problem
          * **Rapid volume failover** - Re-attach ROBIN volumes immediately when a pod is rescheduled to another node, irrespective of where the data is located within the cluster
          * **Works with any infrastructure** - on-premises, VM, bare metal or cloud
          * **Scale on demand** - Scale up or scale out storage based on application requirements


          ### About Robin.io
          ROBIN brings advanced storage and data management that extends the Agility, Efficiency and Portability of Kubernetes to All Stateful Applications, even complex Big Data, Databases, AI/ML and Custom Apps, on Any Infrastructure, On-Premise, Hybrid Cloud or Multi-Cloud

          Enterprise customers are realizing unparalleled agility and cost benefits by running Stateful workloads such as Cloudera, Hortonworks, Spark, Mongo, Cassandra, Kafka, Postgres, MySQL, Elastic, ELK, Splunk, Oracle, Oracle RAC, SAP HANA, etc. on Kubernetes using ROBIN.
        displayName: Robin Storage
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Robin.io
        version: 5.1.0
      name: stable
    defaultChannel: stable
    packageName: robin-operator
    provider:
      name: Robin.io
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Twistlock
      provider-url: ""
    name: twistlock-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/twistlock-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: twistlock-console-operator.v0.0.9
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"TwistlockConsole","metadata":{"name":"example-twistlockconsole"},"spec":{"COMMUNICATION_PORT":"8084","CONSOLE_CN":"","DATA_FOLDER":"/var/lib/twistlock","DATA_RECOVERY_ENABLED":"true","DATA_RECOVERY_VOLUME":"/var/lib/twistlock-backup","DEFENDER_CN":"","DEFENDER_LISTENER_TYPE":"","DOCKER_SOCKET":"","DOCKER_TWISTLOCK_TAG":"_19_03_317","HIGH_AVAILABILITY_ENABLED":"false","HIGH_AVAILABILITY_PORT":"8086","HIGH_AVAILABILITY_STATE":"PRIMARY","MANAGEMENT_PORT_HTTP":"8081","MANAGEMENT_PORT_HTTPS":"8083","READ_ONLY_FS":"true","RUN_CONSOLE_AS_ROOT":"","SCAP_ENABLED":"","SELINUX_LABEL":"disable","SYSTEMD_ENABLED":"","consoleImageName":"registry-auth.twistlock.com/tw_<REPLACE_TWISTLOCK_TOKEN>/twistlock/console:console_19_03_317","namespace":"twistlock","openshift":false,"persistentVolumeDataFolder":"var/lib/twistlock","persistentVolumeDataRecoveryFolder":"var/lib/twistlock-backup","persistentVolumeStorage":"100Gi","privileged":false,"secrets":[],"selinux-type":"","serviceType":"LoadBalancer","twistlock_cfg":"\"#  _____          _     _   _            _    \\n#
            |_   _|_      _(_)___| |_| | ___   ___| | __  \\n#   | | \\\\ \\\\ /\\\\
            / / / __| __| |/ _ \\\\ / __| |/ /      \\n#   | |  \\\\ V  V /| \\\\__
            \\\\ |_| | (_) | (__|   \u003c       \\n#   |_|   \\\\_/\\\\_/ |_|___/\\\\__|_|\\\\___/
            \\\\___|_|\\\\_\\\\\\\\     \\n\\n# This configuration file contains the
            setup parameters for Twistlock\\n# This file is typically stored in the
            same directory as the installation script (twistlock.sh)\\n# To reconfigure
            settings, update this configuration file and re-run twistlock.sh; state
            and unchanged settings will persist\\n\\n\\n\\n#############################################\\n#     Network
            configuration\\n#############################################\\n# Each
            port must be set to a unique value (multiple services cannot share the
            same port)\\n###### Management console ports #####\\n# Sets the ports
            that the Twistlock management website listens on\\n# The system that you
            use to configure Twistlock must be able to connect to the Twistlock Console
            on these ports\\n# To disable the HTTP listner, leave the value empty
            (e.g. MANAGEMENT_PORT_HTTP=)\\nMANAGEMENT_PORT_HTTP=${MANAGEMENT_PORT_HTTP-8081}\\nMANAGEMENT_PORT_HTTPS=8083\\n\\n#####
            Inter-system communication port ##### \\n# Sets the port for communication
            between the Defender(s) and the Console\\nCOMMUNICATION_PORT=8084\\n\\n#####
            Certificate common names (optional) #####\\n# Determines how to construct
            the CN in the Console''s certificate\\n# This value should not be modified
            unless instructed to by Twistlock Support\\nCONSOLE_CN=$(hostname --fqdn
            2\u003e/dev/null); if [[ $? == 1 ]]; then CONSOLE_CN=$(hostname); fi\\n#
            Determines how to construct the CN in the Defenders'' certificates\\n#
            Each Defender authenticates to the Console with this certificate and each
            cert must have a unique CN\\n# These values should not be modified unless
            instructed to by Twistlock Support\\nDEFENDER_CN=${DEFENDER_CN:-}\\n\\n#############################################\\n#     Twistlock
            system configuration\\n#############################################\\n######
            Data recovery #####\\n# Data recovery automatically exports the full Twistlock
            configuration to the specified path every 24 hours\\n# Daily, weekly,
            and monthly snapshots are retained\\n# The exported configuration can
            be stored on durable storage or backed up remotely with other tools\\n#
            Sets data recovery state (enabled or disabled)\\nDATA_RECOVERY_ENABLED=true\\n#
            Sets the directory to which Twistlock data is exported\\nDATA_RECOVERY_VOLUME=/var/lib/twistlock-backup\\n\\n#####
            Read only containers #####\\n# Sets Twistlock containers file-system to
            read-only\\nREAD_ONLY_FS=true\\n\\n##### Storage paths #####\\n# Sets
            the base directory to store Twistlock local data (db and log files)\\nDATA_FOLDER=/var/lib/twistlock\\n\\n#####
            Docker socket #####\\n# Sets the location of the Docker socket file\\nDOCKER_SOCKET=${DOCKER_SOCKET:-/var/run/docker.sock}\\n#
            Sets the type of the docker listener (TCP or NONE)\\nDEFENDER_LISTENER_TYPE=${DEFENDER_LISTENER_TYPE:-NONE}\\n\\n####
            SCAP (XCCDF) configuration ####\\n# Sets SCAP state (enabled or disabled)\\nSCAP_ENABLED=${SCAP_ENABLED:-false}\\n\\n####
            systemd configuration ####\\n# Installs Twistlock as systemd service\\nSYSTEMD_ENABLED=${SYSTEMD_ENABLED:-false}\\n\\n####
            userid configuration ####\\n# Run Twistlock console processes as root
            (default, twistlock user account)\\nRUN_CONSOLE_AS_ROOT=${RUN_CONSOLE_AS_ROOT:-false}\\n\\n####
            selinux configuration ####\\n# If SELINUX is enabled in dockerd, enable
            running Twistlock console and defender with a dedicated SELINUX label\\n#
            See https://docs.docker.com/engine/reference/run/#security-configuration\\nSELINUX_LABEL=disable\\n\\n#############################################\\n#      High
            availability settings\\n#############################################\\n#
            Only to be used when the Console is deployed outside of a Kubernetes cluster\\n#
            This native HA capability uses Mongo clustering and requires 3 or more
            instances\\nHIGH_AVAILABILITY_ENABLED=false\\nHIGH_AVAILABILITY_STATE=PRIMARY\\nHIGH_AVAILABILITY_PORT=8086\\n\\n\\n\\n#############################################\\n#      Twistlock
            repository configuration\\n#############################################\\n#
            Sets the version tag of the Twistlock containers\\n# Do not modify unless
            instructed to by Twistlock Support\\nDOCKER_TWISTLOCK_TAG=_19_03_317\\n\""}}]'
          capabilities: Basic Install
          categories: Security
          certified: "false"
          containerImage: registry.connect.redhat.com/twistlock/console-operator:0.0.2
          createdAt: "2019-04-01 00:00:00"
          description: Deploy Twistlock cloud native security in Kubernetes.
          repository: https://github.com/twistlock/sample-code/tree/master/operators/twistlock-console-helm-operator
          support: Twistlock
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Twistlock Console is installed first and provides policy,
              API endpoints, GUI, and makes install of Defenders on each node easy
              through a daemonset.
            displayName: Twistlock Console
            kind: TwistlockConsole
            name: twistlockconsoles.charts.helm.k8s.io
            version: v1alpha1
        description: "This guide walks through using the Twistlock Console operator
          \npowered by Helm using tools and libraries provided by the Operator SDK.\n\nThe
          operator runs as a container that has the Twistlock Console helm charts
          on board\nand can watch to see if the components are present. If they're
          not, the\noperator will use Helm to install the Twistlock Console correctly
          without\nany user interaction!\n\nAll you have to to do is tell your cluster
          about the new TwistlockConsole custom resource by\napplying the included
          Custom Resource Definition (CRD). Then deploy the operator\ncontainer into
          the cluster and let it do its thing.\n\nLet's get down to it. All we need
          from you is the Twistlock registry/docs token\nthat came with your license.
          All of the yaml files you need to deploy are listed below.\n\n##  Here's
          the plan:\n\n```sh\n# Let the cluster know about our new custom resource,
          TwistlockConsole\nkubectl apply -f deploy/crds/charts_v1alpha1_twistlockconsole_crd.yaml\n#
          Create the 'twistlock' namespace\nkubectl apply -f deploy/namespace.yaml\n#
          Create necessary user and permissions to make things happen\nkubectl apply
          -f deploy/service_account.yaml \nkubectl apply -f deploy/role.yaml\nkubectl
          apply -f deploy/role_binding.yaml\n# Deploy the operator container as a
          pod\nkubectl apply -f deploy/operator.yaml\n# Add your token and apply.
          For more detail, before you apply the CR, read the note below\nkubectl apply
          -f deploy/crds/charts_v1alpha1_twistlockconsole_cr.yaml\n``` \n\n\n### Understanding
          the Twistlock Customer Resource (CR) spec\n\nHelm uses a concept called
          [values][helm_values] to provide customizations\nto a Helm chart's defaults,
          which are defined in the Helm chart's `values.yaml`\nfile.\n\nOverriding
          these defaults is as simple as setting the desired values in the CR\nspec.
          With a Twistlock token from your license you can replace this placeholder
          \n`<REPLACE_TWISTLOCK_TOKEN>` with your token.\n\n```sh\nconsoleImageName:
          registry-auth.twistlock.com/tw_<REPLACE_TWISTLOCK_TOKEN>/twistlock/console:console_19_03_317\n```
          \n\nThe other option to choose is whether we're on OpenShift, since Kubernetes
          is the default. If on OpenShift, be sure \nto add these lines to your CR
          spec as well:\n\n```sh\nopenshift: true\nserviceType: NodePort\n```"
        displayName: Twistlock Console Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Twistlock
        version: 0.0.9
      name: alpha
    defaultChannel: alpha
    packageName: twistlock-certified
    provider:
      name: Twistlock
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: jaeger-product
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/jaeger-product
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: jaeger-operator.v1.13.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "jaegertracing.io/v1",
                "kind": "Jaeger",
                "metadata": {
                  "name": "jaeger-all-in-one-inmemory"
                }
              },
              {
                "apiVersion": "jaegertracing.io/v1",
                "kind": "Jaeger",
                "metadata": {
                  "name": "jaeger-all-in-one-local-storage"
                },
                "spec": {
                  "storage": {
                    "options": {
                      "badger": {
                        "directory-key": "/badger/key",
                        "directory-value": "/badger/data",
                        "ephemeral": false
                      }
                    },
                    "type": "badger",
                    "volumeMounts": [
                      {
                        "mountPath": "/badger",
                        "name": "data"
                      }
                    ],
                    "volumes": [
                      {
                        "emptyDir": {},
                        "name": "data"
                      }
                    ]
                  }
                }
              },
              {
                "apiVersion": "jaegertracing.io/v1",
                "kind": "Jaeger",
                "metadata": {
                  "name": "jaeger-prod-elasticsearch"
                },
                "spec": {
                  "storage": {
                    "options": {
                      "es": {
                        "server-urls": "http://elasticsearch.default.svc:9200"
                      }
                    },
                    "type": "elasticsearch"
                  },
                  "strategy": "production"
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/distributed-tracing/jaeger-rhel7-operator:1.13.1
          createdAt: "2019-07-05T11:16:15+00:00"
          description: Provides tracing, monitoring and troubleshooting microservices-based
            distributed systems
          repository: https://github.com/jaegertracing/jaeger-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A configuration file for a Jaeger custom resource.
            displayName: Jaeger
            kind: Jaeger
            name: jaegers.jaegertracing.io
            version: v1
          required:
          - description: An Elasticsearch cluster instance
            displayName: Elasticsearch
            kind: Elasticsearch
            name: elasticsearches.logging.openshift.io
            version: v1
        description: |-
          Jaeger, inspired by [Dapper](https://research.google.com/pubs/pub36356.html) and [OpenZipkin](http://zipkin.io/), is a distributed tracing system released as open source by Uber Technologies. It is used for monitoring and troubleshooting microservices-based distributed systems.

          ### Core capabilities

          Jaeger is used for monitoring and troubleshooting microservices-based distributed systems, including:

          * Distributed context propagation
          * Distributed transaction monitoring
          * Root cause analysis
          * Service dependency analysis
          * Performance / latency optimization
          * OpenTracing compatible data model
          * Multiple storage backends: Elasticsearch, Memory.

          ### Operator features

          * **Multiple modes** - Supports `allInOne` and `production`[modes of deployment](https://www.jaegertracing.io/docs/latest/operator/#deployment-strategies).

          * **Configuration** - The Operator manages [configuration information](https://www.jaegertracing.io/docs/latest/operator/#configuring-the-custom-resource) when installing Jaeger instances.

          * **Storage** - [Configure storage](https://www.jaegertracing.io/docs/latest/operator/#storage-options) used by Jaeger. By default, `memory` is used. Other options include `elasticsearch`. The operator can delegate creation of an Elasticsearch cluster to the Elasticsearch Operator if deployed.

          * **Agent** - can be deployed as [sidecar](https://www.jaegertracing.io/docs/latest/operator/#auto-injecting-jaeger-agent-sidecars) (default) and/or [daemonset](https://www.jaegertracing.io/docs/latest/operator/#installing-the-agent-as-daemonset).

          * **UI** - Optionally setup secure route to provide [access to the Jaeger UI](https://www.jaegertracing.io/docs/latest/operator/#accessing-the-jaeger-console-ui).

          ### Before you start

          1. Ensure that the appropriate storage solution, that will be used by the Jaeger instance, is available and configured.
          2. If intending to deploy an Elasticsearch cluster via the Jaeger custom resource, then the Elasticsearch Operator must first be installed.

          ### Troubleshooting

          * https://www.jaegertracing.io/docs/latest/troubleshooting/
        displayName: Jaeger Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.13.1
      name: stable
    defaultChannel: stable
    packageName: jaeger-product
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: node-problem-detector
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/node-problem-detector
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: node-problem-detector.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "node-problem-detector.operator.k8s.io/v1alpha1",
                "kind": "NodeProblemDetector",
                "metadata": {
                  "name": "node-problem-detector",
                  "namespace": "openshift-node-problem-detector"
                }
              }
            ]
          categories: OpenShift Optional
          certifiedLevel: Primed
          containerImage: registry.svc.ci.openshift.org/openshift/origin-v4.0:node-problem-detector-operator
          createdAt: 2019/01/31
          description: An operator to run the OpenShift Node Problem Detector
          healthIndex: B
          repository: https://github.com/openshift/node-problem-detector-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an instance of a Node Problem Detector application
            displayName: Node Problem Detector Operator
            kind: NodeProblemDetector
            name: nodeproblemdetectors.node-problem-detector.operator.k8s.io
            version: v1alpha1
        description: |
          # Node Problem Detector

          node-problem-detector aims to make various node problems visible to the upstream
          layers in cluster management stack.
          It is a daemon which runs on each node, detects node problems and reports them to apiserver.
          node-problem-detector runs as a DaemonSet

          ## Background
          There are many problems that could possibly affect the pods running on the node such as:
          * Infrastructure daemon issues: ntp service down;
          * Hardware issues: Bad cpu, memory or disk;
          * Kernel issues: Kernel deadlock, corrupted file system;
          * Container runtime issues: Unresponsive runtime daemon;
          * ...

          Currently these problems are invisible to the upstream layers in cluster management
          stack, so Kubernetes will continue scheduling pods to the bad nodes.

          node-problem-detector watches the node for problems and publishes them to the API server.

          ## Problem API
          node-problem-detector uses `Event` and `NodeCondition` to report problems to
          apiserver.
          * `NodeCondition`: Permanent problem that makes the node unavailable for pods should
          be reported as `NodeCondition`. A reboot of the node will remove the `NodeCondition`.
          * `Event`: Temporary problem that has limited impact on pod but is informative
          should be reported as an `Event`.

          ## Prerequisites

          An Operator Group is required to install this operator. For example:
          ```
          $ oc create ns openshift-node-problem-detector
          $ oc create -f - <<EOF
          apiVersion: operators.coreos.com/v1alpha2
          kind: OperatorGroup
          metadata:
            name: npd-operators
            namespace: openshift-node-problem-detector
          spec:
            targetNamespaces:
            - openshift-node-problem-detector
          EOF
          ```
        displayName: Node Problem Detector
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: node-problem-detector
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: ProphetStor Data Services, Inc.
      provider-url: ""
    name: federatorai
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/federatorai
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: federatorai.v4.2.552
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "federatorai.containers.ai/v1alpha1",
                "kind": "AlamedaService",
                "metadata": {
                  "name": "my-alamedaservice"
                },
                "spec": {
                  "keycode": {
                    "codeNumber": "GRV7J-LA4TX-KPPIT-S6GRS-NK4EB-ILFRQ"
                  },
                  "selfDriving": false,
                  "enableExecution": true,
                  "enableGui": true,
                  "version": "v4.2.552",
                  "prometheusService": "https://prometheus-k8s.openshift-monitoring:9091",
                  "storages": [
                    {
                      "usage": "log",
                      "type": "ephemeral"
                    },
                    {
                      "usage": "data",
                      "type":"ephemeral"
                    }
                  ]
                }
              }
            ]
          capabilities: Auto Pilot
          categories: AI/Machine Learning, OpenShift Optional
          certified: "false"
          containerImage: quay.io/prophetstor/federatorai-operator-ubi:v4.2.552
          createdAt: "2020-01-21T12:33:58Z"
          description: Federator.ai Operator provides easy configuration and management
            of AI-based Kubernetes resource orchestrator
          repository: https://quay.io/repository/prophetstor/federatorai-operator-ubi
          support: ProphetStor Data Services, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An instance of Alameda.
            displayName: AlamedaService
            kind: AlamedaService
            name: alamedaservices.federatorai.containers.ai
            version: v1alpha1
        description: |
          **Federator.ai**, ProphetStor's Artificial Intelligence for IT Operations (AIOps) platform, provides intelligence to orchestrate container resources on top of VMs (virtual machines) or bare metal, allowing users to operate applications without the need to manage the underlying computing resources. It aims to provide optimal resource planning recommendations that will help enterprises make better decisions. The benefits of **Federator.ai** include:
          - Up to 60% resource savings
          - Increased operational efficiency
          - Reduced manual configuration time with digital intelligence

          For more information, visit our [website](https://www.prophetstor.com/federator-ai/federator-ai-for-openshift/) and [github](https://github.com/containers-ai/federatorai-operator).

          **Federator.ai Operator** is an Operator that manages **Federator.ai** components for an OpenShift cluster. Once installed, it provides the following features:
          - **Create/Clean up**: Launch **Federator.ai** components using the Operator.
          - **Easy Configuration**: Easily configure data source of Prometheus and enable/disable add-on components, such as GUI, and predictive autoscaling.
          - **Pod Scaling Recommendation/Autoscaling**: Use provided CRD to setup target pods and desired policies for scaling recommendation and autoscaling.

          ### Prerequisite
          **Federator.ai** requires a Prometheus datasource to get historical metrics of pods and nodes. When launching **Federator.ai** components, Prometheus connection settings need to be provided.

          ### Common Configurations
              apiVersion: federatorai.containers.ai/v1alpha1
              kind: AlamedaService
              metadata:
                name: my-alamedaservice
              spec:
                keycode:
                  codeNumber: GRV7J-LA4TX-KPPIT-S6GRS-NK4EB-ILFRQ
                # experimental feature, set true to enable
                selfDriving: false
                enableExecution: true
                enableGui: true
                version: v4.2.601
                prometheusService: https://prometheus-k8s.openshift-monitoring:9091
                # storages is optional. Omit this field if not needed.
                storages:
                  - usage: log       # the supported usages are log and data
                    type: ephemeral  # the supported types are ephemeral and pvc
                  - usage: data
                    type: ephemeral
        displayName: Federator.ai
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: ProphetStor Data Services, Inc.
        version: 4.2.552
      name: stable
    defaultChannel: stable
    packageName: federatorai
    provider:
      name: ProphetStor Data Services, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: descheduler
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/descheduler
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: descheduler.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "descheduler.io/v1alpha1",
                "kind": "Descheduler",
                "metadata": {
                  "name": "example-descheduler-1"
                },
                "spec": {
                  "schedule": "*/1 * * * ?",
                  "strategies": [
                    {
                      "name": "lownodeutilization",
                      "params": [
                        {
                          "name": "cputhreshold",
                          "value": "10"
                        },
                        {
                          "name": "memorythreshold",
                          "value": "20"
                        },
                        {
                          "name": "memorytargetthreshold",
                          "value": "30"
                        }
                      ]
                    }
                  ]
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional
          certifiedLevel: Primed
          containerImage: registry.svc.ci.openshift.org/openshift/origin-v4.0:descheduler-operator
          createdAt: 2019/11/15
          description: An operator to run the OpenShift descheduler, a scheduler to
            move running Pods according to policies
          healthIndex: B
          repository: https://github.com/openshift/descheduler-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an instance of a Descheduler application
            displayName: Descheduler Operator
            kind: Descheduler
            name: deschedulers.descheduler.io
            version: v1alpha1
        description: |
          # Descheduler for Kubernetes

          ## Introduction

          Scheduling in Kubernetes is the process of binding pending pods to nodes, and is performed by
          a component of Kubernetes called kube-scheduler. The scheduler's decisions, whether or where a
          pod can or can not be scheduled, are guided by its configurable policy which comprises of set of
          rules, called predicates and priorities. The scheduler's decisions are influenced by its view of
          a Kubernetes cluster at that point of time when a new pod appears first time for scheduling.
          As Kubernetes clusters are very dynamic and their state change over time, there may be desired
          to move already running pods to some other nodes for various reasons

          * Some nodes are under or over utilized.
          * The original scheduling decision does not hold true any more, as taints or labels are added to
          or removed from nodes, pod/node affinity requirements are not satisfied any more.
          * Some nodes failed and their pods moved to other nodes.
           New nodes are added to clusters.

          Consequently, there might be several pods scheduled on less desired nodes in a cluster.
          Descheduler, based on its policy, finds pods that can be moved and evicts them. Please
          note, in current implementation, descheduler does not schedule replacement of evicted pods
          but relies on the default scheduler for that.

          ## Note

          Any api could be changed any time with out any notice. That said, your feedback is
          very important and appreciated to make this project more stable and useful.
        displayName: Descheduler
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: descheduler
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Containers & PaaS CoP
      provider-url: ""
    name: resource-locker-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/resource-locker-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: resource-locker-operator.v0.1.2
      currentCSVDesc:
        annotations:
          alm-examples: "[\n  {\n    \"apiVersion\": \"redhatcop.redhat.io/v1alpha1\",\n
            \   \"kind\": \"ResourceLocker\",\n    \"metadata\": {\n      \"name\":
            \"test-simple-resource\"\n    },\n    \"spec\": {\n      \"resources\":
            [\n        {\n          \"object\": {\n            \"apiVersion\": \"v1\",\n
            \           \"kind\": \"ResourceQuota\",\n            \"metadata\": {\n
            \             \"name\": \"small-size\",\n              \"namespace\":
            \"resource-locker-test\"\n            },\n            \"spec\": {\n              \"hard\":
            {\n                \"requests.cpu\": \"4\",\n                \"requests.memory\":
            \"2Gi\"\n              }\n            }\n          }\n        }\n      ],\n
            \     \"serviceAccountRef\": {\n        \"name\": \"default\"\n      }\n
            \   }\n  },\n  {\n    \"apiVersion\": \"redhatcop.redhat.io/v1alpha1\",\n
            \   \"kind\": \"ResourceLocker\",\n    \"metadata\": {\n      \"name\":
            \"test-simple-patch\"\n    },\n    \"spec\": {\n      \"serviceAccountRef\":
            {\n        \"name\": \"default\"\n      },\n      \"patches\": [\n        {\n
            \         \"targetObjectRef\": {\n            \"apiVersion\": \"v1\",\n
            \           \"kind\": \"ServiceAccount\",\n            \"name\": \"test\",\n
            \           \"namespace\": \"resource-locker-test\"\n          },\n          \"patchTemplate\":
            \"metadata:\\n  annotations:\\n    hello: bye\\n\",\n          \"patchType\":
            \"application/strategic-merge-patch+json\"\n        }\n      ]\n    }\n
            \ },\n  {\n    \"apiVersion\": \"redhatcop.redhat.io/v1alpha1\",\n    \"kind\":
            \"ResourceLocker\",\n    \"metadata\": {\n      \"name\": \"test-complex-patch\"\n
            \   },\n    \"spec\": {\n      \"serviceAccountRef\": {\n        \"name\":
            \"default\"\n      },\n      \"patches\": [\n        {\n          \"targetObjectRef\":
            {\n            \"apiVersion\": \"v1\",\n            \"kind\": \"ServiceAccount\",\n
            \           \"name\": \"test\",\n            \"namespace\": \"resource-locker-test\"\n
            \         },\n          \"patchTemplate\": \"metadata:\\n  annotations:\\n
            \   {{ (index . 0).metadata.name }}: {{ (index . 1).metadata.name }}\\n\",\n
            \         \"patchType\": \"application/strategic-merge-patch+json\",\n
            \         \"sourceObjectRefs\": [\n            {\n              \"apiVersion\":
            \"v1\",\n              \"kind\": \"Namespace\",\n              \"name\":
            \"resource-locker-test\"\n            },\n            {\n              \"apiVersion\":
            \"v1\",\n              \"kind\": \"ServiceAccount\",\n              \"name\":
            \"default\",\n              \"namespace\": \"resource-locker-test\"\n
            \           }\n          ]\n        }\n      ]\n    }\n  }                \n]"
          capabilities: Deep Insights
          categories: Security
          certified: "false"
          containerImage: quay.io/redhat-cop/resource-locker-operator:latest
          createdAt: 11/14/2019
          description: This operator provides a facility to lock resources and / or
            patches into the state described by its custom resource preventing any
            drifts.
          repository: https://github.com/redhat-cop/resource-locker-operator
          support: Best Effort
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: represent the desire to lock a set of resources and / or
              patches into the state described by this custom resource preventing
              any drifts.
            displayName: Resource Locker
            kind: ResourceLocker
            name: resourcelockers.redhatcop.redhat.io
            version: v1alpha1
        description: "The resource locker operator allows you to specify a set of
          configurations that the operator will \"keep in place\" (lock) preventing
          any drifts. \nTwo types of configurations may be specified:\n\n* Resources.
          This will instruct the operator to create and enforce the specified resource.
          In this case the operator \"owns\" the created resources.\n* Patches to
          resources. This will instruct the operator to patch- and enforce the change
          on- a pre-existing resource. In this case the operator does not \"own\"
          the resource.\n\nLocked resources are defined with the `ResourceLocker`
          CRD. Here is the high-level structure of this CRD:\n\n```yaml\napiVersion:
          redhatcop.redhat.io/v1alpha1\nkind: ResourceLocker\nmetadata:\n  name: test-simple-resource\nspec:\n
          \ resources:\n  - object:\n      apiVersion: v1\n  ...\n  patches:\n  -
          targetObjectRef:\n    ...\n    patchTemplate: |\n      metadata:\n        annotations:\n
          \         hello: bye\n  ...\n  serviceAccountRef:\n    name: default\n```\n\nIt
          contains:\n\n* `resources`: representing an array of resources\n* `patches`:
          representing an array of patches\n* `serviceAccountRef`: a reference to
          a service account define din the same namespace as the ResourceLocker CR,
          that will be used to create the resources and apply the patches. If not
          specified the service account will be defaulted to: `default`\n\nFor each
          ResourceLocker a manager is dynamically allocated. For each resource and
          patch a controller with the needed watches is created and associated with
          the previously created manager.    \n"
        displayName: Resource Locker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Containers & PaaS CoP
        version: 0.1.2
      name: alpha
    defaultChannel: alpha
    packageName: resource-locker-operator
    provider:
      name: Containers & PaaS CoP
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Seldon Technologies
      provider-url: ""
    name: seldon-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/seldon-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: seldonoperator.v0.1.5
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion": "machinelearning.seldon.io/v1alpha2","kind":
            "SeldonDeployment","metadata": {"labels": {"app": "seldon"},"name": "seldon-model"},"spec":
            {"name": "test-deployment","oauth_key": "oauth-key","oauth_secret": "oauth-secret","predictors":
            [{"componentSpecs": [{"spec": {"containers": [{"image": "seldonio/mock_classifier:1.0","imagePullPolicy":
            "IfNotPresent","name": "classifier","resources": {"requests": {"memory":
            "1Mi"}}}],"terminationGracePeriodSeconds": 1}}],"graph": {"children":
            [],"name": "classifier","endpoint": {"type" : "REST"},"type": "MODEL"},"name":
            "example","replicas": 1,"labels": {"version" : "v1"}}]}}]'
          capabilities: Seamless Upgrades
          categories: Logging & Tracing
          certified: "true"
          containerImage: registry.connect.redhat.com/seldonio/seldon-operator-0-4-0
          createdAt: "2019-05-21 15:00:00"
          description: The Seldon operator for management, monitoring and operations
            of machine learning systems through the Seldon Engine. Once installed,
            the Seldon Operator provides multiple functions which facilitate the productisation,
            monitoring and maintenance of machine learning systems at scale.
          repository: https://github.com/SeldonIO/seldon-operator
          support: Clive Cox
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A seldon engine deployment
            displayName: Seldon Delpoyment
            kind: SeldonDeployment
            name: seldondeployments.machinelearning.seldon.io
            version: v1alpha2
        description: "The Seldon operator enables for native operation of production
          machine learning workloads, including monitoring and operations of language-agnostic
          models with the benefits of real-time metrics and log analysis.\n   \n##
          Overview\nSeldon Core is an open source platform for deploying machine learning
          models on a Kubernetes cluster.\n\n* Deploy machine learning models in the
          cloud or on-premise.\n* Get metrics and ensure proper governance and compliance
          for your running machine learning models.\n* Create powerful inference graphs
          made up of multiple components.\n* Provide a consistent serving layer for
          models built using heterogeneous ML toolkits.\n\nYou can get started by
          following the guides in our documentation at https://docs.seldon.io/projects/seldon-core/en/latest/workflow/README.html\n"
        displayName: Seldon Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Seldon Technologies
        version: 0.1.5
      name: alpha
    defaultChannel: alpha
    packageName: seldon-operator-certified
    provider:
      name: Seldon Technologies
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Tremolo Security, Inc.
      provider-url: ""
    name: openunison-ocp-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/openunison-ocp-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: openunison.1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "openunison.tremolo.io/v1",
                "kind": "OpenUnison",
                "metadata": {
                  "name": "test-openunison"
                },
                "spec": {
                  "openshift": {
                    "git": {
                      "repo": "https://github.com/TremoloSecurity/openunison-qs-simple.git",
                      "branch": "master",
                      "dir": "/"
                    },
                    "builder_image": "registry.connect.redhat.com/tremolosecurity/openunison-s2i-10"
                  },
                  "replicas": 1,
                  "enable_activemq": false,
                  "dest_secret": "openunison",
                  "source_secret": "openunison-secrets-source",
                  "hosts": [
                    {
                      "names": [
                        {
                          "name": "test.apps.mydomain.com",
                          "env_var": "OU_HOST"
                        }
                      ],
                      "ingress_name": "openunison"
                    }
                  ],
                  "secret_data": [
                    "unisonKeystorePassword",
                    "TEST_USER_PASSWORD",
                    "REG_CRED_PASSWORD"
                  ],
                  "non_secret_data": [
                    {
                      "name": "REG_CRED_USER",
                      "value": "rh_user"
                    },
                    {
                      "name": "TEST_USER_NAME",
                      "value": "testuser"
                    },
                    {
                      "name": "MYVD_CONFIG_PATH",
                      "value": "WEB-INF/myvd.conf"
                    },
                    {
                      "name": "unisonKeystorePath",
                      "value": "/etc/openunison/unisonKeyStore.p12"
                    }
                  ],
                  "openunison_network_configuration": {
                    "open_port": 8080,
                    "open_external_port": 80,
                    "secure_port": 8443,
                    "secure_external_port": 443,
                    "secure_key_alias": "unison-tls",
                    "force_to_secure": true,
                    "activemq_dir": "/tmp/amq",
                    "quartz_dir": "/tmp/quartz",
                    "client_auth": "none",
                    "allowed_client_names": [],
                    "ciphers": [
                      "TLS_RSA_WITH_RC4_128_SHA",
                      "TLS_RSA_WITH_AES_128_CBC_SHA",
                      "TLS_RSA_WITH_AES_256_CBC_SHA",
                      "TLS_RSA_WITH_3DES_EDE_CBC_SHA",
                      "TLS_RSA_WITH_AES_128_CBC_SHA256",
                      "TLS_RSA_WITH_AES_256_CBC_SHA256"
                    ],
                    "path_to_deployment": "/usr/local/openunison/work",
                    "path_to_env_file": "/etc/openunison/ou.env"
                  },
                  "key_store": {
                    "static_keys": [
                      {
                        "name": "session-unison",
                        "version": 1
                      }
                    ],
                    "trusted_certificates": [],
                    "key_pairs": {
                      "create_keypair_template": [
                        {
                          "name": "ou",
                          "value": "k8s"
                        },
                        {
                          "name": "o",
                          "value": "Tremolo Security"
                        },
                        {
                          "name": "l",
                          "value": "Alexandria"
                        },
                        {
                          "name": "st",
                          "value": "Virginia"
                        },
                        {
                          "name": "c",
                          "value": "US"
                        }
                      ],
                      "keys": [
                        {
                          "name": "unison-tls",
                          "tls_secret_name": "unison-tls-secret",
                          "import_into_ks": "keypair",
                          "create_data": {
                            "sign_by_k8s_ca": false,
                            "server_name": "test-openunison.openunison.svc.cluster.local",
                            "subject_alternative_names": [],
                            "key_size": 2048,
                            "ca_cert": true
                          }
                        }
                      ]
                    }
                  }
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Security
          certified: "true"
          containerImage: registry.connect.redhat.com/tremolosecurity/openunison-operator
          createdAt: "2020-01-08"
          description: OpenUnison Operator provides identity management and automation
            services including Single Sign-On (SSO), workflow based user provisioning,
            self service access management, auditing and reporting
          repository: https://github.com/TremoloSecurity/openunison-k8s-operator
          support: https://github.com/TremoloSecurity/openunison-k8s-operator
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A running OopenUnison instance
            displayName: OpenUnison
            kind: OpenUnison
            name: openunisons.openunison.tremolo.io
            version: v1
        description: "The OpenUnison operator automates the deployment of OpenUnison
          in OpenShift and OKD.  This operator will generate all the objects needed
          to run OpenUnison and keep them up-to-date.  On OpenShift OpenUnison is
          built and deployed via a Red Hat certified Source-to-Image (s2i) builder
          that generates a container based on your configuration.  The default example
          provided by the operator is a simple login page.  Tremolo Security offers
          [Orchestra for OpenShift](https://www.tremolosecurity.com/openshift/) to
          manage authentication, authorization and project onboarding for your OpenShift
          clusters.  \n\n## Before You Start\n\nBefore deploying, obtain an account
          for Red Hat Connect to be able to access certified images. Next create a
          source secret with information that shouldn't be stored in the `OpenUnison`
          custom resource, referenced by the `source_secret` attribute.  The minimal
          data in this secret should be at least:\n\n* `unisonKeystorePassword` -
          The password to use for the generated keystore\n* `REG_CRED_PASSWORD` -
          Your password for Red Hat Connect\n* `TEST_USER_PASSWORD` - The password
          for your test user \n\n An example of creating a secret used by the example
          configuration is:\n  \n```\n$ mkdir secret\n$ cd secret\n$ echo 'my_secret_password'
          \ > unisonKeystorePassword\n$ echo 'my_rh_connect_password'  > REG_CRED_PASSWORD\n$
          echo 'my_test_user_password'  > TEST_USER_PASSWORD\n$ oc create secret generic
          openunison-secrets-source --from-file=.\n```\n  \nYou should edit the `hosts`
          \\ `names` \\ `name` variable from `test.apps.mydomain.com` to whatever
          you want the host of your OpenUnison instance to be.  The operator will
          generate a `Route` based on this hostname with `reencrypt` enabled.  Also,
          update `key_store` \\ 'key_pairs' \\ `keys` for the `server_name` to reflect
          the correct `Service` name for your OpenUnison.  For instance if your `OpenUnison`
          CR has the name `test-openunison` and is being deployed in the `openunison`
          namespace this value should be `test-openunison.openunison.svc.cluster.local`.\n\n##
          Testing\n\nOnce deployed, point your browser to the host name you created
          via TLS (ie https://myapp.apps.domain.com)\n\n## Updating Keys\n\nThe operator
          builds a keystore for OpenUnison based on secrets referenced in the `OpenUnison`
          custom resource.  To regenerate self signed certificates (such as the default
          `unison-tls` certificate), simple delete the secret and update the custom
          resource.  To use certificates signed by a third party CA, create the secret
          and reference it in your `OpenUnison` custom resource.\n\n## New Images
          and Configuration\n\nThe OpenUnison Operator generates an OpenShift `BuildConfig`
          object and `ImageStream` objects.  The certified s2i builder image is rebuilt
          auotmaticlly by Red Hat as new packages are made available.  To update your
          deployment's container, simply tag it to your image stream.  To point to
          a new git repository for the configuration of OpenUnison, update the CR."
        displayName: OpenUnison
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Tremolo Security, Inc.
        version: 1.0.0
      name: stable
    defaultChannel: stable
    packageName: openunison-ocp-certified
    provider:
      name: Tremolo Security, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: LABS.AI
      provider-url: ""
    name: eddi-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/eddi-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: eddi-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "labs.ai/v1alpha1",
                "kind": "Eddioperator",
                "metadata": {
                  "name": "eddioperator"
                },
                "spec": {
                  "image": {
                    "image_name": "eddi",
                    "repository": "labsai",
                    "tag": "latest"
                  },
                  "size": 1
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: AI/Machine Learning, Developer Tools
          certified: "false"
          containerImage: registry.connect.redhat.com/labsai/eddi
          createdAt: "2019-11-09 12:59:59"
          description: Scalable Open Source Chatbot Platform. Build multiple Chatbots
            with NLP, Behavior Rules, API Connector, Templating.  Developed in Java,
            provided with Docker, orchestrated with Kubernetes or Openshift.
          repository: https://github.com/labsai/EDDI-operator/
          support: Eddi
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Eddi operator description
            displayName: EDDI
            kind: Eddioperator
            name: eddioperator.labs.ai
            version: v1alpha1
        description: "## About the managed application\n**Enterprise-Ready Open Source
          Chatbot Platform**\n\ncreate, run and maintain customizable chatbots\n\n\n**Scale
          Horizontally**\n\nscale out horizontally due to resource-oriented design
          & RESTful\narchitecture\n\n\n**NLP Parser**\n\nmatch user inputs as words
          and phrases even in case of spelling mistakes or\nmissing spaces between
          words\n\n\n**Behavior Rules**\n\nmake decisions with predefined and custom
          conditions\n\n\n**Conversation Memory**\n\nstore context information between
          user interactions in order to make better\nconversation decisions\n\n\n**Extension
          Oriented**\n\nbuild loosely coupled plugins that allows the integration
          of new algorithms\nand communication to other systems\n\n\n**Modular Bot
          Configurations**\n\nshare bot knowledge across bots in order to reuse components
          and thus speed\nup further developments\n\n\n**Versioned Configurations**\n\ntrack
          changes of all of your configurations. Use different versions of your\nconfigurations
          in different bots, to allow multiple bots with shared\nknowledge\n\n\n**Developed
          as Microservices, Restful by Design**\n\nwe don't need to comment on that,
          do we...? ;-) \n\n\n## About this Operator\nLaunch and update EDDI instances\n\n##
          Prerequisites for enabling this Operator\nYou need to first launch mongodb
          before you should launch EDDI, preferably\nwith the mongodb operator\n"
        displayName: EDDI Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: LABS.AI
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: eddi-operator-certified
    provider:
      name: LABS.AI
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: redis-enterprise
      provider-url: ""
    name: redis-enterprise-operator-cert
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/redis-enterprise-operator-cert
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: redis-enterprise-operator.v5.4.10-8a
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "app.redislabs.com/v1",
                "kind": "RedisEnterpriseCluster",
                "metadata": {
                  "name": "example-redisenterprisecluster"
                },
                "spec": {
                  "size": 3
                }
              }
            ]
          capabilities: Basic Install
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Deploy a rec
            displayName: RedisEnterpriseCluster
            kind: RedisEnterpriseCluster
            name: redisenterpriseclusters.app.redislabs.com
            version: v1
        description: The Redis Enterprise Operator is the fastest, most efficient
          way to deploy and maintain a Redis Enterprise Cluster in Kubernetes.
        displayName: Redis Enterprise Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: redis-enterprise
        version: 5.4.10-8a
      name: production
    defaultChannel: production
    packageName: redis-enterprise-operator-cert
    provider:
      name: redis-enterprise
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Crunchy Data
      provider-url: ""
    name: crunchy-postgres-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/crunchy-postgres-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: postgresoperator.v4.2.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgcluster",
                "metadata": {
                  "labels": {
                    "archive": "false"
                  },
                  "name": "example"
                },
                "spec": {
                  "PrimaryStorage": {
                    "accessmode": "ReadWriteOnce",
                    "size": "1G",
                    "storageclass": "standard",
                    "storagetype": "dynamic"
                  },
                  "ccpimage": "crunchy-postgres-ha",
                  "ccpimagetag": "rhel7-12.1-4.2.1",
                  "clustername": "example",
                  "database": "example",
                  "exporterport": "9187",
                  "name": "example",
                  "pgbadgerport": "10000",
                  "port": "5432",
                  "primarysecretname": "example-primaryuser",
                  "rootsecretname": "example-postgresuser",
                  "userlabels": {
                    "archive": "false"
                  },
                  "usersecretname": "example-primaryuser"
                }
              },
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgreplica",
                "metadata": {
                  "name": "example"
                },
                "spec": {},
                "status": {}
              },
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgpolicy",
                "metadata": {
                  "name": "example"
                },
                "spec": {},
                "status": {}
              },
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgtask",
                "metadata": {
                  "name": "example"
                },
                "spec": {}
              },
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgbackup",
                "metadata": {
                  "name": "example"
                },
                "spec": {},
                "status": {}
              }
            ]
          capabilities: Auto Pilot
          categories: Database
          certified: "false"
          containerImage: registry.connect.redhat.com/crunchydata/postgres-operator:rhel7-4.2.1-3
          createdAt: 2019-12-31 19:40Z
          description: Enterprise open source PostgreSQL-as-a-Service
          repository: https://github.com/CrunchyData/postgres-operator
          support: crunchydata.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Postgres primary cluster member
            displayName: Postgres Primary Cluster Member
            kind: Pgcluster
            name: pgclusters.crunchydata.com
            version: v1
          - description: Represents a Postgres replica cluster member
            displayName: Postgres Replica Cluster Member
            kind: Pgreplica
            name: pgreplicas.crunchydata.com
            version: v1
          - description: Represents a Postgres sql policy
            displayName: Postgres SQL Policy
            kind: Pgpolicy
            name: pgpolicies.crunchydata.com
            version: v1
          - description: Represents a Postgres workflow task
            displayName: Postgres workflow task
            kind: Pgtask
            name: pgtasks.crunchydata.com
            version: v1
          - description: Represents a Postgres backup task
            displayName: Postgres backup task
            kind: Pgbackup
            name: pgbackups.crunchydata.com
            version: v1
        description: |-
          Crunchy PostgreSQL for OpenShift lets you run your own production-grade PostgreSQL-as-a-Service on OpenShift!

          Powered by the Crunchy [PostgreSQL Operator](https://github.com/CrunchyData/postgres-operator), Crunchy PostgreSQL
          for OpenShift automates and simplifies deploying and managing open source PostgreSQL clusters on OpenShift by providing the
          essential features you need to keep your PostgreSQL clusters up and running, including:

          - **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],
          while fully customizing your Pods and PostgreSQL configuration!
          - **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].
          Uses [Pod Anti-Affinity][anti-affinity] to help resiliency; you can configure how aggressive this can be!
          Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled
          backups as well and set your backup retention policy
          - **Disaster Recovery**: Backups and restores leverage the open source [pgBackRest][] utility
          and [includes support for full, incremental, and differential backups as well as efficient delta restores][disaster-recovery].
          Set how long you want your backups retained for. Works great with very large databases!
          - **Monitoring**: Track the health of your PostgreSQL clusters using the open source [pgMonitor][] library.
          - **Clone**: Create new clusters from your existing clusters with a simple [`pgo clone`][pgo-clone] command.
          - **Full Customizability**: Crunchy PostgreSQL for OpenShift makes it easy to get your own PostgreSQL-as-a-Service up and running on
          and lets make further enhancements to customize your deployments, including:
            - Selecting different storage classes for your primary, replica, and backup storage
            - Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters!
            - Use your own container image repository, including support `imagePullSecrets` and private repositories
            - Bring your own trusted certificate authority (CA) for use with the Operator API server
            - Override your PostgreSQL configuration for each cluster

          and much more!

          [anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
          [disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/disaster-recovery/
          [high-availability]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/
          [pgo-clone]: https://access.crunchydata.com/documentation/postgres-operator/latest/pgo-client/reference/pgo_clone/
          [provisioning]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/provisioning/

          [pgBackRest]: https://www.pgbackrest.org
          [pgMonitor]: https://github.com/CrunchyData/pgmonitor

          ## Before You Begin

          There are several manual steps that the cluster administrator must perform prior to installing the operator. The
          operator must be provided with an initial configuration to run in the cluster, as well as certificates and
          credentials that need to be generated.

          Start by cloning the operator repository locally.

          ```
          git clone -b v4.2.1 https://github.com/CrunchyData/postgres-operator.git
          cd postgres-operator
          ```

          ### PostgreSQL Operator Configuration

          Edit `conf/postgres-operator/pgo.yaml` to configure the operator deployment. Look over all of the options and make any
          changes necessary for your environment.

          #### Image

          Update the `CCPImageTag` tag to configure the PostgreSQL image being used, updating for the version of PostgreSQL as needed.

          ```
          CCPImageTag:  rhel7-12.1-4.2.1
          ```

          #### Storage

          Configure the backend storage for the Persistent Volumes used by each PostgreSQL cluster. Depending on the type of persistent
          storage you wish to make available, adjust the `StorageClass` as necessary. For example, to deploy on AWS using `gp2`, you
          would set the following:

          ```
          storageos:
            AccessMode:  ReadWriteOnce
            Size:  1G
            StorageType:  dynamic
            StorageClass:  gp2
            Fsgroup:  26
          ```

          Once the storage backend is defined, enable the new storage option as needed.

          ```
          PrimaryStorage: storageos
          ReplicaStorage: storageos
          BackrestStorage: storageos
          ```

          ### Certificates

          You will need to either generate new TLS certificates or use existing certificates for the operator API.

          You can generate new self-signed certificates using scripts in the operator repository.

          ```
          export PGOROOT=$(pwd)
          cd $PGOROOT/deploy
          $PGOROOT/deploy/gen-api-keys.sh
          $PGOROOT/deploy/gen-sshd-keys.sh
          cd $PGOROOT
          ```

          ### Configuration and Secrets

          Once the configuration changes have been updated and certificates are in place, we can save the information to the cluster.

          Create the pgo namespace if it does not exist already. This single namespace is where the operator should be deployed to. PostgreSQL clusters will also be deployed here.

          ```
          oc create namespace pgo
          ```

          Create the `pgo-backrest-repo-config` Secret that is used by the operator.

          ```
          oc create secret generic -n pgo pgo-backrest-repo-config \
            --from-file=config=$PGOROOT/conf/pgo-backrest-repo/config \
            --from-file=sshd_config=$PGOROOT/conf/pgo-backrest-repo/sshd_config \
            --from-file=aws-s3-credentials.yaml=$PGOROOT/conf/pgo-backrest-repo/aws-s3-credentials.yaml \
            --from-file=aws-s3-ca.crt=$PGOROOT/conf/pgo-backrest-repo/aws-s3-ca.crt
          ```

          Create the `pgo-auth-secret` Secret that is used by the operator.

          ```
          oc create secret generic -n pgo pgo-auth-secret \
            --from-file=server.crt=$PGOROOT/conf/postgres-operator/server.crt \
            --from-file=server.key=$PGOROOT/conf/postgres-operator/server.key
          ```

          Install the bootstrap credentials:

          ```
          $PGOROOT/deploy/install-bootstrap-creds.sh
          ```

          Install the security context constraint for OpenShift:

          ```
          oc create -f $PGOROOT/deploy/pgo-scc.yaml
          ```

          Remove existing credentials for pgo-apiserver TLS REST API, if they exist.

          ```
          oc delete secret -n pgo tls pgo.tls
          ```

          Create credentials for pgo-apiserver TLS REST API
          ```
          oc create secret -n pgo tls pgo.tls \
            --key=$PGOROOT/conf/postgres-operator/server.key \
            --cert=$PGOROOT/conf/postgres-operator/server.crt
          ```

          Create the `pgo-config` ConfigMap that is used by the operator.

          ```
          oc create configmap -n pgo pgo-config \
            --from-file=$PGOROOT/conf/postgres-operator
          ```

          Once these resources are in place, the operator can be installed into the cluster.

          ## After You Install

          Once the operator is installed in the cluster, you will need to perform several steps to enable usage.

          ### Service

          ```
          oc expose deployment -n pgo postgres-operator --type=LoadBalancer
          ```

          For the pgo client to communicate with the operator, it needs to know where to connect.
          Export the service URL as `PGO_APISERVER_URL` in the shell.

          ```
          export PGO_APISERVER_URL=https://<url of exposed service>:8443
          ```

          ### Security

          When postgres operator deploys, it creates a set of certificates the pgo client will need to communicate.

          ### Client Certificates

          Copy the client certificates from the apiserver to the local environment - we use /tmp for this example.

          ```
          oc cp <pgo-namespace>/<postgres-operator-pod>:/tmp/server.key /tmp/server.key -c apiserver
          oc cp <pgo-namespace>/<postgres-operator-pod>:/tmp/server.crt /tmp/server.crt -c apiserver
          ```

          Configure the shell for the pgo command line to use the certificates

          ```
          export PGO_CA_CERT=/tmp/server.crt
          export PGO_CLIENT_CERT=/tmp/server.crt
          export PGO_CLIENT_KEY=/tmp/server.key
          ```
        displayName: Crunchy PostgreSQL for OpenShift
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Crunchy Data
        version: 4.2.1
      name: stable
    defaultChannel: stable
    packageName: crunchy-postgres-operator
    provider:
      name: Crunchy Data
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc
      provider-url: ""
    name: cluster-logging
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cluster-logging
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: clusterlogging.4.2.18-202002031246
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
                {
                  "apiVersion": "logging.openshift.io/v1",
                  "kind": "ClusterLogging",
                  "metadata": {
                    "name": "instance",
                    "namespace": "openshift-logging"
                   },
                  "spec": {
                    "managementState": "Managed",
                    "logStore": {
                      "type": "elasticsearch",
                      "elasticsearch": {
                        "nodeCount": 3,
                        "redundancyPolicy": "SingleRedundancy",
                        "storage": {
                          "storageClassName": "gp2",
                          "size": "200G"
                         }
                       }
                    },
                    "visualization": {
                      "type": "kibana",
                      "kibana": {
                        "replicas": 1
                      }
                    },
                    "curation": {
                      "type": "curator",
                      "curator": {
                        "schedule": "30 3 * * *"
                      }
                    },
                    "collection": {
                      "logs": {
                        "type": "fluentd",
                        "fluentd": {}
                      }
                    }
                  }
                }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-cluster-logging-operator@sha256:aa62b457c13331b18c827dd5310b1334faaca1a3fcae158b29c4d0aa1be33956
          createdAt: "2018-08-01T08:00:00Z"
          description: The Cluster Logging Operator for OKD provides a means for configuring
            and managing your aggregated logging stack.
          olm.skipRange: '>=4.1.0 <4.2.18-202002031246'
          support: AOS Logging
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Cluster Logging instance
            displayName: Cluster Logging
            kind: ClusterLogging
            name: clusterloggings.logging.openshift.io
            version: v1
        description: |
          # Cluster Logging
          The Cluster Logging Operator orchestrates and manages the aggregated logging stack as a cluster-wide service.

          ##Features
          * **Create/Destroy**: Launch and create an aggregated logging stack to support the entire OKD cluster.
          * **Simplified Configuration**: Configure your aggregated logging cluster's structure like components and end points easily.

          ## Prerequisites and Requirements
          ### Cluster Logging Namespace
          Cluster logging and the Cluster Logging Operator is only deployable to the **openshift-logging** namespace. This namespace
          must be explicitly created by a cluster administrator (e.g. `oc create ns openshift-logging`). To enable metrics
          service discovery add namespace label `openshift.io/cluster-monitoring: "true"`.
          ### Elasticsearch Operator
          The Elasticsearch Operator is responsible for orchestrating and managing cluster logging's Elasticsearch cluster.  This
          operator must be deployed to the global operator group namespace
          ### Memory Considerations
          Elasticsearch is a memory intensive application.  Cluster Logging will specify that each Elasticsearch node needs
          16G of memory for both request and limit unless otherwise defined in the ClusterLogging custom resource. The initial
          set of OKD nodes may not be large enough to support the Elasticsearch cluster.  Additional OKD nodes must be added
          to the OKD cluster if you desire to run with the recommended(or better) memory. Each ES node can operate with a
          lower memory setting though this is not recommended for production deployments.
        displayName: Cluster Logging
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc
        version: 4.2.18-202002031246
      name: "4.2"
    - currentCSV: clusterlogging.4.2.18-202002031246-s390x
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
                {
                  "apiVersion": "logging.openshift.io/v1",
                  "kind": "ClusterLogging",
                  "metadata": {
                    "name": "instance",
                    "namespace": "openshift-logging"
                   },
                  "spec": {
                    "managementState": "Managed",
                    "logStore": {
                      "type": "elasticsearch",
                      "elasticsearch": {
                        "nodeCount": 3,
                        "redundancyPolicy": "SingleRedundancy",
                        "storage": {
                          "storageClassName": "gp2",
                          "size": "200G"
                         }
                       }
                    },
                    "visualization": {
                      "type": "kibana",
                      "kibana": {
                        "replicas": 1
                      }
                    },
                    "curation": {
                      "type": "curator",
                      "curator": {
                        "schedule": "30 3 * * *"
                      }
                    },
                    "collection": {
                      "logs": {
                        "type": "fluentd",
                        "fluentd": {}
                      }
                    }
                  }
                }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-cluster-logging-operator@sha256:c9b10cded82360af2115be9637e060e64e26ff8b8d2cf27a29bcc71c22c87eae
          createdAt: "2018-08-01T08:00:00Z"
          description: The Cluster Logging Operator for OKD provides a means for configuring
            and managing your aggregated logging stack.
          olm.skipRange: '>=4.1.0 <4.2.18-202002031246'
          support: AOS Logging
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Cluster Logging instance
            displayName: Cluster Logging
            kind: ClusterLogging
            name: clusterloggings.logging.openshift.io
            version: v1
        description: |
          # Cluster Logging
          The Cluster Logging Operator orchestrates and manages the aggregated logging stack as a cluster-wide service.

          ##Features
          * **Create/Destroy**: Launch and create an aggregated logging stack to support the entire OKD cluster.
          * **Simplified Configuration**: Configure your aggregated logging cluster's structure like components and end points easily.

          ## Prerequisites and Requirements
          ### Cluster Logging Namespace
          Cluster logging and the Cluster Logging Operator is only deployable to the **openshift-logging** namespace. This namespace
          must be explicitly created by a cluster administrator (e.g. `oc create ns openshift-logging`). To enable metrics
          service discovery add namespace label `openshift.io/cluster-monitoring: "true"`.
          ### Elasticsearch Operator
          The Elasticsearch Operator is responsible for orchestrating and managing cluster logging's Elasticsearch cluster.  This
          operator must be deployed to the global operator group namespace
          ### Memory Considerations
          Elasticsearch is a memory intensive application.  Cluster Logging will specify that each Elasticsearch node needs
          16G of memory for both request and limit unless otherwise defined in the ClusterLogging custom resource. The initial
          set of OKD nodes may not be large enough to support the Elasticsearch cluster.  Additional OKD nodes must be added
          to the OKD cluster if you desire to run with the recommended(or better) memory. Each ES node can operate with a
          lower memory setting though this is not recommended for production deployments.
        displayName: Cluster Logging
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc
        version: 4.2.18-202002031246
      name: 4.2-s390x
    - currentCSV: clusterlogging.4.3.1-202002032140
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "logging.openshift.io/v1",
                "kind": "ClusterLogging",
                "metadata": {
                  "name": "instance",
                  "namespace": "openshift-logging"
                },
                "spec": {
                  "managementState": "Managed",
                  "logStore": {
                    "type": "elasticsearch",
                    "elasticsearch": {
                      "nodeCount": 3,
                      "redundancyPolicy": "SingleRedundancy",
                      "storage": {
                        "storageClassName": "gp2",
                        "size": "200G"
                      }
                    }
                  },
                  "visualization": {
                    "type": "kibana",
                    "kibana": {
                      "replicas": 1
                    }
                  },
                  "curation": {
                    "type": "curator",
                    "curator": {
                      "schedule": "30 3 * * *"
                    }
                  },
                  "collection": {
                    "logs": {
                      "type": "fluentd",
                      "fluentd": {}
                    }
                  }
                }
              },
              {
                "apiVersion": "logging.openshift.io/v1alpha1",
                "kind": "LogForwarding",
                "metadata": {
                  "name": "instance",
                  "namespace": "openshift-logging"
                },
                "spec": {
                  "outputs": [
                    {
                      "name": "clo-default-output-es",
                      "type": "elasticsearch",
                      "endpoint": "elasticsearch.openshift-logging.svc:9200",
                      "secret": {
                        "name": "elasticsearch"
                      }
                    }
                  ],
                  "pipelines": [
                    {
                      "name": "clo-default-app-pipeline",
                      "inputSource": "logs.app",
                      "outputRefs": ["clo-managaged-output-es"]
                    },
                    {
                      "name": "clo-default-infra-pipeline",
                      "inputSource": "logs.app",
                      "outputRefs": ["clo-managaged-output-es"]
                    }
                  ]
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-cluster-logging-operator@sha256:cc5ca9dddeff2478fe2dd81d81758a61edf3099ba428d4fa5c21c51d460ca535
          createdAt: "2018-08-01T08:00:00Z"
          description: The Cluster Logging Operator for OKD provides a means for configuring
            and managing your aggregated logging stack.
          olm.skipRange: '>=4.2.0 <4.3.0'
          support: AOS Logging
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Cluster Logging instance
            displayName: Cluster Logging
            kind: ClusterLogging
            name: clusterloggings.logging.openshift.io
            version: v1
          - description: Log forwarding spec to define destinations for specific log
              sources
            displayName: Log Forwarding
            kind: LogForwarding
            name: logforwardings.logging.openshift.io
            version: v1alpha1
          - description: Log Collector spec to define log collection
            displayName: Log Collector
            kind: Collector
            name: collectors.logging.openshift.io
            version: v1alpha1
        description: |
          # Cluster Logging
          The Cluster Logging Operator orchestrates and manages the aggregated logging stack as a cluster-wide service.

          ##Features
          * **Create/Destroy**: Launch and create an aggregated logging stack to support the entire OKD cluster.
          * **Simplified Configuration**: Configure your aggregated logging cluster's structure like components and end points easily.

          ## Prerequisites and Requirements
          ### Cluster Logging Namespace
          Cluster logging and the Cluster Logging Operator is only deployable to the **openshift-logging** namespace. This namespace
          must be explicitly created by a cluster administrator (e.g. `oc create ns openshift-logging`). To enable metrics
          service discovery add namespace label `openshift.io/cluster-monitoring: "true"`.

          For additional installation documentation see [Deploying cluster logging](https://docs.openshift.com/container-platform/4.1/logging/efk-logging-deploying.html)
          in the OpenShift product documentation.

          ### Elasticsearch Operator
          The Elasticsearch Operator is responsible for orchestrating and managing cluster logging's Elasticsearch cluster.  This
          operator must be deployed to the global operator group namespace
          ### Memory Considerations
          Elasticsearch is a memory intensive application.  Cluster Logging will specify that each Elasticsearch node needs
          16G of memory for both request and limit unless otherwise defined in the ClusterLogging custom resource. The initial
          set of OKD nodes may not be large enough to support the Elasticsearch cluster.  Additional OKD nodes must be added
          to the OKD cluster if you desire to run with the recommended(or better) memory. Each ES node can operate with a
          lower memory setting though this is not recommended for production deployments.
        displayName: Cluster Logging
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc
        version: 4.3.1-202002032140
      name: "4.3"
    - currentCSV: clusterlogging.4.1.31-202001140447
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
                {
                  "apiVersion": "logging.openshift.io/v1",
                  "kind": "ClusterLogging",
                  "metadata": {
                    "name": "instance",
                    "namespace": "openshift-logging"
                   },
                  "spec": {
                    "managementState": "Managed",
                    "logStore": {
                      "type": "elasticsearch",
                      "elasticsearch": {
                        "nodeCount": 3,
                        "redundancyPolicy": "SingleRedundancy",
                        "storage": {
                          "storageClassName": "gp2",
                          "size": "200G"
                         }
                       }
                    },
                    "visualization": {
                      "type": "kibana",
                      "kibana": {
                        "replicas": 1
                      }
                    },
                    "curation": {
                      "type": "curator",
                      "curator": {
                        "schedule": "30 3 * * *"
                      }
                    },
                    "collection": {
                      "logs": {
                        "type": "fluentd",
                        "fluentd": {}
                      }
                    }
                  }
                }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-cluster-logging-operator@sha256:0edf56e6780cd048c6cde3626150fe7639e229fc9f29a8db65763f726083124e
          createdAt: "2018-08-01T08:00:00Z"
          description: The Cluster Logging Operator for OKD provides a means for configuring
            and managing your aggregated logging stack.
          olm.skipRange: '>=4.1.0 <4.1.31-202001140447'
          support: AOS Logging
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Cluster Logging instance
            displayName: Cluster Logging
            kind: ClusterLogging
            name: clusterloggings.logging.openshift.io
            version: v1
        description: |
          # Cluster Logging
          The Cluster Logging Operator orchestrates and manages the aggregated logging stack as a cluster-wide service.

          ##Features
          * **Create/Destroy**: Launch and create an aggregated logging stack to support the entire OKD cluster.
          * **Simplified Configuration**: Configure your aggregated logging cluster's structure like components and end points easily.

          ## Prerequisites and Requirements
          ### Cluster Logging Namespace
          Cluster logging and the Cluster Logging Operator is only deployable to the **openshift-logging** namespace. This namespace
          must be explicitly created by a cluster administrator (e.g. `oc create ns openshift-logging`)
          ### Elasticsearch Operator
          The Elasticsearch Operator is responsible for orchestrating and managing cluster logging's Elasticsearch cluster.  This
          operator must be deployed to the global operator group namespace
          ### Memory Considerations
          Elasticsearch is a memory intensive application.  Cluster Logging will specify that each Elasticsearch node needs
          16G of memory for both request and limit unless otherwise defined in the ClusterLogging custom resource. The initial
          set of OKD nodes may not be large enough to support the Elasticsearch cluster.  Additional OKD nodes must be added
          to the OKD cluster if you desire to run with the recommended(or better) memory. Each ES node can operate with a
          lower memory setting though this is not recommended for production deployments.
        displayName: Cluster Logging
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc
        version: 4.1.31-202001140447
      name: preview
    defaultChannel: "4.3"
    packageName: cluster-logging
    provider:
      name: Red Hat, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: IBM
      provider-url: ""
    name: ibmcloud-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ibmcloud-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ibmcloud-operator.v0.1.3
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion": "ibmcloud.ibm.com/v1alpha1", "kind": "Service",
            "metadata": {"name": "mytranslator"}, "spec": {"serviceClass": "language-translator",
            "plan": "lite"}}, {"apiVersion": "ibmcloud.ibm.com/v1alpha1", "kind":
            "Binding", "metadata": {"name": "mytranslator-binding"}, "spec": {"serviceName":
            "mytranslator"}}]'
          capabilities: Basic Install
          categories: Cloud Provider
          certified: "false"
          containerImage: cloudoperators/ibmcloud-operator:0.1.3
          createdAt: "2019-12-19T15:51:23Z"
          description: The IBM Cloud Operator provides a Kubernetes CRD-Based API
            to manage the lifecycle of IBM public cloud services.
          repository: https://github.com/IBM/cloud-operators
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an instance of a Service resource on IBM Cloud.
            displayName: Service
            kind: Service
            name: services.ibmcloud.ibm.com
            version: v1alpha1
          - description: Represents an instance of a service binding resource on IBM
              Cloud. A Binding creates a secret with the service instance credentials.
            displayName: Binding
            kind: Binding
            name: bindings.ibmcloud.ibm.com
            version: v1alpha1
        description: "The IBM Cloud Operator provides a simple Kubernetes CRD-Based
          API to provision and bind  IBM public cloud services on your Kubernetes
          cluster. With this operator,  you can simply provide service and binding
          custom resources as part of your Kubernetes  application templates and let
          the operator reconciliation logic ensure that the required  services and
          credentials are automatically created and available for your application.\nThe
          IBM Cloud Operator is currently in preview. It will get updated as we release
          new versions of the [upstream repository](https://github.com/IBM/cloud-operators).\n##
          Features\n* **Service Provisioning** - supports provisioning for any service
          and plan available in the IBM Cloud catalog.\n* **Bindings Management**
          - automatically creates secrets with the credentials to bind to any provisioned
          service.\n## Requirements\nThe operator can be installed on any OLM-enabled
          Kubernetes cluster with version >= 1.11.  You need an [IBM Cloud account](https://cloud.ibm.com/registration)
          and the  [IBM Cloud CLI](https://cloud.ibm.com/docs/cli?topic=cloud-cli-getting-started).
          You need also to have the [kubectl CLI](https://kubernetes.io/docs/tasks/tools/install-kubectl/)
          \  already configured to access your cluster. Before installing the operator,
          you need to login to  your IBM cloud account with the IBM Cloud CLI:\n\n
          \   ibmcloud login\n\nand set a default target environment for your resources
          with the command:\n\n    ibmcloud target --cf -g default\n\nThis will use
          the IBM Cloud resource group `default`. To specify a different resource
          group, use the following command:\n\n    ibmcloud target -g <resource-group>\n\nyou
          can then configure the operator running the following script:\n\n    curl
          -sL https://raw.githubusercontent.com/IBM/cloud-operators/master/hack/config-operator.sh
          | bash \n\nThe script above creates an IBM Cloud API Key and stores it in
          a Kubernetes secret that can be accessed by the operator, and it sets defaults
          such as the resource group and region  used to provision IBM Cloud Services.
          You can always override the defaults in the `Service` custom resource. If
          you prefer to create the secret and the defaults manually, consult the [IBM
          Cloud Operator documentation](https://github.com/IBM/cloud-operators).\n##
          Using the IBM Cloud Operator\nYou can create an instance of an IBM public
          cloud service using the following custom resource:\n\n    apiVersion: ibmcloud.ibm.com/v1alpha1\n
          \   kind: Service\n    metadata:\n      name: myservice\n    spec:\n      plan:
          <PLAN>\n      serviceClass: <SERVICE_CLASS>\n\nto find the value for `<SERVICE_CLASS>`,
          you can list the names of all IBM public cloud  services with the command:\n\n
          \   ibmcloud catalog service-marketplace\n\nonce you find the `<SERVICE_CLASS>`
          name, you can list the available plans to select a `<PLAN>` with the command:\n\n
          \   ibmcloud catalog service <SERVICE_CLASS> | grep plan\n\nAfter creating
          a service, you can find its status with:\n\n    kubectl get services.ibmcloud
          \n    NAME           STATUS   AGE\n    myservice      Online   12s\n\nYou
          can bind to a service with name `myservice` using the following custom resource:\n\n
          \   apiVersion: ibmcloud.ibm.com/v1alpha1\n    kind: Binding\n    metadata:\n
          \     name: mybinding\n    spec:\n      serviceName: myservice\n\nTo find
          the status of your binding, you can run the command:\n\n    kubectl get
          bindings.ibmcloud \n    NAME                 STATUS   AGE\n    mybinding
          \           Online   25s\n\nA `Binding` generates a secret with the same
          name as the binding resource and  contains service credentials that can
          be consumed by your application.\n\n    kubectl get secrets\n    NAME                       TYPE
          \                                 DATA   AGE\n    mybinding                  Opaque
          \                               6      102s\n\nFor additional configuration
          options, samples and more information on using the operator, consult  the
          [IBM Cloud Operator documentation](https://github.com/IBM/cloud-operators).\n"
        displayName: IBM Cloud Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: IBM
        version: 0.1.3
      name: alpha
    defaultChannel: alpha
    packageName: ibmcloud-operator
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: 3scale-community-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/3scale-community-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: 3scale-community-operator.v0.3.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager"},"spec":{"wildcardDomain":"example.com"}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager-ha"},"spec":{"highAvailability":{"enabled":true},"wildcardDomain":"example.com"}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager-s3"},"spec":{"system":{"fileStorage":{"amazonSimpleStorageService":{"awsBucket":"\u003cbucket-name\u003e","awsCredentialsSecret":{"name":"\u003ccredentials-secret-name\u003e"},"awsRegion":"\u003cregion\u003e"}}},"wildcardDomain":"\u003cdesired-domain\u003e"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"API","metadata":{"labels":{"environment":"testing"},"name":"example-api"},"spec":{"description":"api01","integrationMethod":{"apicastHosted":{"apiTestGetRequest":"/","authenticationSettings":{"credentials":{"apiKey":{"authParameterName":"user-key","credentialsLocation":"headers"}},"errors":{"authenticationFailed":{"contentType":"text/plain;
            charset=us-ascii","responseBody":"Authentication failed","responseCode":403},"authenticationMissing":{"contentType":"text/plain;
            charset=us-ascii","responseBody":"Authentication Missing","responseCode":403}},"hostHeader":"","secretToken":"MySecretTokenBetweenApicastAndMyBackend_1237120312"},"mappingRulesSelector":{"matchLabels":{"api":"api01"}},"privateBaseURL":"https://echo-api.3scale.net:443"}}}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Binding","metadata":{"name":"example-binding"},"spec":{"APISelector":{"matchLabels":{"environment":"testing"}},"credentialsRef":{"name":"ecorp-tenant-secret"}}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Limit","metadata":{"labels":{"api":"api01"},"name":"plan01-metric01-day-10"},"spec":{"description":"Limit
            for metric01 in plan01","maxValue":10,"metricRef":{"name":"metric01"},"period":"day"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"MappingRule","metadata":{"labels":{"api":"api01"},"name":"metric01-get-path01"},"spec":{"increment":1,"method":"GET","metricRef":{"name":"metric01"},"path":"/path01"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Metric","metadata":{"labels":{"api":"api01"},"name":"metric01"},"spec":{"description":"metric01","incrementHits":false,"unit":"hit"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Plan","metadata":{"labels":{"api":"api01"},"name":"example-plan"},"spec":{"approvalRequired":false,"costs":{"costMonth":0,"setupFee":0},"default":true,"limitSelector":{"matchLabels":{"plan":"plan01"}},"trialPeriod":0}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Tenant","metadata":{"name":"example-tenant"},"spec":{"email":"admin@example.com","masterCredentialsRef":{"name":"system-seed"},"organizationName":"Example.com","passwordCredentialsRef":{"name":"ecorp-admin-secret"},"systemMasterUrl":"https://master.example.com","tenantSecretRef":{"name":"ecorp-tenant-secret","namespace":"operator-test"},"username":"admin"}}]'
          capabilities: Deep Insights
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/3scale/3scale-operator:v0.3.0
          createdAt: "2019-05-30 22:40:00"
          description: Install 3scale and publish/manage your API
          repository: https://github.com/3scale/3scale-operator
          support: Red Hat, Inc.
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: API Manager
            displayName: API Manager
            kind: APIManager
            name: apimanagers.apps.3scale.net
            version: v1alpha1
          - description: API
            displayName: API
            kind: API
            name: apis.capabilities.3scale.net
            version: v1alpha1
          - description: Binding
            displayName: Binding
            kind: Binding
            name: bindings.capabilities.3scale.net
            version: v1alpha1
          - description: Limit
            displayName: Limit
            kind: Limit
            name: limits.capabilities.3scale.net
            version: v1alpha1
          - description: MappingRule
            displayName: MappingRule
            kind: MappingRule
            name: mappingrules.capabilities.3scale.net
            version: v1alpha1
          - description: Metric
            displayName: Metric
            kind: Metric
            name: metrics.capabilities.3scale.net
            version: v1alpha1
          - description: Plan
            displayName: Plan
            kind: Plan
            name: plans.capabilities.3scale.net
            version: v1alpha1
          - description: Tenant
            displayName: Tenant
            kind: Tenant
            name: tenants.capabilities.3scale.net
            version: v1alpha1
        description: |
          The 3scale community Operator creates and maintains the Red Hat 3scale API Management - Community version on [OpenShift](https://www.openshift.com/) in various deployment configurations.

          [3scale API Management](https://www.redhat.com/en/technologies/jboss-middleware/3scale) makes it easy to manage your APIs.
          Share, secure, distribute, control, and monetize your APIs on an infrastructure platform built for performance, customer control, and future growth.

          ### Supported Features
          * **Installer** A way to install a 3scale API Management solution, providing configurability options at the time of installation
          * **Capabilities** Ability to define 3scale API definitions and set them into a 3scale API Management solution

          ### Upgrading your installation
          Currently upgrading feature is not available.

          ### Documentation
          Documentation can be found on our [website](https://github.com/3scale/3scale-operator/blob/v0.3.0/doc/user-guide.md).

          ### Getting help
          If you encounter any issues while using 3scale community operator, you can create an issue on our [website](https://github.com/3scale/3scale-operator) for bugs, enhancements, or other requests.

          ### Contributing
          You can contribute by:

          * Raising any issues you find using 3scale community Operator
          * Fixing issues by opening [Pull Requests](https://github.com/3scale/3scale-operator/pulls)
          * Improving [documentation](https://github.com/3scale/3scale-operator/blob/v0.3.0/doc/user-guide.md)
          * Talking about 3scale community Operator

          All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/3scale-operator/issues).

          ### License
          3scale community Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/3scale-operator/blob/master/LICENSE)
        displayName: 3scale
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.3.0
      name: threescale-2.6
    - currentCSV: 3scale-community-operator.v0.4.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager"},"spec":{"wildcardDomain":"example.com"}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager-ha"},"spec":{"highAvailability":{"enabled":true},"wildcardDomain":"example.com"}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager-s3"},"spec":{"system":{"fileStorage":{"amazonSimpleStorageService":{"awsBucket":"\u003cbucket-name\u003e","awsCredentialsSecret":{"name":"\u003ccredentials-secret-name\u003e"},"awsRegion":"\u003cregion\u003e"}}},"wildcardDomain":"\u003cdesired-domain\u003e"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"API","metadata":{"labels":{"environment":"testing"},"name":"example-api"},"spec":{"description":"api01","integrationMethod":{"apicastHosted":{"apiTestGetRequest":"/","authenticationSettings":{"credentials":{"apiKey":{"authParameterName":"user-key","credentialsLocation":"headers"}},"errors":{"authenticationFailed":{"contentType":"text/plain;
            charset=us-ascii","responseBody":"Authentication failed","responseCode":403},"authenticationMissing":{"contentType":"text/plain;
            charset=us-ascii","responseBody":"Authentication Missing","responseCode":403}},"hostHeader":"","secretToken":"MySecretTokenBetweenApicastAndMyBackend_1237120312"},"mappingRulesSelector":{"matchLabels":{"api":"api01"}},"privateBaseURL":"https://echo-api.3scale.net:443"}}}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Binding","metadata":{"name":"example-binding"},"spec":{"APISelector":{"matchLabels":{"environment":"testing"}},"credentialsRef":{"name":"ecorp-tenant-secret"}}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Limit","metadata":{"labels":{"api":"api01"},"name":"plan01-metric01-day-10"},"spec":{"description":"Limit
            for metric01 in plan01","maxValue":10,"metricRef":{"name":"metric01"},"period":"day"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"MappingRule","metadata":{"labels":{"api":"api01"},"name":"metric01-get-path01"},"spec":{"increment":1,"method":"GET","metricRef":{"name":"metric01"},"path":"/path01"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Metric","metadata":{"labels":{"api":"api01"},"name":"metric01"},"spec":{"description":"metric01","incrementHits":false,"unit":"hit"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Plan","metadata":{"labels":{"api":"api01"},"name":"example-plan"},"spec":{"approvalRequired":false,"costs":{"costMonth":0,"setupFee":0},"default":true,"limitSelector":{"matchLabels":{"plan":"plan01"}},"trialPeriod":0}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Tenant","metadata":{"name":"example-tenant"},"spec":{"email":"admin@example.com","masterCredentialsRef":{"name":"system-seed"},"organizationName":"Example.com","passwordCredentialsRef":{"name":"ecorp-admin-secret"},"systemMasterUrl":"https://master.example.com","tenantSecretRef":{"name":"ecorp-tenant-secret","namespace":"operator-test"},"username":"admin"}}]'
          capabilities: Full Lifecycle
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/3scale/3scale-operator:v0.4.0
          createdAt: "2019-05-30T22:40:00Z"
          description: 3scale Operator to provision 3scale and publish/manage API
          repository: https://github.com/3scale/3scale-operator
          support: Red Hat, Inc.
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: API Manager
            displayName: API Manager
            kind: APIManager
            name: apimanagers.apps.3scale.net
            version: v1alpha1
          - description: API
            displayName: API
            kind: API
            name: apis.capabilities.3scale.net
            version: v1alpha1
          - description: Binding
            displayName: Binding
            kind: Binding
            name: bindings.capabilities.3scale.net
            version: v1alpha1
          - description: Limit
            displayName: Limit
            kind: Limit
            name: limits.capabilities.3scale.net
            version: v1alpha1
          - description: MappingRule
            displayName: MappingRule
            kind: MappingRule
            name: mappingrules.capabilities.3scale.net
            version: v1alpha1
          - description: Metric
            displayName: Metric
            kind: Metric
            name: metrics.capabilities.3scale.net
            version: v1alpha1
          - description: Plan
            displayName: Plan
            kind: Plan
            name: plans.capabilities.3scale.net
            version: v1alpha1
          - description: Tenant
            displayName: Tenant
            kind: Tenant
            name: tenants.capabilities.3scale.net
            version: v1alpha1
        description: |
          The 3scale community Operator creates and maintains the Red Hat 3scale API Management - Community version on [OpenShift](https://www.openshift.com/) in various deployment configurations.

          [3scale API Management](https://www.redhat.com/en/technologies/jboss-middleware/3scale) makes it easy to manage your APIs.
          Share, secure, distribute, control, and monetize your APIs on an infrastructure platform built for performance, customer control, and future growth.

          ### Supported Features
          * **Installer** A way to install a 3scale API Management solution, providing configurability options at the time of installation
          * **Upgrade** Upgrade from previously installed 3scale API Management solution
          * **Reconcilliation** Tunable CRD parameters after 3scale API Management solution is installed
          * **Capabilities** Ability to define 3scale API definitions and set them into a 3scale API Management solution

          ### Documentation
          Documentation can be found on our [website](https://github.com/3scale/3scale-operator/blob/v0.4.0/doc/user-guide.md).

          ### Getting help
          If you encounter any issues while using 3scale operator, you can create an issue on our [Github repo](https://github.com/3scale/3scale-operator) for bugs, enhancements, or other requests.

          ### Contributing
          You can contribute by:

          * Raising any issues you find using 3scale Operator
          * Fixing issues by opening [Pull Requests](https://github.com/3scale/3scale-operator/pulls)
          * Improving [documentation](https://github.com/3scale/3scale-operator/blob/v0.4.0/doc/user-guide.md)
          * Talking about 3scale Operator

          All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/3scale-operator/issues).

          ### License
          3scale Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/3scale-operator/blob/master/LICENSE)
        displayName: 3scale
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.4.0
      name: threescale-2.7
    defaultChannel: threescale-2.7
    packageName: 3scale-community-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: MongoDB, Inc
      provider-url: ""
    name: mongodb-enterprise
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/mongodb-enterprise
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: mongodb-enterprise.v1.2.4
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "kind": "MongoDB",
                "apiVersion": "mongodb.com/v1",
                "metadata": {
                    "name": "sample-replica-set"
                },
                "spec": {
                  "version": "4.0.10",
                  "type": "ReplicaSet",
                  "project": "my-project",
                  "persistent": false,
                  "members": 3,
                  "credentials": "my-credentials"
                },
                "status": {
                  "version": "4.0.10",
                  "type": "ReplicaSet",
                  "phase": "Running",
                  "members": 3,
                  "link": "https://cloud.mongodb.com/v2/5d35f7cdbf994268c3",
                  "lastTransition": "2019-07-23T09:00:00Z"
                }
              },
              {
                "apiVersion": "mongodb.com/v1",
                "kind": "MongoDB",
                "metadata": {
                    "name": "sample-sharded-cluster"
                },
                "spec": {
                    "configServerCount": 3,
                    "credentials": "my-credentials",
                    "mongodsPerShardCount": 3,
                    "mongosCount": 2,
                    "persistent": false,
                    "project": "my-project",
                    "shardCount": 1,
                    "type": "ShardedCluster",
                    "version": "3.6.8"
                },
                "status": {
                    "lastTransition": "2019-07-23T09:00:00Z",
                    "phase": "Running"
                }
              },
              {
                "apiVersion": "mongodb.com/v1",
                "kind": "MongoDBUser",
                "metadata": {
                    "name": "sample-user"
                },
                "spec": {
                    "db": "$external",
                    "project": "my-project",
                    "roles": [
                        {
                            "db": "admin",
                            "name": "clusterAdmin"
                        }
                    ],
                    "username": "CN=mms-user-1,OU=cloud,O=MongoDB,L=New York,ST=New York,C=US"
                }
              },
              {
              "apiVersion": "mongodb.com/v1",
              "kind": "MongoDBOpsManager",
              "metadata": {
                "name": "ops-manager"
              },
              "spec": {
                "version": "4.2.0",
                "adminCredentials": "ops-manager-admin",
                "configuration": {
                  "mms.fromEmailAddr": "admin@thecompany.com"
                },
                "applicationDatabase": {
                  "members": 3,
                  "version": "4.2.0",
                  "persistent": true,
                  "type": "ReplicaSet",
                  "podSpec": {
                    "cpu": "0.25"
                  }
                }
              }
            }
            ]
          capabilities: Deep Insights
          categories: Database
          certified: "true"
          containerImage: registry.connect.redhat.com/mongodb/enterprise-operator:1.2.4
          createdAt: "2019-10-03 17:05:22"
          description: The MongoDB Enterprise Kubernetes Operator enables easy deploys
            of MongoDB into Kubernetes clusters, using our management, monitoring
            and backup platforms, Ops Manager and Cloud Manager.
          repository: https://github.com/mongodb/mongodb-enterprise-kubernetes
          support: MongoDB Community Support
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: MongoDB Deployment
            displayName: MongoDB Deployment
            kind: MongoDB
            name: mongodb.mongodb.com
            version: v1
          - description: MongoDB x509 User
            displayName: MongoDB User
            kind: MongoDBUser
            name: mongodbusers.mongodb.com
            version: v1
          - description: MongoDB Ops Manager Alpha
            displayName: MongoDB Ops Manager
            kind: MongoDBOpsManager
            name: opsmanagers.mongodb.com
            version: v1
        description: |
          The MongoDB Enterprise Kubernetes Operator enables easy deploys of MongoDB
          into Kubernetes clusters, using our management, monitoring and backup
          platforms, Ops Manager and Cloud Manager.

          The Operator has alpha support for a containerized Ops Manager with the `MongoDBOpsManager` custom resource.

          ## Before You Start

          To start using the operator you'll need an account in MongoDB Cloud Manager or a MongoDB Ops Manager deployment.

          * [Create a Secret with your OpsManager API key](https://docs.opsmanager.mongodb.com/current/tutorial/install-k8s-operator/#create-credentials)


          * [Create a ConfigMap with your OpsManager project ID and URL](https://docs.opsmanager.mongodb.com/current/tutorial/install-k8s-operator/#create-onprem-project)


          By installing this integration, you will be able to deploy MongoDB instances
          with a single simple command.

          ## Required Parameters

          * `project` - Enter the name of the ConfigMap containing project information


          * `credentials` - Enter the name of the Secret containing your OpsManager credentials


          ## Supported MongoDB Deployment Types ##


          * Standalone: An instance of mongod that is running as a single server and
          not as part of a replica set, this is, it does not do any kind of
          replication.


          * Replica Set: A replica set in MongoDB is a group of mongod processes that
          maintain the same data set. Replica sets provide redundancy and high
          availability, and are the basis for all production deployments. This section
          introduces replication in MongoDB as well as the components and architecture
          of replica sets. The section also provides tutorials for common tasks
          related to replica sets.


          * Sharded Cluster: The set of nodes comprising a sharded MongoDB deployment.
          A sharded cluster consists of config servers, shards, and one or more mongos
          routing processes. Sharding is a A database architecture that partitions
          data by key ranges and distributes the data among two or more database
          instances. Sharding enables horizontal scaling.

          ## Security ##

          The operator can enable TLS for all traffic between servers and also between
          clients and servers. Before enabling `security.tls.enabled` to `true` you should
          create your certificates or you can leave the operator to create all the
          certificates for you. For more information, please read the official MongoDB
          Kubernetes Operator [docs](https://docs.mongodb.com/kubernetes-operator/stable/).
        displayName: MongoDB
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: MongoDB, Inc
        version: 1.2.4
      name: stable
    defaultChannel: stable
    packageName: mongodb-enterprise
    provider:
      name: MongoDB, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Percona
      provider-url: ""
    name: percona-server-mongodb-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/percona-server-mongodb-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: percona-server-mongodb-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"psmdb.percona.com/v1","kind":"PerconaServerMongoDB","metadata":{"name":"my-cluster-name"},"spec":{"allowUnsafeConfigurations":true,"backup":{"coordinator":{"affinity":{"antiAffinityTopologyKey":"kubernetes.io/hostname"},"enableClientsLogging":true,"resources":{"limits":{"cpu":"1","memory":"1Gi"},"requests":{"cpu":"100m","memory":"0.1Gi","storage":"1Gi"}}},"debug":true,"enabled":true,"image":"registry.connect.redhat.com/percona/percona-server-mongodb-operator-containers:1.0.0-backup","restartOnFailure":true,"storages":null,"tasks":null},"image":"registry.connect.redhat.com/percona/percona-server-mongodb-operator-containers:1.0.0-mongod4.0.9","imagePullPolicy":"Always","mongod":{"net":{"hostPort":0,"port":27017},"operationProfiling":{"mode":"slowOp","rateLimit":100,"slowOpThresholdMs":100},"security":{"redactClientLogData":false},"setParameter":{"ttlMonitorSleepSecs":60,"wiredTigerConcurrentReadTransactions":128,"wiredTigerConcurrentWriteTransactions":128},"storage":{"engine":"wiredTiger","inMemory":{"engineConfig":{"inMemorySizeRatio":0.9}},"mmapv1":{"nsSize":16,"smallfiles":false},"wiredTiger":{"collectionConfig":{"blockCompressor":"snappy"},"engineConfig":{"cacheSizeRatio":0.5,"directoryForIndexes":false,"journalCompressor":"snappy"},"indexConfig":{"prefixCompression":true}}}},"pause":false,"pmm":{"enabled":false,"image":"perconalab/pmm-client:1.17.1","serverHost":"monitoring-service"},"replsets":[{"affinity":{"antiAffinityTopologyKey":"kubernetes.io/hostname"},"arbiter":{"enabled":false},"expose":{"enabled":false},"name":"rs0","resources":{"requests":{"cpu":"1","memory":"1Gi"}},"size":3,"volumeSpec":{"persistentVolumeClaim":{"resources":{"requests":{"storage":"3Gi"}}}}}],"secrets":{"users":"my-cluster-name-secrets"}}},{"apiVersion":"psmdb.percona.com/v1","kind":"PerconaServerMongoDBBackup","metadata":{"name":"backup1"},"spec":{"psmdbCluster":"my-cluster-name","storageName":"minio"}},{"apiVersion":"psmdb.percona.com/v1","kind":"PerconaServerMongoDBRestore","metadata":{"name":"restore1"},"spec":{"backupName":"backup1","clusterName":"my-cluster-name"}}]'
          capabilities: Full Lifecycle
          categories: Database
          certified: "true"
          containerImage: registry.connect.redhat.com/percona/percona-server-mongodb-operator:1.0.0
          createdAt: "2019-05-21 20:35:26"
          description: Percona Server for MongoDB Operator manages the lifecycle of
            Percona Server for MongoDB replica sets
          repository: https://github.com/percona/percona-server-mongodb-operator
          support: Percona
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Instance of a Percona Server for MongoDB replica set
            displayName: PerconaServerMongoDB
            kind: PerconaServerMongoDB
            name: perconaservermongodbs.psmdb.percona.com
            version: v1
          - description: Instance of a Percona Server for MongoDB Backup
            displayName: PerconaServerMongoDBBackup
            kind: PerconaServerMongoDBBackup
            name: perconaservermongodbbackups.psmdb.percona.com
            version: v1
          - description: Instance of a Percona Server for MongoDB Restore
            displayName: PerconaServerMongoDBRestore
            kind: PerconaServerMongoDBRestore
            name: perconaservermongodbrestores.psmdb.percona.com
            version: v1
        description: |2-

          ## Percona is Cloud Native

          The Percona Kubernetes Operator for Percona Server for MongoDB automates the creation, alteration, or deletion of nodes in your cluster environment. It can be used to instantiate a new database cluster or to scale an existing database cluster. The Operator contains all necessary Kubernetes settings to provide a proper and consistent Percona Server for MongoDB replica set.

          Consult the [documentation](https://percona.github.io/percona-server-mongodb-operator/) on the Percona Kubernetes Operator for Percona Server for MongoDB for complete details on capabilities and options.

          ### Supported Features

          * **Scale Your Cluster** â€“ change the `size` parameter to [add or remove members](https://percona.github.io/percona-server-mongodb-operator/install/scaling) of the replica set. Three is the minimum recommended size for a functioning replica set.

          * **Add Monitoring** - [Percona Monitoring and Management (PMM) can be easily deployed](https://percona.github.io/percona-server-mongodb-operator/install/monitoring) to monitor your Percona Server for MongoDB replica set(s). The recommended installation process uses Helm, the package manager for Kubernetes.

          * **Automate Your Backups** â€“ [configure automatic backups](https://percona.github.io/percona-server-mongodb-operator/configure/backups) to run on a scheduled basis or run an on-demand backup at any time. Backups are performed using Percona Backup for MongoDB (PBM) and can be stored on local PVs or in any S3-compatible cloud storage provider.

          ### Common Configurations

          * **Set Member as Arbiter** - [Set up a replica set which contains an arbiter](https://percona.github.io/percona-server-mongodb-operator/configure/arbiter), which participates in elections but does not store any data.  This reduces storage costs while helping maintain replica set integrity.

          * **Expose Members Outside K8S** - [by appropriately configuring the ServiceType](https://percona.github.io/percona-server-mongodb-operator/configure/expose) you can expose replica set members outside of Kubernetes or provide statically assigned IP addresses.

          * **Utilize Local Storage Options** - [with support for Local Storage you can mount existing data directories](https://percona.github.io/percona-server-mongodb-operator/configure/storage) into your replica set managed by Kubernetes or utilize high performance hardware for local storage rather than network storage for your database.

          ### Before You Start

          Add the PSMDB user `Secret` to Kubernetes. User information must be placed in the data section of the `secrets.yaml`
          file with Base64-encoded logins and passwords for the user accounts.

          Below is a sample `secrets.yaml` file for the correct formatting.

          apiVersion: v1
          kind: Secret
          metadata:
            name: my-cluster-name-secrets
          type: Opaque
          data:
            MONGODB_BACKUP_USER: YmFja3Vw
            MONGODB_BACKUP_PASSWORD: YmFja3VwMTIzNDU2
            MONGODB_CLUSTER_ADMIN_USER: Y2x1c3RlckFkbWlu
            MONGODB_CLUSTER_ADMIN_PASSWORD: Y2x1c3RlckFkbWluMTIzNDU2
            MONGODB_CLUSTER_MONITOR_USER: Y2x1c3Rlck1vbml0b3I=
            MONGODB_CLUSTER_MONITOR_PASSWORD: Y2x1c3Rlck1vbml0b3IxMjM0NTY=
            MONGODB_USER_ADMIN_USER: dXNlckFkbWlu
            MONGODB_USER_ADMIN_PASSWORD: dXNlckFkbWluMTIzNDU2
            PMM_SERVER_USER: cG1t
            PMM_SERVER_PASSWORD: c3VwYXxefHBheno=
        displayName: Percona Server for MongoDB Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Percona
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: percona-server-mongodb-operator-certified
    provider:
      name: Percona
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: serverless-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/serverless-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: serverless-operator.v1.4.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "operator.knative.dev/v1alpha1",
                "kind": "KnativeServing",
                "metadata": {
                  "name": "knative-serving"
                },
                "spec": {
                  "config": {
                    "autoscaler": {
                      "container-concurrency-target-default": "100",
                      "container-concurrency-target-percentage": "1.0",
                      "enable-scale-to-zero": "true",
                      "max-scale-up-rate": "10",
                      "panic-threshold-percentage": "200.0",
                      "panic-window": "6s",
                      "panic-window-percentage": "10.0",
                      "scale-to-zero-grace-period": "30s",
                      "stable-window": "60s",
                      "tick-interval": "2s"
                    },
                    "defaults": {
                      "revision-cpu-limit": "1000m",
                      "revision-cpu-request": "400m",
                      "revision-memory-limit": "200M",
                      "revision-memory-request": "100M",
                      "revision-timeout-seconds": "300"
                    },
                    "deployment": {
                      "registriesSkippingTagResolving": "ko.local,dev.local"
                    },
                    "gc": {
                      "stale-revision-create-delay": "24h",
                      "stale-revision-lastpinned-debounce": "5h",
                      "stale-revision-minimum-generations": "1",
                      "stale-revision-timeout": "15h"
                    },
                    "logging": {
                      "loglevel.activator": "info",
                      "loglevel.autoscaler": "info",
                      "loglevel.controller": "info",
                      "loglevel.queueproxy": "info",
                      "loglevel.webhook": "info"
                    },
                    "observability": {
                      "logging.enable-var-log-collection": "false",
                      "metrics.backend-destination": "prometheus"
                    },
                    "tracing": {
                      "backend": "none",
                      "sample-rate": "0.1"
                    }
                  }
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Networking,Integration & Delivery,Cloud Provider,Developer Tools
          certified: "false"
          containerImage: registry.redhat.io/openshift-serverless-1-tech-preview/knative-rhel8-operator@sha256:4a20e629496ef5c693614a600c05a8cf6712d5cf3443aa991fa556338568438e
          createdAt: "2019-07-27T17:00:00Z"
          description: |-
            Provides a collection of API's based on Knative to support deploying and serving
            of serverless applications and functions.
          repository: https://github.com/openshift-knative/serverless-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an installation of a particular version of Knative
              Serving
            displayName: Knative Serving
            kind: KnativeServing
            name: knativeservings.operator.knative.dev
            version: v1alpha1
          - description: Represents an installation of a particular version of Knative
              Serving
            displayName: Knative Serving (obsolete)
            kind: KnativeServing
            name: knativeservings.serving.knative.dev
            version: v1alpha1
          required:
          - description: A list of namespaces in Service Mesh
            displayName: Istio Service Mesh Member Roll
            kind: ServiceMeshMemberRoll
            name: servicemeshmemberrolls.maistra.io
            version: v1
          - description: An Istio control plane installation
            displayName: Istio Service Mesh Control Plane
            kind: ServiceMeshControlPlane
            name: servicemeshcontrolplanes.maistra.io
            version: v1
        description: |-
          The Red Hat Serverless Operator provides a collection of API's to
          install various "serverless" services.

          This is a **[Tech Preview release](https://access.redhat.com/support/offerings/techpreview)!**

          # Knative Serving

          Knative Serving builds on Kubernetes to support deploying and
          serving of serverless applications and functions. Serving is easy
          to get started with and scales to support advanced scenarios. The
          Knative Serving project provides middleware primitives that
          enable:

          - Rapid deployment of serverless containers
          - Automatic scaling up and down to zero
          - Routing and network programming for Istio components
          - Point-in-time snapshots of deployed code and configurations

          ## Prerequisites

          The Serverless Operator's provided APIs such as Knative Serving
          have certain requirements with regards to the size of the underlying
          cluster and a working installation of Service Mesh. See the [installation
          section](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.2/html-single/serverless/index#installing-openshift-serverless)
          of the Serverless documentation for more info.

          ## Further Information

          For documentation on using Knative Serving, see the
          [serving section](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.2/html-single/serverless/index#knative-serving_serverless-architecture) of the
          [Serverless documentation site](https://access.redhat.com/documentation/en-us/openshift_container_platform/4.2/html-single/serverless/index).
        displayName: OpenShift Serverless Operator
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.4.1
      name: techpreview
    defaultChannel: techpreview
    packageName: serverless-operator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat, Inc.
      provider-url: ""
    name: service-binding-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/service-binding-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: service-binding-operator.v0.0.24-214
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"apps.openshift.io/v1alpha1","kind":"ServiceBindingRequest","metadata":{"name":"example-servicebindingrequest"},"spec":{"applicationSelector":{"group":"apps","resource":"deployments","resourceRef":"nodejs-rest-http-crud","version":"v1"},"backingServiceSelector":{"group":"postgresql.example.dev","kind":"Database","resourceRef":"pg-instance","version":"v1alpha1"},"mountPathPrefix":"/var/credentials"}}]'
          capabilities: Basic Install
          categories: Developer Tools, OpenShift Optional, Integration & Delivery
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Expresses intent to bind an operator-backed service with
              a Deployment
            displayName: ServiceBindingRequest
            kind: ServiceBindingRequest
            name: servicebindingrequests.apps.openshift.io
            version: v1alpha1
        description: " The Service Binding Operator enables application developers
          to more easily bind applications together with operator managed backing
          services such as databases, without having to perform manual configuration
          of secrets, configmaps, etc. The Service Binding Operator accomplishes this
          through automatically collecting binding information and sharing with an
          application to bind it with operator managed backing services. The binding
          is performed through a new custom resource called a ServiceBindingRequest.\n###
          Example\nA set of examples, each of which illustrates a usage scenario for
          the Service Binding Operator, is being developed in parallel with the Operator.
          Each example includes documentation and can be run either through the OpenShift
          web console or command line client. The examples are available [here](https://github.com/redhat-developer/service-binding-operator/blob/master/README.md#example-scenarios)\n###
          Documentation\nRefer to the [documentation](https://github.com/redhat-developer/service-binding-operator/blob/master/README.md)\n###
          \ Help\nRaise a ticket for bugs, features and enhancement [here](https://github.com/redhat-developer/service-binding-operator/issues)\n###
          Licence\nService Binding Operator is licensed under [Apache License 2.0](https://github.com/redhat-developer/service-binding-operator/blob/master/LICENSE) "
        displayName: Service Binding Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 0.0.24-214
      name: alpha
    defaultChannel: alpha
    packageName: service-binding-operator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: m88i Labs
      provider-url: ""
    name: nexus-operator-hub
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/nexus-operator-hub
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: nexus-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "apps.m88i.io/v1alpha1",
                "kind": "Nexus",
                "metadata": {
                  "name": "nexus3"
                },
                "spec": {
                  "networking": {
                    "expose": true
                  },
                  "persistence": {
                    "persistent": true,
                    "volumeSize": "10Gi"
                  },
                  "replicas": 1,
                  "resources": {
                    "limits": {
                      "cpu": "2",
                      "memory": "2Gi"
                    },
                    "requests": {
                      "cpu": "1",
                      "memory": "2Gi"
                    }
                  },
                  "useRedHatImage": false
                }
              }
            ]
          capabilities: Basic Install
          categories: Developer Tools
          certified: "false"
          containerImage: quay.io/m88i/nexus-operator:0.1.0
          createdAt: "2019-11-16T13:12:22Z"
          description: Nexus Operator to deploy and manage Nexus 3.x servers
          repository: https://github.com/m88i/nexus-operator
          support: m88i Labs
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Representation of a Nexus 3.x server
            displayName: Nexus
            kind: Nexus
            name: nexus.apps.m88i.io
            version: v1alpha1
        description: |-
          Creates a new Nexus 3.x deployment in a Kubernetes cluster. Will help DevOps to have a quick Nexus application exposed to the world that can be used in a CI/CD process:

          * Deploys a new Nexus 3.x server based on either Community or Red Hat images
          * Creates an [Ingress controller](https://kubernetes.io/docs/concepts/services-networking/ingress/) in Kubernetes (1.14+) environments to expose the application to the world
          * On OpenShift, creates a Route to expose the service outside the cluster

          After installing it, you will have to grab the `admin` user password from the deployed container. There's a file auto generated in `/nexus-data/admin.password`.
          Use `cat` to read the file and view the password. Use it to login for the first time and follow the on screen instructions to have the Nexus server ready for use.

          If you experience any issues or have any ideas for new features, please [file an issue in our Github repository](https://github.com/m88i/nexus-operator/issues).

          *Please note that the operator is an individual work and it's not provided nor supported by Sonatype.*
        displayName: Nexus Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: m88i Labs
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: nexus-operator-hub
    provider:
      name: m88i Labs
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: codeready-workspaces
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/codeready-workspaces
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: crwoperator.v2.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "org.eclipse.che/v1",
                "kind": "CheCluster",
                "metadata": {
                   "name":"codeready-workspaces"
                },
                "spec": {
                   "server": {
                      "cheImageTag": "",
                      "cheFlavor":"codeready",
                      "devfileRegistryImage": "",
                      "pluginRegistryImage": "",
                      "tlsSupport": false,
                      "selfSignedCert": false
                   },
                   "database": {
                      "externalDb": false,
                      "chePostgresHostName": "",
                      "chePostgresPort": "",
                      "chePostgresUser": "",
                      "chePostgresPassword": "",
                      "chePostgresDb": ""
                   },
                   "auth": {
                      "openShiftoAuth": true,
                      "identityProviderImage": "",
                      "externalIdentityProvider": false,
                      "identityProviderURL": "",
                      "identityProviderRealm": "",
                      "identityProviderClientId": ""
                   },
                   "storage": {
                      "pvcStrategy": "per-workspace",
                      "pvcClaimSize": "1Gi",
                      "preCreateSubPaths": true
                   }
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Developer Tools, OpenShift Optional
          certified: "true"
          containerImage: registry.redhat.io/codeready-workspaces/server-operator-rhel8:2.0
          createdAt: "2019-10-29T11:59:59Z"
          description: A Kube-native development solution that delivers portable and
            collaborative developer workspaces in OpenShift.
          repository: https://github.com/eclipse/che-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: CodeReady Workspaces cluster with DB and Auth Server
            displayName: CodeReady Workspaces Cluster
            kind: CheCluster
            name: checlusters.org.eclipse.che
            version: v1
        description: |
          A collaborative Kubernetes-native development solution that delivers OpenShift workspaces and in-browser IDE for rapid cloud application development.
          This operator installs PostgreSQL, Red Hat SSO, and the Red Hat CodeReady Workspaces server, as well as configures all three services.

          ## How to Install

          Press the **Install** button, choose the upgrade strategy, and wait for the **Installed** Operator status.

          When the operator is installed, create a new CR of Kind CheCluster (click the **Create New** button).
          The CR spec contains all defaults (see below).

          You can start using CodeReady Workspaces when the CR status is set to **Available**, and you see a URL to CodeReady Workspaces.

          ## Defaults

          By default, the operator deploys CodeReady Workspaces with:

          * Bundled PostgreSQL and SSO

          * Per-Workspace PVC strategy

          * Auto-generated passwords

          * HTTP mode (non-secure routes)

          * Regular login extended with OpenShift OAuth authentication

          ## Installation Options

          CodeReady Workspaces operator installation options include:

          * Connection to external database and SSO

          * Configuration of default passwords and object names

          * TLS mode

          * PVC strategy (once shared PVC for all workspaces, PVC per workspace, or PVC per volume)

          * Authentication options

          ### External Database and SSO

          To instruct the operator to skip deploying PostgreSQL and SSO and connect to an existing DB and SSO instead:

          * set respective fields to `true` in a custom resource spec

          * provide the operator with connection and authentication details:



            `externalDb: true`


            `chePostgresHostname: 'yourPostgresHost'`


            `chePostgresPort: '5432'`


            `chePostgresUser: 'myuser'`


            `chePostgresPassword: 'mypass'`


            `chePostgresDb: 'mydb'`


            `externalIdentityProvider: true`


            `identityProviderURL: 'https://my-sso.com'`


            `identityProviderRealm: 'myrealm'`


            `identityProviderClientId: 'myClient'`


          ### TLS Mode

          To activate TLS mode, set the respective field in the CR spec to `true` (in the `server` block):


          ```
          tlsSupport: true
          ```

          #### Self-signed Certificates

          To use CodeReady Workspaces with TLS enabled, but the OpenShift router does not use certificates signed by a public authority, you can use self-signed certificates, which the operator can fetch for you (requires cluster-admin privileges):


          ```
          selfSignedCert: true
          ```


          You can also manually create a secret:



          ```
          oc create secret self-signed-certificate generic --from-file=/path/to/certificate/ca.crt -n=$codeReadyNamespace
          ```
        displayName: Red Hat CodeReady Workspaces
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 2.0.0
      name: latest
    - currentCSV: crwoperator.v1.2.2
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion":"org.eclipse.che/v1",
                "kind":"CheCluster",
                "metadata":{
                   "name":"codeready"
                },
                "spec":{
                   "server":{
                      "cheFlavor":"codeready",
                      "tlsSupport":false,
                      "selfSignedCert":false
                   },
                   "database":{
                      "externalDb":false,
                      "chePostgresHostName":"",
                      "chePostgresPort":"",
                      "chePostgresUser":"",
                      "chePostgresPassword":"",
                      "chePostgresDb":""
                   },
                   "auth":{
                      "openShiftoAuth":false,
                      "externalKeycloak":false,
                      "keycloakURL":"",
                      "keycloakRealm":"",
                      "keycloakClientId":""
                   },
                   "storage":{
                      "pvcStrategy":"per-workspace",
                      "pvcClaimSize":"1Gi",
                      "preCreateSubPaths": true
                   }
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Developer Tools
          certified: "true"
          containerImage: registry.redhat.io/codeready-workspaces/server-operator-rhel8:1.2
          createdAt: "2019-06-03T11:59:59Z"
          description: A Kube-native development solution that delivers portable and
            collaborative developer workspaces in OpenShift.
          repository: https://github.com/eclipse/che-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Red Hat CodeReady Workspaces cluster with DB and Auth Server
            displayName: Red Hat CodeReady Workspaces Cluster
            kind: CheCluster
            name: checlusters.org.eclipse.che
            version: v1
        description: |
          A collaborative Kubernetes-native development solution that delivers OpenShift workspaces and in-browser IDE for rapid cloud application development. This operator installs PostgreSQL, Red Hat SSO, and the Red Hat CodeReady Workspaces server, as well as configures all three services.
          ## Pre-Reqs
          In addition to a standard namespaced role, this operator may require a **ClusterRole** that allows the operator service account to:
          * read secrets in openshift-ingress namespace (the operator will get TLS secret in openshift-ingress namespace to extract certificate and add it to Java trust store for Red Hat CodeReady Workspaces server)
          * list, get, create, update, patch, watch and delete oauthclients at a cluster scope (the operator creates oAuthclient and OpenShift v3 identity provider in Keycloak to enable Login With OpenShift in Red Hat CodeReady Workspaces)

          The operator service account will require extra privileges if you enable either `server.selfSignedCerts` or `auth.openShiftoAuth` which are false in CR template by default. After the operator is installed, grant the **codeready-operator** service account such privileges: When **auth.openShiftoAuth** is enabled:

          ```
          oc create clusterrole codeready-operator --resource=oauthclients --verb=get,create,delete,update,list,watch
          oc create clusterrolebinding codeready-operator --clusterrole=codeready-operator --serviceaccount=${NAMESPACE}:codeready-operator
          ```

          When **server.selfSignedCerts** is enabled:
          ```
          oc create role secret-reader --resource=secrets --verb=get -n=openshift-ingress
          oc create rolebinding codeready-operator --role=secret-reader --serviceaccount=${NAMESPACE}:codeready-operator -n=openshift-ingress
          ```

          `${NAMESPACE}` is an OpenShift project where you installed the operator.

          ## How to Install
          Press the **Install** button, choose the upgrade strategy, and wait for the **Installed** Operator status.
          When the operator is installed, create a new CR of Kind CheCluster (click the **Create New** button). The CR spec contains all defaults (see below).
          You can start using Red Hat CodeReady Workspaces when the CR status is set to **Available**, and you see a URL to Red Hat CodeReady Workspaces.
          ## Defaults
          By default, the operator deploys Red Hat CodeReady Workspaces with:
          * Bundled PostgreSQL and Red Hat SSO
          * Per-Workspace PVC strategy
          * Auto-generated passwords
          * HTTP mode (non-secure routes)
          * Regular login (no login with OpenShift)
          ## Installation Options
          Red Hat CodeReady Workspaces operator installation options include:
          * Connection to external database and Keycloak/Red Hat SSO
          * Configuration of default passwords and object names
          * TLS mode
          * PVC strategy (once shared PVC for all workspaces, PVC per workspace, or PVC per volume)
          * Authentication options
          ### External Database and Keycloak
          To instruct the operator to skip deploying PostgreSQL and Red Hat SSO and connect to an existing DB and Red Hat SSO or Keycloak instead:
          * set respective fields to `true` in a custom resource spec * provide the operator with connection and authentication details:


          `externalDb: true`

          `chePostgresHostname: 'yourPostgresHost'`

          `chePostgresPort: '5432'`

          `chePostgresUser: 'myuser'`

          `chePostgresPassword: 'mypass'`

          `chePostgresDb: 'mydb'`

          `externalKeycloak: true`

          `keycloakURL: 'https://my-keycloak.com'`

          `keycloakRealm: 'myrealm'`

          `keycloakClientId: 'myClient'`

          ### TLS Mode
          To activate TLS mode, set the respective field in the CR spec to `true` (in the `server` block):

          ``` tlsSupport: true ```
          #### Self-signed Certificates
          To use Red Hat CodeReady Workspaces with TLS enabled, but the OpenShift router does not use certificates signed by a public authority, you can use self-signed certificates, which the operator can fetch for you (requires cluster-admin privileges):

          ``` selfSignedCert: true ```

          You can also manually create a secret:


          ``` oc create secret self-signed-certificate generic --from-file=/path/to/certificate/ca.crt -n=$codeReadyNamespace ```
        displayName: Red Hat CodeReady Workspaces
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.2.2
      name: previous
    defaultChannel: latest
    packageName: codeready-workspaces
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat, Inc.
      provider-url: ""
    name: maistraoperator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/maistraoperator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: maistraoperator.v1.0.6
      currentCSVDesc:
        annotations:
          alm-examples: "[\n  {\n    \"apiVersion\": \"maistra.io/v1\",\n    \"kind\":
            \"ServiceMeshControlPlane\",\n    \"metadata\": {\n      \"name\": \"basic-install\"\n
            \   },\n    \"spec\": {\n      \"istio\": {\n        \"gateways\": {\n
            \         \"istio-egressgateway\": {\n            \"autoscaleEnabled\":
            false\n          },\n          \"istio-ingressgateway\": {\n            \"autoscaleEnabled\":
            false\n          }\n        },\n        \"mixer\": {\n          \"policy\":
            {\n            \"autoscaleEnabled\": false\n          },\n          \"telemetry\":
            {\n            \"autoscaleEnabled\": false\n          }\n        },\n
            \       \"pilot\": {\n          \"autoscaleEnabled\": false,\n          \"traceSampling\":
            100.0\n        },\n        \"kiali\": {\n          \"enabled\": true\n
            \       },\n        \"grafana\": {\n          \"enabled\": true\n        },\n
            \       \"tracing\": {\n          \"enabled\": true,\n          \"jaeger\":
            {\n            \"template\": \"all-in-one\"\n          }\n        }\n
            \     }\n    }\n  },\n  {\n    \"apiVersion\": \"maistra.io/v1\",\n    \"kind\":
            \"ServiceMeshMemberRoll\",\n    \"metadata\": {\n      \"name\": \"default\"\n
            \   },\n    \"spec\": {\n      \"members\": [\n        \"your-project\",\n
            \       \"another-of-your-projects\" \n      ]\n    }\n  }\n]"
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Integration & Delivery
          certified: "false"
          containerImage: maistra/istio-ubi8-operator:1.0.6
          createdAt: 2020-01-29T02:22:02UTC
          description: The Maistra Operator enables you to install, configure, and
            manage an instance of Maistra service mesh. Maistra is based on the open
            source Istio project.
          repository: https://github.com/maistra/istio-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A list of namespaces in Service Mesh
            displayName: Istio Service Mesh Member Roll
            kind: ServiceMeshMemberRoll
            name: servicemeshmemberrolls.maistra.io
            version: v1
          - description: An Istio control plane installation
            displayName: Istio Service Mesh Control Plane
            kind: ServiceMeshControlPlane
            name: servicemeshcontrolplanes.maistra.io
            version: v1
          required:
          - description: A configuration file for a Kiali installation.
            displayName: Kiali
            kind: Kiali
            name: kialis.kiali.io
            version: v1alpha1
          - description: A configuration file for a Jaeger installation.
            displayName: Jaeger
            kind: Jaeger
            name: jaegers.jaegertracing.io
            version: v1
        description: 'Red Hat OpenShift Service Mesh is a platform that provides behavioral
          insight and operational control over the service mesh, providing a uniform
          way to connect, secure, and monitor microservice applications. '
        displayName: Red Hat OpenShift Service Mesh
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.0.6
      name: "1.0"
    defaultChannel: "1.0"
    packageName: maistraoperator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: TriggerMesh, Inc
      provider-url: ""
    name: triggermesh
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/triggermesh
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: triggermesh.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"app.triggermesh.io/v1alpha1","kind":"TriggerMesh","metadata":{"name":"example-triggermesh"},"spec":{"backend":{"baseHost":"app-test.tm.demo.vshn.net","route":{"annotations":{"kubernetes.io/tls-acme":"true"},"enabled":true}},"frontend":{"authApiId":"\u003cREDACTED\u003e","baseHost":"cloud-test.tm.demo.vshn.net","route":{"annotations":{"kubernetes.io/tls-acme":"true"},"enabled":true},"sentryDSN":"https://\u003cREDACTED\u003e@sentry.io/"},"k8sAPI":{"CA":"-----BEGIN
            CERTIFICATE-----\nMIIC6jCCAdKgAwIBAgIBATANBgkqhkiG9w0BAQsFADAmMSQwIgYDVQQDDBtvcGVu\nc2hpZnQtc2lnbmVyQDE1NTQzMDQ3MDIwHhcNMTkwNDAzMTUxODIyWhcNMjQwNDAx\nMTUxODIzWjAmMSQwIgYDVQQDDBtvcGVuc2hpZnQtc2lnbmVyQDE1NTQzMDQ3MDIw\nggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDd/NHyQyCg6a65VywMe3TT\nmuam6XK3gd3iB9vPyTXnzYTx2IEc9vRG9kYmUwu5XE7ZsOXKNaT6Xez1FPhX1FfL\nfKL9MC4n86sFRiT1zE2DfW6ycL6rJ2XaxmAe3HSw5q7FrgH9EWEC3lLPkO4Xygn+\n0T8G3sdBNbPIK8LGLjb1RBeM8Tif1AUABehCY9eb9suY+zzU1/RJ9hY5Y1X4yI4l\n5yotE3KzwL/aphiBiQGXajQLzPg7tFlE5qUuHRgo07Cqsyt9m89gVf8s3s3hfRPe\nttjA5dBnkjfaI8POJp71Nof2rXy2zaDG7QBkILrNozN1PJFH9f6+uCFKkJ8eg/Br\nAgMBAAGjIzAhMA4GA1UdDwEB/wQEAwICpDAPBgNVHRMBAf8EBTADAQH/MA0GCSqG\nSIb3DQEBCwUAA4IBAQCkSB7BiLxBsFIm/JkyoKBupB4MB4GIqNTs+Pxwus0Z2S0U\nfW1QKh745nvNmHkXIaRyKh8dkIqIQDmUFo9FlxiS5zXxAe3F6OYmxAJ3sE24oA1m\nnoyDQ/TlaxLxLFfVo8q3fc/dIYlvyTMeFjoNb22ZClAHgXgwsxJIKhLl7J1Nux4N\nq7nCj1tBff7WGNkzN0Wj7BTMLfCcUpTYXxtRq9gmaXXxWbFLV+B4ViYg1VNJ8VSZ\n/DYgkGOKzO7Y9DFq90gVpat7+XNOGHDZtPP+IiTQZwyONxKT/b1UHJm2Q7iD23Yt\nk302uCcbAC6JcP7+lCmH/X3ZgevgeAe715kwj8Lj\n-----END
            CERTIFICATE-----\n"},"secrets":{"auth0ClientId":"\u003cREDACTED\u003e","auth0ClientSecret":"\u003cREDACTED\u003e","gitHookSecret":"\u003cREDACTED\u003e"}}}]'
          capabilities: Basic Install
          categories: Developer Tools, Streaming & Messaging, Integration & Delivery
          containerImage: gcr.io/triggermesh/operator:v0.0.1
          createdAt: "2019-04-11"
          description: A serverless management platform that runs on Knative. TriggerMesh
            provides continuous delivery of serverless functions, build, auto-scaling
            and eventing capabilities.
          repository: https://github.com/triggermesh/operator
          support: TriggerMesh Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Configures TriggerMesh with Auth0 authentication, GitHub
              secret
            displayName: Running TriggerMesh instance
            kind: TriggerMesh
            name: triggermeshes.app.triggermesh.io
            version: v1alpha1
        description: "The [TriggerMesh](https://triggermesh.com) operator creates
          and maintains the TriggerMesh serverless management platform on Kubernetes.
          It provides an easy to use dashboard to Knative with a backend that simplifies
          functions deployment, event and workflow management.\n\n### Supported Features\n**Knative
          services**:\nKnative services can be deployed from source manifest or by
          specifying a container image or by specifying a git repository.\n\n**Knative
          Build**:\nContainer builds and more generically workflows can be configured
          and deployed.\n\n**Knative Eventing**:\nEvent sources such as GitHub, GitLab
          and a few AWS event sources can be configured and deployed.\n\n### Dependency\nTriggerMesh
          depends on a working Knative installation (Serving, Build/Pipeline and Eventing).\n\nYou
          can install Knative Serving via the community operator:\n- [https://github.com/operator-framework/community-operators/tree/master/community-operators/knative-serving-operator](https://github.com/operator-framework/community-operators/tree/master/community-operators/knative-serving-operator)\n\nYou
          can install Knative eventing via the community operator:\n- [https://github.com/operator-framework/community-operators/tree/master/community-operators/knative-eventing-operator](https://github.com/operator-framework/community-operators/tree/master/community-operators/knative-eventing-operator)\n\nYou
          can install Tekton Pipeline via the Openshift operator:\n- [https://github.com/operator-framework/community-operators/tree/master/community-operators/openshift-pipelines-operator](https://github.com/operator-framework/community-operators/tree/master/community-operators/openshift-pipelines-operator)\n\nTriggerMesh
          authentication is supported by [Auth0](https://auth0.com/), follow the [documentation](https://github.com/triggermesh/operator/blob/master/auth0.md)
          to configure the Operator custom resource properly. \n"
        displayName: TriggerMesh
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: TriggerMesh, Inc
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: triggermesh
    provider:
      name: TriggerMesh, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: hawtio-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/hawtio-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: hawtio-operator.v0.2.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "hawt.io/v1alpha1",
              "kind": "Hawtio",
              "metadata": {
                "name": "example-hawtio"
              },
              "spec": {
                "replicas": 1,
                "version": "1.8.0"
              }
            }]
          capabilities: Seamless Upgrades
          categories: Monitoring
          certified: "false"
          containerImage: docker.io/hawtio/operator:0.2.0
          createdAt: "2020-01-09T10:59:00Z"
          description: Hawtio eases the discovery and management of Java applications
            deployed on OpenShift.
          repository: https://github.com/hawtio/hawtio-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Hawtio Console
            displayName: Hawtio
            kind: Hawtio
            name: hawtios.hawt.io
            version: v1alpha1
        description: "Hawtio\n==============\n\nThe Hawtio console eases the discovery
          and management of Java applications deployed on OpenShift.\n\nTo secure
          the communication between the Hawtio console and the Jolokia agents, a client
          certificate must be generated and mounted into the Hawtio console pod with
          a secret, to be used for TLS client authentication. This client certificate
          must be signed using the [service signing certificate](https://docs.openshift.com/container-platform/4.2/authentication/certificates/service-serving-certificate.html)
          authority private key.\n\nHere are the steps to be performed prior to the
          deployment:\n\n1. First, retrieve the service signing certificate authority
          keys, by executing the following commmands as a _cluster-admin_ user:\n
          \   ```sh\n    # The CA certificate\n    $ oc get secrets/signing-key -n
          openshift-service-ca -o \"jsonpath={.data['tls\\.crt']}\" | base64 --decode
          > ca.crt\n    # The CA private key\n    $ oc get secrets/signing-key -n
          openshift-service-ca -o \"jsonpath={.data['tls\\.key']}\" | base64 --decode
          > ca.key\n    ```\n\n2. Then, generate the client certificate, as documented
          in [Kubernetes certificates administration](https://kubernetes.io/docs/concepts/cluster-administration/certificates/),
          using either `easyrsa`, `openssl`, or `cfssl`, e.g., using `openssl`:\n
          \   ```sh\n    # Generate the private key\n    $ openssl genrsa -out server.key
          2048\n    # Write the CSR config file\n    $ cat <<EOT >> csr.conf\n    [
          req ]\n    default_bits = 2048\n    prompt = no\n    default_md = sha256\n
          \   distinguished_name = dn\n    \n    [ dn ]\n    CN = hawtio-online.hawtio.svc\n
          \   \n    [ v3_ext ]\n    authorityKeyIdentifier=keyid,issuer:always\n    keyUsage=keyEncipherment,dataEncipherment,digitalSignature\n
          \   extendedKeyUsage=serverAuth,clientAuth\n    EOT\n    # Generate the
          CSR\n    $ openssl req -new -key server.key -out server.csr -config csr.conf\n
          \   # Issue the signed certificate\n    $ openssl x509 -req -in server.csr
          -CA ca.crt -CAkey ca.key -CAcreateserial -out server.crt -days 10000 -extensions
          v3_ext -extfile csr.conf\n    ```\n\n3. Finally, you can create the secret
          to be mounted in the Hawtio console pod, from the generated certificate:\n
          \  ```sh\n   $ oc create secret tls hawtio-online-tls-proxying --cert server.crt
          --key server.key\n   ```\n\nNote that `CN=hawtio-online.hawtio.svc` must
          be trusted by the Jolokia agents, for which client certification authentication
          is enabled. See the `clientPrincipal` parameter from the [Jolokia agent
          configuration options](https://jolokia.org/reference/html/agents.html#agent-jvm-config).\n"
        displayName: Hawtio Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.2.0
      name: alpha
    defaultChannel: alpha
    packageName: hawtio-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Community
      provider-url: ""
    name: awss3-operator-registry
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/awss3-operator-registry
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: awss3operator.1.0.1
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"objectbucket.io/v1alpha1","kind":"ObjectBucketClaim","metadata":{"name":"myobc","namespace":"app-namespace"},"spec":{"generateBucketName":"mybucket-","bucketName":"my-awesome-bucket","storageClassName":"mystorageclass"}},{"apiVersion":"objectbucket.io/v1alpha1","kind":"ObjectBucket","metadata":{"name":"myobc-app-namespace-my-awesome-bucket","namespace":""},"spec":{"Connection":{"additionalState":{"ARN":"","UserName":""},"endpoint":{"additionalConfig":null,"bucketHost":"theendpoint.com","bucketName":"my-awesome-bucket","bucketPort":443,"region":"myregion","ssl":true,"subRegion":""}},"claimRef":"47ac88b7-7d6e-11e9-b8cf-42010af00007","reclaimPolicy":"Delete","storageClassName":"my-storageclass"}}]'
          capabilities: Basic Install
          categories: Storage
          certified: "false"
          containerImage: quay.io/screeley44/aws-s3-provisioner:v1.0.0
          createdAt: "2019-05-02 16:12:00"
          description: Manage the full lifecycle of installing, configuring and managing
            AWS S3 Provisioner.
          repository: https://github.com/yard-turkey/aws-s3-provisioner
          support: Community
        apiservicedefinitions: {}
        customresourcedefinitions:
          required:
          - description: instance of an AWS S3 Bucket
            displayName: Object Bucket
            kind: ObjectBucket
            name: objectbuckets.objectbucket.io
            version: v1alpha1
          - description: Request for an AWS S3 Bucket
            displayName: Object Bucket Claim
            kind: ObjectBucketClaim
            name: objectbucketclaims.objectbucket.io
            version: v1alpha1
        description: |
          AWS S3 Operator will deploy the AWS S3 Provisioner which will dynamically or statically
          provision AWS S3 Bucket storage and access. The operator deploys the ObjectBucket (OB) and ObjectBucketClaim (OBC)
          CustomResourceDefinitions. The OB/OBC model follows the traditional Kubernetes PV/PVC pattern, when an OBC is detected
          the operator will act on the OBC to either provision a brand new S3 Bucket in AWS or gain access to an existing
          S3 Bucket in AWS. The operator produces an OB and ConfigMap and Secret which can then be consumed by application pods.

          **Important Note**: Currently, while in preview, this operator does not
          support automatic upgrades. You must remove the old version of the operator
          manually before installing a new version.
          ## Using AWS S3 Operator

          ### Administrator Creates Secret
          This secret will contain the elevated/admin privileges needed by the provisioner
          to properly access and create S3 Buckets and IAM users and policies. The AWS Access ID
          and AWS Secret Key will be needed for this.
          1. Create the Kubernetes Secret for the Provisioner's Owner Access.
          ```yaml
          apiVersion: v1
          kind: Secret
          metadata:
            name: s3-bucket-owner [1]
            namespace: s3-provisioner [2]
          type: Opaque
          data:
            AWS_ACCESS_KEY_ID: *base64 encoded value* [3]
            AWS_SECRET_ACCESS_KEY: *base64 encoded value* [4]
          ```
          1. Name of the secret, this will be referenced in StorageClass.
          1. Namespace where the Secret will exist.
          1. Your AWS_ACCESS_KEY_ID base64 encoded.
          1. Your AWS_SECRET_ACCESS_KEY base64 encoded.
          ```
           # kubectl create -f creds.yaml
          secret/s3-bucket-owner created
          ```
          ### Administrator Creates StorageClass
          The StorageClass defines the name of the provisioner and holds other properties that are needed to provision a new bucket, including
          the Owner Secret and Namespace, and the AWS Region.
          #### Greenfield Example:
          For Greenfield, a new, dynamic bucket will be generated.
          1. Create the Kubernetes StorageClass for the Provisioner.
          ```yaml
          kind: StorageClass
          apiVersion: storage.k8s.io/v1
          metadata:
            name: s3-buckets [1]
          provisioner: aws-s3.io/bucket [2]
          parameters:
            region: us-west-1 [3]
            secretName: s3-bucket-owner [4]
            secretNamespace: s3-provisioner [5]
          reclaimPolicy: Delete [6]
          ```
          1. Name of the StorageClass, this will be referenced in the User ObjectBucketClaim.
          1. Provisioner name
          1. AWS Region that the StorageClass will serve
          1. Name of the bucket owner Secret created above
          1. Namespace where the Secret will exist
          1. reclaimPolicy (Delete or Retain) indicates if the bucket can be deleted when the OBC is deleted.
          **NOTE:** the absence of the `bucketName` Parameter key in the storage class indicates this is a new bucket and its name is based on the bucket name fields in the OBC.
          ```
           # kubectl create -f storageclass-greenfield.yaml
          storageclass.storage.k8s.io/s3-buckets created
          ```
          #### Brownfield Example:
          For brownfield, the StorageClass defines the name of the provisioner and the name of the existing bucket. It also includes other properties needed by the target
          provisioner, including: the Owner Secret and Namespace, and the AWS Region
          1. Create the Kubernetes StorageClass for the Provisioner.
          ```yaml
          kind: StorageClass
          apiVersion: storage.k8s.io/v1
          metadata:
            name: s3-existing-buckets [1]
          provisioner: aws-s3.io/bucket [2]
          parameters:
            bucketName: my-existing-bucket [3]
            region: us-west-1 [4]
            secretName: s3-bucket-owner [5]
            secretNamespace: s3-provisioner [6]
          ```
          1. Name of the StorageClass, this will be referenced in the User ObjectBucketClaim.
          1. Provisioner name
          1. Name of the existing bucket
          1. AWS Region that the StorageClass will serve
          1. Name of the bucket owner Secret created above
          1. Namespace for that bucket owner secret
          **NOTE:** the storage class's `reclaimPolicy` is ignored for existing buckets.
          ```
           # kubectl create -f storageclass-brownfield.yaml
          storageclass.storage.k8s.io/s3-buckets created
          ```
          ### User Creates ObjectBucketClaim
          An ObjectBucketClaim follows the same concept as a PVC, in that
          it is a request for Object Storage, the user doesn't need to
          concern him/herself with the underlying storage, just that
          they need access to it. The user will work with the cluster/storage
          administrator to get the proper StorageClass needed and will
          then request access via the OBC.
          #### Greenfield Request Example:
          1. Create the ObjectBucketClaim.
          ```yaml
          apiVersion: objectbucket.io/v1alpha1
          kind: ObjectBucketClaim
          metadata:
            name: myobc [1]
            namespace: s3-provisioner [2]
          spec:
            generateBucketName: mybucket [3]
            bucketName: my-awesome-bucket [4]
            storageClassName: s3-buckets [5]
          ```
          1. Name of the OBC
          1. Namespace of the OBC
          1. Name prepended to a random string used to generate a bucket name. It is ignored if bucketName is defined
          1. Name of new bucket which must be unique across all AWS regions, otherwise an error occurs when creating the bucket. If present, this name overrides `generateName`
          1. StorageClass name
          **NOTE:** if both `generateBucketName` and `bucketName` are omitted, and the storage class does _not_ define a bucket name, then a new, random bucket name is generated with no prefix.
          ```
           # kubectl create -f obc-brownfield.yaml
          objectbucketclaim.objectbucket.io/myobc created
          ```
          #### Brownfield Request Example:
          1. Create the ObjectBucketClaim.
          ```yaml
          apiVersion: objectbucket.io/v1alpha1
          kind: ObjectBucketClaim
          metadata:
            name: myobc [1]
            namespace: s3-provisioner [2]
          spec:
            storageClassName: s3-existing-buckets [3]
          ```
          1. Name of the OBC
          1. Namespace of the OBC
          1. StorageClass name
          **NOTE:** in the OBC here there is no reference to the bucket's name. This is defined in the storage class and is not a concern of the user creating the claim to this bucket.  An OBC does have fields for defining a bucket name for greenfield use only.
          ```
           # kubectl create -f obc-brownfield.yaml
          objectbucketclaim.objectbucket.io/myobc created
          ```
          ### Results and Recap
          Let's pause for a moment and digest what just happened.
          After creating the OBC, and assuming the S3 provisioner is running, we now have
          the following Kubernetes resources:
          .  a global ObjectBucket (OB) which contains: bucket endpoint info (including region and bucket name), a reference to the OBC, and a reference to the storage class. Unique to S3, the OB also contains the bucket Amazon Resource Name (ARN).Note: there is always a 1:1 relationship between an OBC and an OB.
          .  a ConfigMap in the same namespace as the OBC, which contains the same endpoint data found in the OB.
          .  a Secret in the same namespace as the OBC, which contains the AWS key-pairs needed to access the bucket.
          And of course, we have a *new* AWS S3 Bucket which you should be able to see via the AWS Console.
          *ObjectBucket*
          ```yaml
           # kubectl get ob obc-s3-provisioner-my-awesome-bucket -o yaml
          apiVersion: objectbucket.io/v1alpha1
          kind: ObjectBucket
          metadata:
            creationTimestamp: "2019-04-03T15:42:22Z"
            generation: 1
            name: obc-s3-provisioner-my-awesome-bucket
            resourceVersion: "15057"
            selfLink: /apis/objectbucket.io/v1alpha1/objectbuckets/obc-s3-provisioner-my-awesome-bucket
            uid: 0bfe8e84-576d-4c4e-984b-f73c4460f736
          spec:
            Connection:
              additionalState:
                ARN: arn:aws:iam::<accountid>:policy/my-awesome-bucket-vSgD5 [1]
                UserName: my-awesome-bucket-vSgD5 [2]
              endpoint:
                additionalConfig: null
                bucketHost: s3-us-west-1.amazonaws.com
                bucketName: my-awesome-bucket [3]
                bucketPort: 443
                region: us-west-1
                ssl: true
                subRegion: ""
            claimRef: null [4]
            reclaimPolicy: null
            storageClassName: s3-buckets [5]
          ```
          1. The AWS Policy created for this user and bucket.
          1. The new user generated by the Provisioner to access this existing bucket.
          1. The bucket name.
          1. The reference to the OBC (not filled in yet).
          1. The reference to the StorageClass used.
          *ConfigMap*
          ```yaml
           # kubectl get cm myobc -n s3-provisioner -o yaml
          apiVersion: v1
          data:
            BUCKET_HOST: s3-us-west-1.amazonaws.com [1]
            BUCKET_NAME: my-awesome-bucket [2]
            BUCKET_PORT: "443"
            BUCKET_REGION: us-west-1
            BUCKET_SSL: "true"
            BUCKET_SUBREGION: ""
          kind: ConfigMap
          metadata:
            creationTimestamp: "2019-04-01T19:11:38Z"
            finalizers:
            - objectbucket.io/finalizer
            name: my-awesome-bucket
            namespace: s3-provisioner
            resourceVersion: "892"
            selfLink: /api/v1/namespaces/s3-provisioner/configmaps/my-awesome-bucket
            uid: 2edcc58a-aff8-4a29-814a-ffbb6439a9cd
          ```
          1. The AWS S3 host.
          1. The name of the new bucket we are gaining access to.
          *Secret*
          ```yaml
           # kubectl get secret my-awesome-bucket -n s3-provisioner -o yaml
          apiVersion: v1
          data:
            AWS_ACCESS_KEY_ID: *the_new_access_id* [1]
            AWS_SECRET_ACCESS_KEY: *the_new_access_key_value* [2]
          kind: Secret
          metadata:
            creationTimestamp: "2019-04-03T15:42:22Z"
            finalizers:
            - objectbucket.io/finalizer
            name: my-awesome-bucket
            namespace: s3-provisioner
            resourceVersion: "15058"
            selfLink: /api/v1/namespaces/s3-provisioner/secrets/screeley-provb-5
            uid: 225c71a5-9d75-4ccc-b41f-bfe91b272a13
          type: Opaque
          ```
          1. The new generated AWS Access Key ID.
          1. The new generated AWS Secret Access Key.
          What happened in AWS? The first thing we do on any OBC request is
          create a new IAM user and generate Access ID and Secret Keys.
          This allows us to also better control ACLs. We also create a policy
          in IAM which we then attach to the user and bucket. We also created a new bucket, called *my-awesome-bucket*.
          When the OBC is deleted all of its Kubernetes and AWS resources will also be deleted, which includes:
          the generated OB, Secret, ConfigMap, IAM user, and policy.
          If the _retainPolicy_ on the StorageClass for this bucket is *"Delete"*, then, in addition to the above cleanup, the physical bucket is also deleted.
          **NOTE:** The actual bucket is only deleted if the Storage Class's _reclaimPolicy_ is "Delete".
          ### User Creates Pod
          Now that we have our bucket and connection/access information, a pod
          can be used to access the bucket. This can be done in several different
          ways, but the key here is that the provisioner has provided the proper
          endpoints and keys to access the bucket. The user then simply references
          the keys.
          1. Create a Sample Pod to Access the Bucket.
          ```yaml
          apiVersion: v1
          kind: Pod
          metadata:
            name: photo1
            labels:
              name: photo1
          spec:
            containers:
            - name: photo1
              image: docker.io/screeley44/photo-gallery:latest
              imagePullPolicy: Always
              envFrom:
              - configMapRef:
                  name: my-awesome-bucket <1>
              - secretRef:
                  name: my-awesome-bucket <2>
              ports:
              - containerPort: 3000
                protocol: TCP
          ```
          1. Name of the generated configmap from the provisioning process
          1. Name of the generated secret from the provisioning process
          *[Note]* Generated ConfigMap and Secret are same name as the OBC!
          Lastly, expose the pod as a service so you can access the url from a browser. In this example,
          I exposed as a LoadBalancer
          ```
            # kubectl expose pod photo1 --type=LoadBalancer --name=photo1 -n your-namespace
          ```
          To access via a url use the EXTERNAL-IP
          ```
            # kubectl get svc photo1
            NAME                         TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)          AGE
            photo1                       LoadBalancer   100.66.124.105   a00c53ccb3c5411e9b6550a7c0e50a2a-2010797808.us-east-1.elb.amazonaws.com   3000:32344/TCP   6d
          ```
          **NOTE:** This is just one example of a Pod that can utilize the bucket information,
          there are several ways that these pod applications can be developed and therefore
          the method of getting the actual values needed from the Secrets and ConfigMaps
          will vary greatly, but the idea remains the same, that the pod consumes the generated
          ConfigMap and Secret created by the provisioner.
        displayName: AWS S3 Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Community
        version: 1.0.1
      name: alpha
    defaultChannel: alpha
    packageName: awss3-operator-registry
    provider:
      name: Community
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Aqua Security, Inc.
      provider-url: ""
    name: aqua-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/aqua-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: aqua-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaCsp",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "platform": "openshift",
                    "requirements": true
                  },
                  "registry": {
                    "url": "registry.aquasec.com",
                    "username": "example@gmail.com",
                    "password": "",
                    "email": "example@gmail.com"
                  },
                  "database": {
                    "replicas": 1,
                    "service": "ClusterIP"
                  },
                  "gateway": {
                    "replicas": 1,
                    "service": "ClusterIP"
                  },
                  "server": {
                    "replicas": 1,
                    "service": "LoadBalancer"
                  },
                  "adminPassword": "Password1",
                  "licenseToken": null
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaDatabase",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5",
                    "platform": "openshift"
                  },
                  "deploy": {
                    "replicas": 1,
                    "service": "ClusterIP"
                  },
                  "diskSize": 10
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaEnforcer",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5"
                  },
                  "gateway": {
                    "host": "aqua-gateway-svc",
                    "port": 3622
                  },
                  "token": "token"
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaGateway",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5"
                  },
                  "common": {
                    "databaseSecret": {
                      "name": "aqua-aqua-db",
                      "key": "password"
                    }
                  },
                  "externalDb": {
                    "host": "aqua-db",
                    "port": 5432,
                    "username": "postgres"
                  },
                  "deploy": {
                    "replicas": 1,
                    "service": "ClusterIP"
                  }
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaScanner",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5"
                  },
                  "deploy": {
                    "replicas": 1
                  },
                  "login": {
                    "username": "administrator",
                    "password": "Password1",
                    "host": "http://aqua-server:8080"
                  }
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaServer",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5"
                  },
                  "common": {
                    "databaseSecret": {
                      "name": "aqua-aqua-db",
                      "key": "password"
                    }
                  },
                  "externalDb": {
                    "host": "aqua-db",
                    "port": 5432,
                    "username": "postgres"
                  },
                  "deploy": {
                    "replicas": 1,
                    "service": "LoadBalancer"
                    },
                  "adminPassword": "Password1",
                  "licenseToken": null
                }
              }
            ]
          capabilities: Basic Install
          categories: Security
          certified: "true"
          containerImage: registry.connect.redhat.com/aquasec/aquasec:1.0.0
          createdAt: "2019-12-30T08:00:00Z"
          description: The Aqua Security Operator runs within a Openshift cluster
            and provides a means to deploy and manage Aqua Security cluster and components.
          repository: https://github.com/aquasecurity/aqua-operator
          support: Aqua Security, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Aqua Security CSP Deployment with Aqua Operator
            displayName: AquaCsp
            kind: AquaCsp
            name: aquacsps.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Database Deployment with Aqua Operator
            displayName: AquaDatabase
            kind: AquaDatabase
            name: aquadatabases.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Enforcer Deployment with Aqua Operator
            displayName: AquaEnforcer
            kind: AquaEnforcer
            name: aquaenforcers.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Gateway Deployment with Aqua Operator
            displayName: AquaGateway
            kind: AquaGateway
            name: aquagateways.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Scanner Deployment with Aqua Operator
            displayName: AquaScanner
            kind: AquaScanner
            name: aquascanners.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Server Deployment with Aqua Operator
            displayName: AquaServer
            kind: AquaServer
            name: aquaservers.operator.aquasec.com
            version: v1alpha1
        description: "The Aqua Security Operator runs within an OpenShift cluster,
          and provides a means to deploy and manage the Aqua Security cluster and
          components\n* Server (sometimes called â€œconsoleâ€)\n* Database (not recommended
          for production environments)\n* Gateway\n* Enforcer (sometimes called â€œagentâ€)\n*
          Scanner\n* CSP (package containing the Server, Database, and Gateway - not
          supported, and not for production environments)\nUse the aqua-operator to
          \n* Deploy Aqua Security components on OpenShift\n* Scale up Aqua Security
          components with extra replicas\n* Assign metadata tags to Aqua Security
          components\n* Automatically scale the number of Aqua scanners according
          to the number of images in the scan queue\n## Before You Begin Using the
          Operator CRDs\nObtain access to the Aqua registry - https://www.aquasec.com/about-us/contact-us/\nYou
          need to create\n* A secret for the Docker registry\n* A secret for the database\n```bash\noc
          create secret docker-registry aqua-registry --docker-server=registry.aquasec.com
          --docker-username=<AQUA_USERNAME> --docker-password=<AQUA_PASSWORD> --docker-email=<user
          email> -n aqua\noc create secret generic aqua-database-password --from-literal=db-password=<password>
          -n aqua\noc secrets add aqua-sa aqua-registry --for=pull -n aqua\n```\n##
          After the Installation\nOnce the operator is installed in the cluster, you
          now can use the CRDs to install the Aqua cluster and components."
        displayName: Aqua Security Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Aqua Security, Inc.
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: aqua-operator-certified
    provider:
      name: Aqua Security, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Aqua Security, Inc.
      provider-url: ""
    name: aqua-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/aqua-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: aqua-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaCsp",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "requirements": true,
                  "password": "password",
                  "rbac": {
                    "enable": true,
                    "openshift": true,
                    "privileged": true
                  },
                  "registry": {
                    "url": "registry.aquasec.com",
                    "username": "example@gmail.com",
                    "password": "",
                    "email": "example@gmail.com"
                  },
                  "database": {
                    "replicas": 1,
                    "service": "ClusterIP",
                    "image": {
                      "repository": "database",
                      "registry": "registry.aquasec.com",
                      "tag": "4.0",
                      "pullPolicy": "IfNotPresent"
                    }
                  },
                  "gateway": {
                    "replicas": 1,
                    "service": "ClusterIP",
                    "image": {
                      "repository": "gateway",
                      "registry": "registry.aquasec.com",
                      "tag": "4.0",
                      "pullPolicy": "IfNotPresent"
                    }
                  },
                  "server": {
                    "replicas": 1,
                    "service": "LoadBalancer",
                    "image": {
                      "repository": "console",
                      "registry": "registry.aquasec.com",
                      "tag": "4.0",
                      "pullPolicy": "IfNotPresent"
                    }
                  },
                  "scanner": {
                    "deploy": {
                      "replicas": 1,
                      "service": "ClusterIP",
                      "image": {
                        "repository": "scanner",
                        "registry": "registry.aquasec.com",
                        "tag": "4.0",
                        "pullPolicy": "IfNotPresent"
                      }
                    },
                    "min": 1,
                    "max": 5,
                    "imagesPerScanner": 100
                  },
                  "adminPassword": "Password1",
                  "licenseToken": null,
                  "clustermode": false,
                  "ssl": false,
                  "auditSsl": false
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaDatabase",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "requirements": true,
                  "password": "password",
                  "registry": {
                    "url": "registry.aquasec.com",
                    "username": "example@gmail.com",
                    "password": "",
                    "email": "example@gmail.com"
                  },
                  "deploy": {
                    "replicas": 1,
                    "service": "ClusterIP",
                    "image": {
                      "repository": "database",
                      "registry": "registry.aquasec.com",
                      "tag": "4.0",
                      "pullpolicy": "IfNotPresent"
                    }
                  }
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaEnforcer",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "requirements": false,
                  "serviceAccount": "aqua-sa",
                  "token": "tests",
                  "rbac": {
                    "enable": false,
                    "openshift": true,
                    "privileged": true
                  },
                  "deploy": {
                    "image": {
                      "repository": "enforcer",
                      "registry": "registry.aquasec.com",
                      "tag": "4.0",
                      "pullPolicy": "IfNotPresent"
                    }
                  },
                  "gateway": {
                    "host": "aqua-gateway-svc",
                    "port": 3622
                  },
                  "sendingHostImages": false,
                  "runcInterception": false
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaGateway",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "requirements": false,
                  "serviceAccount": "aqua-sa",
                  "dbSecretName": "aqua-database-password",
                  "dbSecretKey": "db-password",
                  "deploy": {
                    "replicas": 1,
                    "service": "ClusterIP",
                    "image": {
                      "repository": "gateway",
                      "registry": "registry.aquasec.com",
                      "tag": "3.5",
                      "pullpolicy": "IfNotPresent"
                    }
                  },
                  "aquaDb": "aqua-database",
                  "ssl": false,
                  "auditSsl": false,
                  "openshift": true
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaScanner",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "requirements": false,
                  "serviceAccount": "aqua-sa",
                  "deploy": {
                    "replicas": 1,
                    "service": "ClusterIP",
                    "image": {
                      "repository": "scanner",
                      "registry": "registry.aquasec.com",
                      "tag": "4.0",
                      "pullPolicy": "IfNotPresent"
                    }
                  },
                  "login": {
                    "username": "administrator",
                    "password": "Password1",
                    "host": "http://aqua-server-svc:8080"
                  },
                  "openshift": true
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaServer",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "requirements": false,
                  "add": false,
                  "serviceAccount": "aqua-sa",
                  "dbSecretName": "aqua-database-password",
                  "dbSecretKey": "db-password",
                  "rbac": {
                    "enable": true,
                    "openshift": true,
                    "privileged": true
                  },
                  "deploy": {
                    "replicas": 1,
                    "service": "LoadBalancer",
                    "image": {
                      "repository": "console",
                      "registry": "registry.aquasec.com",
                      "tag": "3.5",
                      "pullPolicy": "IfNotPresent"
                    }
                  },
                  "aquaDb": "aqua-database",
                  "adminPassword": "Password1",
                  "licenseToken": null,
                  "clusterMode": true,
                  "ssl": false,
                  "auditSsl": false,
                  "dockerless": false,
                  "openshift": true
                }
              }
            ]
          capabilities: Basic Install
          categories: Security
          certified: "true"
          containerImage: registry.connect.redhat.com/aquasec/aqua:0.0.1
          createdAt: "2019-03-21 08:00:00"
          description: The Aqua Security Operator runs within a Openshift cluster
            and provides a means to deploy and manage Aqua Security cluster and components.
          repository: https://github.com/aquasecurity/aqua-operator
          support: Aqua Security, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Aqua Security CSP Deployment with Aqua Operator
            displayName: AquaCsp
            kind: AquaCsp
            name: aquacsps.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Database Deployment with Aqua Operator
            displayName: AquaDatabase
            kind: AquaDatabase
            name: aquadatabases.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Enforcer Deployment with Aqua Operator
            displayName: AquaEnforcer
            kind: AquaEnforcer
            name: aquaenforcers.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Gateway Deployment with Aqua Operator
            displayName: AquaGateway
            kind: AquaGateway
            name: aquagateways.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Scanner Deployment with Aqua Operator
            displayName: AquaScanner
            kind: AquaScanner
            name: aquascanners.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Server Deployment with Aqua Operator
            displayName: AquaServer
            kind: AquaServer
            name: aquaservers.operator.aquasec.com
            version: v1alpha1
        description: "The Aqua Security Operator runs within an OpenShift cluster,
          and provides a means to deploy and manage the Aqua Security cluster and
          components\n* Server (sometimes called â€œconsoleâ€)\n* Database (not recommended
          for production environments)\n* Gateway\n* Enforcer (sometimes called â€œagentâ€)\n*
          Scanner\n* CSP (package containing the Server, Database, and Gateway - not
          supported, and not for production environments)\n\nUse the aqua-operator
          to \n* Deploy Aqua Security components on OpenShift\n* Scale up Aqua Security
          components with extra replicas\n* Assign metadata tags to Aqua Security
          components\n* Automatically scale the number of Aqua scanners according
          to the number of images in the scan queue\n\n## Before You Begin Using the
          Operator CRDs\nObtain access to the Aqua registry - https://www.aquasec.com/about-us/contact-us/\nYou
          need to create\n* A secret for the Docker registry\n* A secret for the database\n\n```bash\noc
          create secret docker-registry aqua-registry --docker-server=registry.aquasec.com
          --docker-username=<AQUA_USERNAME> --docker-password=<AQUA_PASSWORD> --docker-email=<user
          email> -n aqua\noc create secret generic aqua-database-password --from-literal=db-password=<password>
          -n aqua\noc secrets add aqua-sa aqua-registry --for=pull -n aqua\n```\n\n##
          After the Installation\nOnce the operator is installed in the cluster, you
          now can use the CRDs to install the Aqua cluster and components."
        displayName: Aqua Security Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Aqua Security, Inc.
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: aqua-certified
    provider:
      name: Aqua Security, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: StorageOS, Inc
      provider-url: ""
    name: storageos-10tb
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/storageos-10tb
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: storageosoperator-rhm-10tb.v1.5.3
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "storageos.com/v1",
                "kind": "StorageOSCluster",
                "metadata": {
                  "name": "example-storageos",
                  "namespace": "openshift-operators"
                },
                "spec": {
                  "namespace": "kube-system",
                  "secretRefName": "storageos-api",
                  "secretRefNamespace": "openshift-operators",
                  "k8sDistro": "openshift",
                  "csi": {
                    "enable": true,
                    "deploymentStrategy": "deployment"
                  }
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "Job",
                "metadata": {
                  "name": "example-job",
                  "namespace": "default"
                },
                "spec": {
                  "image": "registry.connect.redhat.com/storageos/cluster-operator:latest",
                  "args": ["/var/lib/storageos"],
                  "mountPath": "/var/lib",
                  "hostPath": "/var/lib",
                  "completionWord": "done"
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "StorageOSUpgrade",
                "metadata": {
                  "name": "example-upgrade",
                  "namespace": "default"
                },
                "spec": {
                  "newImage": "registry.connect.redhat.com/storageos/node:latest"
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "NFSServer",
                "metadata": {
                  "name": "example-nfsserver",
                  "namespace": "default"
                },
                "spec": {
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                }
              }
            ]
          capabilities: Deep Insights
          categories: Storage
          certified: "true"
          containerImage: registry.connect.redhat.com/storageos/cluster-operator:1.5.3
          createdAt: "2020-02-13T17:27:53Z"
          description: Cloud-native, persistent storage for containers.
          repository: https://github.com/storageos/cluster-operator
          support: StorageOS, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: StorageOS Cluster installs StorageOS in the cluster. It contains
              all the configuration for setting up a StorageOS cluster and also shows
              the status of the running StorageOS cluster.
            displayName: StorageOS Cluster
            kind: StorageOSCluster
            name: storageosclusters.storageos.com
            version: v1
          - description: StorageOS Job creates special pods that run on all the node
              and perform an administrative task. This could be used for cluster maintenance
              tasks.
            displayName: StorageOS Job
            kind: Job
            name: jobs.storageos.com
            version: v1
          - description: StorageOS Upgrade automatically upgrades an existing StorageOS
              cluster as per the upgrade configuration.
            displayName: StorageOS Upgrade
            kind: StorageOSUpgrade
            name: storageosupgrades.storageos.com
            version: v1
          - description: StorageOS NFS Server provides support for shared volumes.
              The StorageOS control plane will automatically create and manage NFS
              Server instances when a PersistentVolumeClaim requests a volume with
              AccessMode=ReadWriteMany.
            displayName: NFS Server
            kind: NFSServer
            name: nfsservers.storageos.com
            version: v1
        description: |
          StorageOS is a cloud native, software-defined storage platform that
          transforms commodity server or cloud based disk capacity into
          enterprise-class persistent storage for containers. StorageOS is ideal for
          deploying databases, message busses, and other mission-critical stateful
          solutions, where rapid recovery and fault tolerance are essential.

          The StorageOS Operator installs and manages StorageOS within a cluster.
          Cluster nodes may contribute local or attached disk-based storage into a
          distributed pool, which is then available to all cluster members via a
          global namespace.

          Volumes are available across the cluster so if a container gets moved to
          another node it has immediate access to re-attach its data. Data can be
          protected with synchronous replication. Compression, caching, and QoS are
          enabled by default, and all volumes are thinly-provisioned.

          No other hardware or software is required.

          StorageOS is free to use up to 50GB of presented storage, increasing to
          500GB after registration.  For additional capacity and support plans contact
          sales@storageos.com.

          ## Supported Features

          * **Rapid volume failover** - If a container gets re-scheduled to another
          node, any StorageOS volumes can be re-attached immediately, irrespective of
          where the data is located within the cluster.

          * **Data protection** - Data is replicated synchronously, up to 6 copies.

          * **Block checksums** - Each block is protected by a checksum which
          automatically detects any corruption of data in the underlying storage
          media.

          * **Thin provisioning** - Only consume the space you need in a storage pool.

          * **Data reduction** - Transparent inline data compression to reduce the
          amount of storage used in a backing store as well as reducing the network
          bandwidth requirements for replication.

          * **In-memory caching** - Speed up access to data even when accessed
          remotely.

          * **Quality of service** - Prioritize data traffic and address the â€œnoisy
          neighborsâ€ problem.

          * **Flexible configuration** - Use labels to automate data placement and
          enforce data policy such as replication. Ideal for compliance and infosec
          teams to enforce policies and rules while still enabling self-service
          storage by developers and DevOps teams.

          * **Access control** - Support multiple namespace â€“ individual users are
          permissioned to specific storage namespaces.

          * **Observability & instrumentation** - Log streams for observability and
          Prometheus support for instrumentation.

          * **Deployment flexibility** - Scale up or scale out storage based on
          application requirements.  Works with any infrastructure â€“ on-premises, VM,
          bare metal or cloud.

          * **Small footprint** - Lightweight container requires minimum 1 core with
          2GB of RAM.  Runs in user-space on any 64-bit Linux with no custom kernel
          modules.

          ## Prerequisites

          * Ensure port 5705 is open on the worker nodes.

          ## Required Parameters

          * `secretRefName` - the name of a secret that contains two keys for the
          `apiUsername` and `apiPassword` to be used as api credentials
          ([documentation](https://docs.storageos.com/docs/reference/cluster-operator/examples))
          * `secretRefNamespace` - the namespace where the api credentials secret is
          stored

          ## About StorageOS

          StorageOS gives users total control of their own storage environment â€“
          whether in the cloud or on-premises. Our customers take advantage of storage
          on any platform while maintaining full control of business requirements
          around availability, data mobility, performance, security, data residency
          compliance and business continuity.
        displayName: StorageOS
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: StorageOS, Inc
        version: 1.5.3
      name: stable
    defaultChannel: stable
    packageName: storageos-10tb
    provider:
      name: StorageOS, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: amq-broker
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/amq-broker
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: amq-broker-operator.v0.9.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
                "apiVersion": "broker.amq.io/v2alpha1",
                "kind": "ActiveMQArtemis",
                "metadata": {
                    "name": "ex-aao"
                },
                "spec": {
                    "deploymentPlan": {
                        "size": 1,
                        "image": "registry.redhat.io/amq7/amq-broker:7.5"
                    }
                }
            }, {
                "apiVersion": "broker.amq.io/v2alpha1",
                "kind": "ActiveMQArtemisAddress",
                "metadata": {
                    "name": "ex-aaoaddress"
                },
                "spec": {
                    "addressName": "myAddress0",
                    "queueName": "myQueue0",
                    "routingType": "anycast"
                }
            }, {
                "apiVersion": "broker.amq.io/v2alpha1",
                "kind": "ActiveMQArtemisScaledown",
                "metadata": {
                    "name": "ex-aaoscaledown"
                },
                "spec": {
                    "localOnly": "true"
                }
            }]
          capabilities: Basic Install
          categories: Streaming & Messaging
          certified: "false"
          containerImage: registry.redhat.io/amq7/amq-broker-rhel7-operator:0.9
          createdAt: "2019-12-09T08:16:32Z"
          description: AMQ Broker Operator provides the ability to deploy and manage
            stateful AMQ Broker broker clusters
          repository: https://github.com/rh-messaging/amq-broker-operator
          support: Red Hat, Inc.
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An instance of Active MQ Artemis
            displayName: AMQ Broker
            kind: ActiveMQArtemis
            name: activemqartemises.broker.amq.io
            version: v2alpha1
          - description: Adding and removing addresses via custom resource definitions
            displayName: AMQ Broker Address
            kind: ActiveMQArtemisAddress
            name: activemqartemisaddresses.broker.amq.io
            version: v2alpha1
          - description: Provides message migration on clustered broker scaledown
            displayName: AMQ Broker Scaledown
            kind: ActiveMQArtemisScaledown
            name: activemqartemisscaledowns.broker.amq.io
            version: v2alpha1
        description: AMQ Broker Operator provides the ability to deploy and manage
          stateful AMQ Broker broker clusters
        displayName: AMQ Broker
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 0.9.1
      name: alpha
    defaultChannel: alpha
    packageName: amq-broker
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat, Inc.
      provider-url: ""
    name: hyperfoil-bundle
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/hyperfoil-bundle
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: hyperfoil-operator.v0.5.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "hyperfoil.io/v1alpha1",
                "kind": "Hyperfoil",
                "metadata": {
                  "name": "example-hyperfoil"
                },
                "spec": {
                  "agentDeployTimeout": 60000,
                  "log": "myConfigMap/log4j2-superverbose.xml",
                  "route": "hyperfoil.apps.mycloud.example.com",
                  "version": "latest"
                }
              }
            ]
          capabilities: Basic Install
          categories: Developer Tools
          containerImage: quay.io/hyperfoil/hyperfoil-operator:0.5.1
          description: Microservice-oriented distributed benchmark framework.
          repository: https://github.com/Hyperfoil/hyperfoil-operator
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Configures Hyperfoil Controller and related resources.
            displayName: Hyperfoil
            kind: Hyperfoil
            name: hyperfoils.hyperfoil.io
            version: v1alpha1
        description: |-
          Hyperfoil is a modern benchmark framework oriented promising accurate results
          with more flexibility and distributed architecture.

          See more on [hyperfoil.io](http://hyperfoil.io).

          Hyperfoil is licensed under Apache License 2.0
        displayName: Hyperfoil
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 0.5.1
      name: alpha
    defaultChannel: alpha
    packageName: hyperfoil-bundle
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: IBM
      provider-url: ""
    name: ibm-spectrum-scale-csi-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ibm-spectrum-scale-csi-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ibm-spectrum-scale-csi-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "csi.ibm.com/v1",
                "kind": "CSIScaleOperator",
                "metadata": {
                  "labels": {
                    "app.kubernetes.io/instance": "ibm-spectrum-scale-csi-operator",
                    "app.kubernetes.io/managed-by": "ibm-spectrum-scale-csi-operator",
                    "app.kubernetes.io/name": "ibm-spectrum-scale-csi-operator"
                  },
                  "name": "ibm-spectrum-scale-csi"
                },
                "spec": {
                  "clusters": [
                    {
                      "id": "\u003c Primary Cluster ID - WARNING: THIS IS A STRING NEEDS YAML QUOTES!\u003e",
                      "primary": {
                        "primaryFs": "\u003c Primary Filesystem \u003e",
                        "primaryFset": "\u003c Fileset in Primary Filesystem \u003e"
                      },
                      "restApi": [
                        {
                          "guiHost": "\u003c Primary cluster GUI IP/Hostname \u003e"
                        }
                      ],
                      "secrets": "secret1",
                      "secureSslMode": false
                    }
                  ],
                  "scaleHostpath": "\u003c GPFS FileSystem Path \u003e"
                },
                "status": {}
              }
            ]
          capabilities: Basic Install
          categories: Storage
          certified: "false"
          containerImage: quay.io/ibm-spectrum-scale/ibm-spectrum-scale-csi-operator:v1.0.0
          createdAt: Wed Dec  4 06:41:31 EST 2019
          description: An operator for deploying and managing the IBM CSI Spectrum
            Scale Driver.
          repository: https://github.com/IBM/ibm-spectrum-scale-csi-operator/
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a deployment of the IBM CSI Spectrum Scale driver.
            displayName: IBM CSI Spectrum Scale Driver
            kind: CSIScaleOperator
            name: csiscaleoperators.csi.ibm.com
            version: v1
        description: "IBM Spectrum Scale CSI Operator Quickstart\n==========================================\n\nThe
          IBM Spectrum Scale CSI Operator runs within a Kubernetes cluster providing
          a means to \ndeploy and manage the CSI plugin for spectrum scale. For more
          in depth documentation please refer\nto the [README](https://github.com/IBM/ibm-spectrum-scale-csi-operator/blob/v1.0.0/README.md).\n\nThis
          operator should be used to deploy the CSI plugin.\n\nThe configuration process
          is as follows:\n\n1. [Spectrum Scale GUI Setup](#spectrum-scale-gui-setup)\n2.
          [Custom Resource Configuration](#custom-resource-configuration)\n\nSpectrum
          Scale GUI Setup \n------------------------\n> **NOTE:** This step only needs
          to be preformed once per GUI.\n\n> **WARNING:** If your daemonset pods (driver
          pods) do not come up, generally this means you have a  secret that  has
          not been defined in the correct namespace.\n\n1. Ensure the Spectrum Scale
          GUI is running by pointing your browser to the IP hosting the GUI:\n\n    ![](https://user-images.githubusercontent.com/1195452/67230992-6d2d9700-f40c-11e9-96d5-3f0e5bcb2d9a.png)\n\n
          \   > If you do not see a login follow on screen instructions, or review
          the [GUI Documentation](https://www.ibm.com/support/knowledgecenter/en/STXKQY_5.0.3/com.ibm.spectrum.scale.v5r03.doc/bl1ins_quickrefforgui.htm)\n\n\n2.
          Create a CsiAdmin group account on in the GUI (currently requires a CLI
          call):\n\n   ```\n\n   export USERNAME=\"SomeUser\"\n   export PASSWORD=\"SomePassword\"\n
          \  /usr/lpp/mmfs/gui/cli/mkuser ${USERNAME} -p ${PASSWORD} -g CsiAdmin\n\n
          \  ```\n\n3. Create a Kubernetes secret for the `CsiAdmin` user:\n\n  ```\n\n
          \ export USERNAME_B64=$(echo $USERNAME | base64)\n  export PASSWORD_B64=$(echo
          $PASSWORD | base64)\n  export OPERATOR_NAMESPACE=\"ibm-spectrum-scale-csi-driver\"
          \ # Set this to the namespace you deploy the operator in.\n  \n\n  cat <<
          EOF > /tmp/csisecret.yaml\n  apiVersion: v1\n  data:\n    password: ${PASSWORD_B64}\n
          \   username: ${USERNAME_B64}\n  kind: Secret\n  type: Opaque\n  metadata:\n
          \   name: csisecret    # This should be in your CSIScaleOperator definition\n
          \   namespace: ${OPERATOR_NAMESPACE} \n    labels:\n      app.kubernetes.io/name:
          ibm-spectrum-scale-csi-operator # Used by the operator to detect changes,
          set on load of CR change if secret matches name in CR and namespace.\n  EOF\n
          \ \n\n  kubectl create -f /tmp/csisecret.yaml\n  rm -f /tmp/csisecret.yaml\n
          \ \n  ```\n\nCustom Resource Configuration\n-----------------------------\n\nThe
          bundled Custom Resource example represents the minimum settings needed to
          run the operator.\nIf your environment needs more advanced settings (e.g.
          remote clusters, node mapping, etc.) please\nrefer to the sample [Custom
          Resource](https://github.com/IBM/ibm-spectrum-scale-csi-operator/blob/v1.0.0/stable/ibm-spectrum-scale-csi-operator-bundle/operators/ibm-spectrum-scale-csi-operator/deploy/crds/ibm-spectrum-scale-csi-operator-cr.yaml).\n\n\n"
        displayName: IBM Spectrum Scale CSI Plugin Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: IBM
        version: 1.0.0
      name: stable
    defaultChannel: stable
    packageName: ibm-spectrum-scale-csi-operator
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: redis-enterprise
      provider-url: ""
    name: redis-enterprise-operator-aplpha-cert
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/redis-enterprise-operator-aplpha-cert
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: redis-enterprise-operator.v5.4.10-8
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "app.redislabs.com/v1",
                "kind": "RedisEnterpriseCluster",
                "metadata": {
                  "name": "example-redisenterprisecluster"
                },
                "spec": {
                  "size": 3
                }
              }
            ]
          capabilities: Basic Install
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Deploy a rec
            displayName: RedisEnterpriseCluster
            kind: RedisEnterpriseCluster
            name: redisenterpriseclusters.app.redislabs.com
            version: v1
        description: The Redis Enterprise Operator is the fastest, most efficient
          way to deploy and maintain a Redis Enterprise Cluster in Kubernetes.
        displayName: Redis Enterprise Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: redis-enterprise
        version: 5.4.10-8
      name: alpha
    defaultChannel: alpha
    packageName: redis-enterprise-operator-aplpha-cert
    provider:
      name: redis-enterprise
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: IBM
      provider-url: ""
    name: ibm-block-csi-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ibm-block-csi-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ibm-block-csi-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "csi.ibm.com/v1",
                "kind": "IBMBlockCSI",
                "metadata": {
                  "name": "ibm-block-csi"
                },
                "spec": {
                  "controller": {
                    "repository": "ibmcom/ibm-block-csi-driver-controller",
                    "tag": "1.0.0",
                    "imagePullPolicy": "IfNotPresent",
                    "affinity": {
                      "nodeAffinity": {
                        "requiredDuringSchedulingIgnoredDuringExecution": {
                          "nodeSelectorTerms": [
                            {
                              "matchExpressions": [
                                {
                                  "key": "kubernetes.io/arch",
                                  "operator": "In",
                                  "values": [
                                    "amd64"
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      }
                    }
                  },
                  "node": {
                    "repository": "ibmcom/ibm-block-csi-driver-node",
                    "tag": "1.0.0",
                    "imagePullPolicy": "IfNotPresent",
                    "affinity": {
                      "nodeAffinity": {
                        "requiredDuringSchedulingIgnoredDuringExecution": {
                          "nodeSelectorTerms": [
                            {
                              "matchExpressions": [
                                {
                                  "key": "kubernetes.io/arch",
                                  "operator": "In",
                                  "values": [
                                    "amd64"
                                  ]
                                }
                              ]
                            }
                          ]
                        }
                      }
                    }
                  },
                  "sidecars": [
                    {
                      "name": "csi-node-driver-registrar",
                      "repository": "quay.io/k8scsi/csi-node-driver-registrar",
                      "tag": "v1.2.0",
                      "imagePullPolicy": "IfNotPresent"
                    },
                    {
                      "name": "csi-provisioner",
                      "repository": "quay.io/k8scsi/csi-provisioner",
                      "tag": "v1.3.0",
                      "imagePullPolicy": "IfNotPresent"
                    },
                    {
                      "name": "csi-attacher",
                      "repository": "quay.io/k8scsi/csi-attacher",
                      "tag": "v1.2.1",
                      "imagePullPolicy": "IfNotPresent"
                    },
                    {
                      "name": "livenessprobe",
                      "repository": "quay.io/k8scsi/livenessprobe",
                      "tag": "v1.1.0",
                      "imagePullPolicy": "IfNotPresent"
                    }
                  ]
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Storage,Cloud Provider
          certified: "true"
          containerImage: registry.connect.redhat.com/ibm/ibm-block-csi-operator:1.0.0
          createdAt: "2019-11-19T13:14:00Z"
          description: Run IBM block storage CSI driver on OpenShift.
          repository: https://github.com/IBM/ibm-block-csi-operator
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an block storage CSI driver
            displayName: IBM block storage CSI driver
            kind: IBMBlockCSI
            name: ibmblockcsis.csi.ibm.com
            version: v1
        description: "**IBM block storage CSI driver** is a Container Storage Interface
          (CSI) Driver for IBM block storage systems which enables container orchestrators
          to manage the life cycle of persistent storage.\n\nThis is the official
          operator to deploy and manage IBM block storage CSI driver.\n\nSupported
          container platforms:\n  - OpenShift v4.2\n  - Kubernetes v1.14\n\nSupported
          IBM storage systems:\n  - IBM FlashSystem 9100\n  - IBM Spectrum Virtualize\n
          \ - IBM Storwize\n  - IBM FlashSystem A9000/R\n\nSupported operating systems:\n
          \ - RHEL 7.x (x86 architecture)\n\n> **Note**: The operator must be installed
          in the `kube-system` project.\n\nFull documentation can be found on the
          [IBM knowledge center](https://www.ibm.com/support/knowledgecenter/SSRQ8T).\n\n\n##
          Prerequisites\n\n### Preparing worker nodes\nPerform these steps for each
          worker node in OpenShift:\n\n#### 1. Install Linux packages to ensure Fibre
          Channel and iSCSI connectivity.\nSkip this step if the packages are already
          installed.\n\nRHEL 7.x:\n```bash\n$ yum -y install iscsi-initiator-utils
          \  # only if iSCSI connectivity is required\n$ yum -y install xfsprogs                #
          Only if xfs filesystem is required.\n```\n\n#### 2. Configure Linux multipath
          devices on the host.\nCreate and set the relevant storage system parameters
          in the `/etc/multipath.conf` file.\nYou can also use the default `multipath.conf`
          file located in the `/usr/share/doc/device-mapper-multipath-*` directory.\nVerify
          that the `systemctl status multipathd` output indicates that the multipath
          status is active and error-free.\n\nRHEL 7.x:\n```bash\n$ yum install device-mapper-multipath\n$
          modprobe dm-multipath\n$ systemctl enable multipathd\n$ systemctl start
          multipathd\n$ systemctl status multipathd\n$ multipath -ll\n```\n\nImportant:
          When configuring Linux multipath devices, verify that the `find_multipaths`
          parameter in the `multipath.conf` file is disabled.\n  - RHEL 7.x: Remove
          the `find_multipaths yes` string from the `multipath.conf` file.\n\n####
          3. Configure storage system connectivity.\n3.1. Define the hostname of each
          OpenShift worker node on the relevant storage systems with the valid WWPN
          or IQN of the node.\n\n3.2. For Fibre Channel, configure the relevant zoning
          from the storage to the host.\n\n3.3. For iSCSI, perform these three steps.\n\n3.3.1.
          Make sure that the login used to log in to the iSCSI targets is permanent
          and remains available after a reboot of the worker node. To do this, verify
          that the node.startup in the /etc/iscsi/iscsid.conf file is set to automatic.
          If not, set it as required and then restart the iscsid service `$> service
          iscsid restart`.\n\n3.3.2. Discover and log into at least two iSCSI targets
          on the relevant storage systems. (NOTE: Without at least two ports, multipath
          device will not be created.)\n\n```bash\n$ iscsiadm -m discoverydb -t st
          -p ${STORAGE-SYSTEM-iSCSI-PORT-IP1}:3260 --discover\n$ iscsiadm -m node
          -p ${STORAGE-SYSTEM-iSCSI-PORT-IP1} --login\n\n$ iscsiadm -m discoverydb
          -t st -p ${STORAGE-SYSTEM-iSCSI-PORT-IP2}:3260 --discover\n$ iscsiadm -m
          node -p ${STORAGE-SYSTEM-iSCSI-PORT-IP2} --login\n```\n\n3.3.3. Verify that
          the login was successful and display all targets that you logged into. The
          portal value must be the iSCSI target IP address.\n\n```bash\n$ iscsiadm
          -m session --rescan\nRescanning session [sid: 1, target: {storage system
          IQN},\nportal: {STORAGE-SYSTEM-iSCSI-PORT-IP1},{port number}\nportal: {STORAGE-SYSTEM-iSCSI-PORT-IP2},{port
          number}\n```\n\n\n\n## Configuring k8s secret and storage class\nIn order
          to use the driver, create the relevant storage classes and secrets, as needed.
          \nNote: This section can be done also after the operator installation.\n\nThis
          section describes how to:\n 1. Create a storage system secret - to define
          the storage system credentials (user and password) and its address.\n 2.
          Configure the k8s storage class - to define the storage system pool name,
          secret reference, SpaceEfficiency (thin, compressed, or deduplicated) and
          fstype (xfs\\ext4).\n\n#### 1. Create an array secret \nCreate a secret
          file `array-secret.yaml` as follows and update the relevant credentials:\n\n```\nkind:
          Secret\napiVersion: v1\nmetadata:\n  name: <VALUE-1>\n  namespace: kube-system\ntype:
          Opaque\nstringData:\n  management_address: <VALUE-2,VALUE-3> # Array management
          addresses\n  username: <VALUE-4>                   # Array username  \ndata:\n
          \ password: <VALUE-5 base64>            # Array password\n```\n\nApply the
          secret:\n\n```bash\n$ kubectl apply -f array-secret.yaml\n```\n\n#### 2.
          Create storage classes\n\nCreate a storage class `storageclass-gold.yaml`
          file as follows, with the relevant capabilities, pool and, array secret:\n\n```\nkind:
          StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: gold\nprovisioner:
          block.csi.ibm.com\nparameters:\n  #SpaceEfficiency: <VALUE>    # Optional:
          Values applicable for Storwize are: thin, compressed, or deduplicated\n
          \ pool: <VALUE_POOL_NAME>\n\n  csi.storage.k8s.io/provisioner-secret-name:
          <VALUE_ARRAY_SECRET>\n  csi.storage.k8s.io/provisioner-secret-namespace:
          <VALUE_ARRAY_SECRET_NAMESPACE>\n  csi.storage.k8s.io/controller-publish-secret-name:
          <VALUE_ARRAY_SECRET>\n  csi.storage.k8s.io/controller-publish-secret-namespace:
          <VALUE_ARRAY_SECRET_NAMESPACE>\n\n  csi.storage.k8s.io/fstype: xfs   # Optional:
          Values ext4/xfs. The default is ext4.\n```\n\nApply the storage class:\n\n```bash\n$
          kubectl apply -f storageclass-gold.yaml\nstorageclass.storage.k8s.io/gold
          created\n```\n"
        displayName: Operator for IBM block storage CSI driver
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: IBM
        version: 1.0.0
      name: stable
    defaultChannel: stable
    packageName: ibm-block-csi-operator
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Containers & PaaS CoP
      provider-url: ""
    name: cert-utils-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cert-utils-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: cert-utils-operator.v0.0.3
      currentCSVDesc:
        annotations:
          alm-examples: |
            []
          capabilities: Full Lifecycle
          categories: Security
          certified: "false"
          containerImage: quay.io/redhat-cop/cert-utils-operator:latest
          createdAt: 5/26/2019
          description: Set of utilities for TLS certificates
          repository: https://github.com/redhat-cop/cert-utils-operator
          support: Best Effort
        apiservicedefinitions: {}
        customresourcedefinitions: {}
        description: |
          Cert utils operator is a set of functionalities around certificates packaged in a [Kubernetes operator](https://github.com/operator-framework/operator-sdk).

          Certificates are assumed to be available in a [secret](https://kubernetes.io/docs/concepts/configuration/secret/) of type `kubernetes.io/tls` (other types of secrets are *ignored* by this operator).
          By convention this type of secrets have three optional entries:

          1. `tls.key`: the private key of the certificate.
          2. `tls.crt`: the actual certificate.
          3. `ca.crt`: the CA bundle that validates the certificate.

          The functionalities are the following:

          1. [Ability to populate route certificates](https://github.com/redhat-cop/cert-utils-operator#Populating-route-certificates)
          2. [Ability to create java keystore and truststore from the certificates](https://github.com/redhat-cop/cert-utils-operator#Creating-java-keystore-and-truststore)
          3. [Ability to show info regarding the certificates](https://github.com/redhat-cop/cert-utils-operator#Showing-info-on-the-certificates)
          4. [Ability to alert when a certificate is about to expire](https://github.com/redhat-cop/cert-utils-operator#Alerting-when-a-certificate-is-about-to-expire)
          5. [Ability to inject ca bundles in ValidatingWebhookConfiguration, MutatingWebhookConfiguration, CustomResourceDefinition object](https://github.com/redhat-cop/cert-utils-operator#CA-injection)

          All these feature are activated via opt-in annotations.
        displayName: Cert Utils Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Containers & PaaS CoP
        version: 0.0.3
      name: alpha
    defaultChannel: alpha
    packageName: cert-utils-operator
    provider:
      name: Containers & PaaS CoP
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Submariner
      provider-url: ""
    name: submariner
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/submariner
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: submariner-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "submariner.io/v1alpha1",
                "kind": "Submariner",
                "metadata": {
                  "name": "submariner"
                },
                "spec": {
                  "size": 3,
                  "namespace": "submariner",
                  "serviceCIDR": "100.95.0.0/16",
                  "clusterCIDR": "10.245.0.0/16",
                  "token": "SECRET TO FILL IN",
                  "clusterID": "clusterid",
                  "colorCodes": "blue",
                  "debug": false,
                  "natEnabled": false,
                  "broker": "k8s",
                  "brokerK8sApiServer":"URL TO FILL IN",
                  "brokerK8sApiServerToken": "SECRET TO FILL IN",
                  "brokerK8sRemoteNamespace": "submariner",
                  "brokerK8sCA": "CA TO FILL IN",
                  "ceIPSecPSK": "SHARED PSK",
                  "ceIPSecDebug": false,
                  "ceIPSecIKEPort": 500,
                  "ceIPSecNATTPort": 4500,
                  "repository": "quay.io/submariner",
                  "version": "0.0.1"
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: quay.io/submariner/submariner-operator:0.0.1
          createdAt: "2019-10-14T07:00:00Z"
          description: Operator that creates and manages Submariner engines and route
            agents.
          repository: https://github.com/submariner-io/submariner
          support: Submariner
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: The Submariner engine.
            displayName: Submariner
            kind: Submariner
            name: submariners.submariner.io
            version: v1alpha1
          - description: Represents a Submariner endpoint.
            displayName: Endpoint
            kind: Endpoint
            name: endpoints.submariner.io
            version: v1
          - description: Represents a cluster of Submariner nodes.
            displayName: Cluster
            kind: Cluster
            name: clusters.submariner.io
            version: v1
        description: "[Submariner](https://submariner.io) provides connectivity between
          multiple Kubernetes clusters.\nIt establishs IPsec tunnels between each
          Kubernetes cluster.\nIt uses a central broker to facilitate exchange of
          information and sync CRDs between clusters.\nThe two primary Submariner
          components within connected clusters are:\n\n  * the Submariner engine (deployment)\n
          \ * the Submariner route agent (daemonset)\n\nSubmariner pods run on gateway
          nodes and perform leader election to elect an active IPSec endpoint.\nThis
          also updates the broker with local cluster information which is then shared
          to other clusters.\n\nThe Submariner route agent configures routes to enable
          full connectivity among all nodes between the clusters.\n\nThis operator
          uses a single CRD to control deployment of all Submariner-related resources.\n\n##
          Prerequisites\n  * The two (or more) Kubernetes clusters which need to be
          connected.\n  * A central broker, which can be a separate cluster or one
          of\n    the connected clusters, and must be accessible by all\n    connected
          clusters.\n  * Different cluster/service CIDRs (as well as different\n    Kubernetes
          DNS suffixes) between clusters.\n    This is to prevent traffic selector/policy/routing
          conflicts.\n  * Direct IP connectivity between instances through the Internet\n
          \   (or on the same network if not running Submariner over the\n    Internet).\n
          \   Submariner supports 1:1 NAT setups, but has a few\n    caveats/provider
          specific configuration instructions in this\n    configuration.\n  * Knowledge
          of each cluster's network configuration.\n\n## Deploying the broker\nThis
          currently requires Helm and Tiller. On the cluster which will host the broker:\n\n
          \   helm repo add submariner-latest https://submariner-io.github.io/submariner-charts/charts\n
          \   helm repo update\n    helm install submariner-latest/submariner-k8s-broker
          --name submariner-k8s-broker --namespace submariner-k8s-broker\n\nThree
          values must be retrieved from the broker; they will be used in the CR for
          the operator:\n\n  * the broker URL:\n  \n        kubectl -n default get
          endpoints kubernetes -o jsonpath=\"{.subsets[0].addresses[0].ip}:{.subsets[0].ports[?(@.name=='https')].port}\"\n
          \ \n  * the broker CA:\n\n        kubectl -n submariner-k8s-broker get secrets
          -o jsonpath=\"{.items[?(@.metadata.annotations['kubernetes\\.io/service-account\\.name']=='submariner-k8s-broker-client')].data['ca\\.crt']}\"\n\n
          \ * the broker token:\n\n        kubectl -n submariner-k8s-broker get secrets
          -o jsonpath=\"{.items[?(@.metadata.annotations['kubernetes\\.io/service-account\\.name']=='submariner-k8s-broker-client')].data.token}\"|base64
          --decode\n\n[The detailed broker installation instructions](https://github.com/submariner-io/submariner/blob/master/README.md#broker-installationsetup)\nprovide
          more details.\n\n## Deploying Submariner\nSubmariner is deployed using the
          Submariner CR. The following values can be specified:\n  * `namespace`:
          the namespace to install in;\n  * `serviceCIDR`: the service CIDR;\n  *
          `clusterCIDR`: the cluster CIDR;\n  * `token`: the Submariner token\n    (`kubectl
          -n submariner get secrets -o jsonpath=\"{.items[?(@.metadata.annotations['kubernetes\\.io/service-account\\.name']=='submariner-client')].data.token}\"|base64
          --decode`\n    on the broker cluster, assuming the broker is running in
          the `submariner` namespace â€” replace the two\n    `submariner` instances
          as appropriate);\n  * `clusterID`: the Submariner cluster ID;\n  * `colorCodes`:
          the Submariner color code(s) for this cluster;\n  * `debug`: `'true'` to
          enable logging debugging information;\n  * `natEnabled`: `'true'` to enable
          NAT, for use in clusters with 1:1 NAT between instances (such as\n    AWS
          VPC);\n  * `broker`: this must be `k8s` currently;\n  * `brokerK8sApiServer`:
          the brokerâ€™s API server URL\n    (`kubectl -n default get endpoints kubernetes
          -o jsonpath=\"{.subsets[0].addresses[0].ip}:{.subsets[0].ports[?(@.name=='https')].port}\"`\n
          \   on the broker cluster);\n  * `brokerK8sApiServerToken`: the brokerâ€™s
          API server token (same as `submariner_token`);\n  * `brokerK8sRemoteNamespace`:
          the brokerâ€™s namespace;\n  * `brokerK8sCA`: the brokerâ€™s certificate authority\n
          \   (`kubectl -n submariner get secrets -o jsonpath=\"{.items[?(@.metadata.annotations['kubernetes\\.io/service-account\\.name']=='submariner-client')].data['ca\\.crt']}\"`\n
          \   on the broker cluster, assuming the broker is running in the `submariner`
          namespace â€” replace the two\n    `submariner` instances as appropriate);\n
          \ * `ceIPSecPSK`: the IPsec PSK (which must be identical in all route agents
          across the cluster);\n  * `ceIPSecDebug`: `'true'` to enable logging IPsec
          debugging information (from the IPsec daemon);\n  * `ceIPSecIKEPort`: the
          IPsec IKE port (500 usually);\n  * `ceIPSecNATTPort`: the IPsec NAT traversal
          port (4500 usually);\n  * `repository`: the container repository to use.\n
          \ * `version`: the container version to use.\n"
        displayName: Submariner
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Submariner
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: submariner
    provider:
      name: Submariner
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: NuoDB, Inc.
      provider-url: ""
    name: nuodb-ce-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/nuodb-ce-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: nuodb-operator.v2.0.3
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
               "apiVersion": "nuodb.com/v2alpha1",
               "kind": "Nuodb",
               "metadata": {
                  "name": "nuodb"
               },
               "spec": {
                  "dbAvailability" : "manual",
                  "storageMode" : "ephemeral",
                  "insightsEnabled" : true,
                  "adminCount" : 1,
                  "adminStorageSize" : "5G",
                  "adminStorageClass" : "local-disk",
                  "dbName" : "test1",
                  "dbUser" : "dba",
                  "dbPassword" : "secret",
                  "smMemory" : "2Gi",
                  "smCount" : 1,
                  "smCpu" : "1",
                  "smStorageSize" : "20G",
                  "smStorageClass" : "local-disk",
                  "engineOptions" : "",
                  "teCount" : 1,
                  "teMemory" : "2Gi",
                  "teCpu" : "1",
                  "apiServer" : "https://domain:8888",
                  "container" : "nuodb/nuodb-ce:latest"
               }
            },
            {
               "apiVersion": "nuodb.com/v2alpha1",
               "kind": "NuodbYcsbWl",
               "metadata": {
                  "name": "nuodbycsbwl"
               },
               "spec": {
                  "dbName": "test1",
                  "ycsbWorkloadCount": 1,
                  "ycsbLoadName": "ycsb-load",
                  "ycsbWorkload": "b",
                  "ycsbLbPolicy": "",
                  "ycsbNoOfProcesses": 2,
                  "ycsbNoOfRows": 10000,
                  "ycsbNoOfIterations": 0,
                  "ycsbOpsPerIteration": 10000,
                  "ycsbMaxDelay": 240000,
                  "ycsbDbSchema": "User1",
                  "ycsbContainer": "nuodb/ycsb:latest"
               }
            },
            {
               "apiVersion": "nuodb.com/v2alpha1",
               "kind": "NuodbInsightsServer",
               "metadata": {
                  "name": "insightsserver"
               },
               "spec": {
                  "elasticVersion": "7.3.0",
                  "elasticNodeCount": 1,
                  "kibanaVersion": "7.3.0",
                  "kibanaNodeCount": 1
               }
            }]
          capabilities: Basic Install
          categories: Database,OpenShift Optional
          certified: "true"
          containerImage: registry.connect.redhat.com/nuodb/nuodb-operator:latest
          createdAt: "2019-06-06 12:59:59"
          description: This Operator will deploy the Community Edition of NuoDB
          repository: https://github.com/nuodb/nuodb-operator
          support: NuoDB, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a NuoDB instance
            displayName: NuoDB Server
            kind: Nuodb
            name: nuodbs.nuodb.com
            version: v2alpha1
          - description: Represents a YCSB instance
            displayName: YCSB Server
            kind: NuodbYcsbWl
            name: nuodbycsbwls.nuodb.com
            version: v2alpha1
          - description: Represents a NuoDB Insights instance
            displayName: NuoDb Insights Server
            kind: NuodbInsightsServer
            name: nuodbinsightsservers.nuodb.com
            version: v2alpha1
        description: |
          The NuoDB Kubernetes Operator deploys the NuoDB database on OpenShift 3.11 or greater. It also supports either ephemeral or persistent storage options with configurations to run NuoDB Insights, a visual database monitoring Web UI, and start a sample application (ycsb) to quickly generate a configurable SQL workload against the database.  See: [NuoDB's github] (https://github.com/nuodb/nuodb-operator) for requirements and deployment instructions.

          About NuoDB

          NuoDB provides a modern cloud-native, cloud-agnostic distributed SQL database that delivers on-demand scale-out and continuous availability for high throughput transactional workloads. Deploying new applications and delivering updates to existing applications rapidly are critical global business imperatives for enterprises today. NuoDBâ€™s cloud-native distributed SQL database enables your organization to do both. Built on the radical notion that a database should never be what holds your applications or business back, weâ€™ve liberated the enterprise-critical database from its inherent limitations. That frees businesses to focus on creating breakthrough solutions, built on a database designed for modern architectures. See: [NuoDB's website] (https://nuodb.com) for more information.
        displayName: NuoDB Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: NuoDB, Inc.
        version: 2.0.3
      name: alpha
    defaultChannel: alpha
    packageName: nuodb-ce-certified
    provider:
      name: NuoDB, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Containers & PaaS CoP
      provider-url: ""
    name: microsegmentation-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/microsegmentation-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: microsegmentation-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |
            []
          capabilities: Full Lifecycle
          categories: Security,Networking
          certified: "false"
          containerImage: quay.io/redhat-cop/microsegmentation-operator:latest
          createdAt: 5/28/2019
          description: This operator deducts and creates network policies from services
            definition
          repository: https://github.com/redhat-cop/microsegmentation-operator
          support: Best Effort
        apiservicedefinitions: {}
        customresourcedefinitions: {}
        description: |
          The microsegmentation operator allows to create [NetworkPolicies](https://kubernetes.io/docs/concepts/services-networking/network-policies/) rules starting from [Services](https://kubernetes.io/docs/concepts/services-networking/service/).

          This feature is activated by this annotation: `microsegmentation-operator.redhat-cop.io/microsegmentation: "true"`.

          By default the generated NetworkPolicy will allow traffic from pods in the same namespace and to the ports described in the service.

          The intent of this operator is to offer an easy way to adopt NetworkPolicies. It allows to automatically create sensible network Policies around your pods without having to directly manage the NetworkPolicy objects. This operator can be used to slowly adopt NetworkPolicies by starting with some sensible defaults and then let the users create more complex configurations.

          For more information this the [github repo](https://github.com/redhat-cop/microsegmentation-operator)
        displayName: Microsegmentation Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Containers & PaaS CoP
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: microsegmentation-operator
    provider:
      name: Containers & PaaS CoP
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: keycloak-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/keycloak-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: keycloak-operator.8.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "keycloak.org/v1alpha1",
                "kind": "Keycloak",
                "metadata": {
                  "name": "example-keycloak",
                  "labels": {
                    "app": "sso"
                  }
                },
                "spec": {
                  "instances": 1,
                  "extensions": [
                    "https://github.com/aerogear/keycloak-metrics-spi/releases/download/1.0.4/keycloak-metrics-spi-1.0.4.jar"
                  ],
                  "externalAccess": {
                    "enabled": true
                  }
                }
              },
              {
                "apiVersion": "keycloak.org/v1alpha1",
                "kind": "KeycloakRealm",
                "metadata": {
                  "name": "example-keycloakrealm"
                },
                "spec": {
                  "realm": {
                    "id": "basic",
                    "realm": "basic",
                    "enabled": true,
                    "displayName": "Basic Realm"
                  },
                  "instanceSelector": {
                    "matchLabels": {
                      "app": "sso"
                    }
                  }
                }
              },
              {
                "apiVersion": "keycloak.org/v1alpha1",
                "kind": "KeycloakBackup",
                "metadata": {
                  "name": "example-keycloakbackup"
                }
              },
              {
                "apiVersion": "keycloak.org/v1alpha1",
                "kind": "KeycloakClient",
                "metadata": {
                  "name": "client-secret",
                  "labels": {
                    "app": "sso"
                  }
                },
                "spec": {
                  "realmSelector": {
                    "matchLabels": {
                      "app": "sso"
                    }
                  },
                  "client": {
                    "clientId": "client-secret",
                    "secret": "client-secret",
                    "clientAuthenticatorType": "client-secret",
                    "protocol": "openid-connect"
                  }
                }
              },
              {
                "apiVersion": "keycloak.org/v1alpha1",
                "kind": "KeycloakUser",
                "metadata": {
                  "name": "example-realm-user",
                  "labels": {
                    "app": "sso"
                  }
                },
                "spec": {
                  "user": {
                    "username": "realm_user",
                    "firstName": "John",
                    "lastName": "Doe",
                    "email": "user@example.com",
                    "enabled": true,
                    "emailVerified": false
                  },
                  "realmSelector": {
                    "matchLabels": {
                      "app": "sso"
                    }
                  }
                }
              }
            ]
          capabilities: Deep Insights
          categories: Security
          certified: "False"
          containerImage: quay.io/keycloak/keycloak-operator:8.0.1
          createdAt: "2012-01-10 00:00:00"
          description: An Operator for installing and managing Keycloak
          repository: https://github.com/keycloak/keycloak-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Keycloak Instance
            displayName: Keycloak
            kind: Keycloak
            name: keycloaks.keycloak.org
            version: v1alpha1
          - description: Represents a Keycloak Realm
            displayName: KeycloakRealm
            kind: KeycloakRealm
            name: keycloakrealms.keycloak.org
            version: v1alpha1
          - description: Represents a Keycloak Backup
            displayName: KeycloakBackup
            kind: KeycloakBackup
            name: keycloakbackups.keycloak.org
            version: v1alpha1
          - description: Represents a Keycloak Client
            displayName: KeycloakClient
            kind: KeycloakClient
            name: keycloakclients.keycloak.org
            version: v1alpha1
          - description: Represents a Keycloak User
            displayName: KeycloakUser
            kind: KeycloakUser
            name: keycloakusers.keycloak.org
            version: v1alpha1
        description: |
          A Kubernetes Operator based on the Operator SDK for installing and managing Keycloak.

          Keycloak lets you add authentication to applications and secure services with minimum fuss. No need to deal with storing users or authenticating users. It's all available out of the box.

          The operator can deploy and manage Keycloak instances on Kubernetes and OpenShift.
          The following features are supported:

          * Install Keycloak to a namespace
          * Import Keycloak Realms
          * Import Keycloak Clients
          * Import Keycloak Users
          * Create scheduled backups of the database
          * Install Extensions
        displayName: Keycloak Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 8.0.1
      name: alpha
    defaultChannel: alpha
    packageName: keycloak-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Kong Inc
      provider-url: ""
    name: kong
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kong
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kong.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"Kong","metadata":{"name":"example-kong"},"spec":{"admin":{"annotations":{},"containerPort":8444,"ingress":{"annotations":{},"enabled":false,"hosts":[],"path":"/"},"servicePort":8444,"type":"NodePort","useTLS":true},"cassandra":{"enabled":false},"env":{"database":"postgres"},"image":{"pullPolicy":"IfNotPresent","repository":"kong","tag":1.1},"ingressController":{"enabled":false,"image":{"repository":"registry.connect.redhat.com/kong/kong-operator1","tag":"v0.1.0-2"},"ingressClass":"kong","installCRDs":true,"livenessProbe":{"failureThreshold":3,"httpGet":{"path":"/healthz","port":10254,"scheme":"HTTP"},"initialDelaySeconds":30,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":5},"rbac":{"create":true},"readinessProbe":{"failureThreshold":3,"httpGet":{"initialDelaySeconds":30,"path":"/healthz","port":10254,"scheme":"HTTP"},"periodSeconds":10,"successThreshold":1,"timeoutSeconds":5},"replicaCount":1,"serviceAccount":{"create":true,"name":null}},"livenessProbe":{"failureThreshold":5,"httpGet":{"path":"/status","port":"admin","scheme":"HTTPS"},"initialDelaySeconds":30,"periodSeconds":30,"successThreshold":1,"timeoutSeconds":5},"nodeSelector":{},"podAnnotations":{},"postgresql":{"enabled":true,"postgresqlDatabase":"kong","postgresqlPassword":"imnotwhatyouarelookingfor","postgresqlUsername":"kong","service":{"port":5432}},"proxy":{"annotations":{},"externalIPs":[],"http":{"containerPort":8000,"enabled":true,"servicePort":80},"ingress":{"annotations":{},"enabled":false,"hosts":[],"path":"/"},"tls":{"containerPort":8443,"enabled":true,"servicePort":443},"type":"NodePort"},"readinessProbe":{"failureThreshold":5,"httpGet":{"path":"/status","port":"admin","scheme":"HTTPS"},"initialDelaySeconds":30,"periodSeconds":10,"successThreshold":1,"timeoutSeconds":1},"replicaCount":1,"resources":{},"runMigrations":true,"tolerations":[],"waitImage":{"repository":"busybox","tag":"latest"}}}]'
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.connect.redhat.com/kong/kong-operator1:v0.1.0-2
          createdAt: "2019-05-03T12:00:00Z"
          description: Install and manage Kong clusters.
          repository: https://github.com/kong/kong-operator
          support: Harry Bagdi
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A configuration file for a Kong cluster.
            displayName: Kong
            kind: Kong
            name: kongs.charts.helm.k8s.io
            version: v1alpha1
        description: Install and manage Kong clusters.
        displayName: Kong Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Kong Inc
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: kong
    provider:
      name: Kong Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Hazelcast, Inc
      provider-url: ""
    name: hazelcast-enterprise-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/hazelcast-enterprise-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: hazelcast-operator.v0.1.9
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
             "apiVersion": "hazelcast.com/v1alpha1",
             "kind": "Hazelcast",
             "metadata": {
                "name": "hz"
             },
             "spec": {
                "hazelcast": {
                  "licenseKeySecretName": "hz-license-key-secret"
                }
             }
            }]
          capabilities: Basic Install
          categories: Database
          certified: "true"
          containerImage: registry.connect.redhat.com/hazelcast/hazelcast-operator:0.1.9
          createdAt: "2020-01-17 09:58:52"
          description: Install Hazelcast Enterprise cluster.
          repository: https://github.com/hazelcast/hazelcast-operator
          support: Hazelcast, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Hazelcast Enterprise cluster.
            displayName: Hazelcast Enterprise
            kind: Hazelcast
            name: hazelcasts.hazelcast.com
            version: v1alpha1
        description: |
          Hazelcast IMDG Enterprise is the most widely used in-memory data grid with hundreds of thousands of installed clusters around the world. It offers caching solutions ensuring that data is in the right place when itâ€™s needed for optimal performance.

          ### Before Your Start

          Add a Secret within the Project that contains the Hazelcast License Key. If you don't have one, get a trial key from this [link](https://hazelcast.com/hazelcast-enterprise-download/trial/).

              $ oc create secret generic hz-license-key-secret --from-literal=key=LICENSE-KEY-HERE

          ### Configuration

          For the complete configuration options, please refer to the [Hazelcast Enterprise Helm Chart](https://github.com/hazelcast/charts/tree/master/stable/hazelcast-enterprise) description.
        displayName: Hazelcast Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Hazelcast, Inc
        version: 0.1.9
      name: alpha
    defaultChannel: alpha
    packageName: hazelcast-enterprise-certified
    provider:
      name: Hazelcast, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Ubixlabs
      provider-url: ""
    name: ubix-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ubix-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ubix-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "ubixlabs.com/v1alpha1",
                "kind": "Ubix",
                "metadata": {
                  "name": "example-ubix"
                },
                "spec": {
                  "cassandra": {
                    "enable": "yes",
                    "image": "ubix/cassandra",
                    "imageTag": "v2"
                  },
                  "defaultImageTag": "v1",
                  "elasticsearch": {
                    "enable": "yes",
                    "image": "ubix/elasticsearch",
                    "imageTag": "v2"
                  },
                  "hdfs": {
                    "datanodeImage": "ubix/hadoop",
                    "datanodeImageTag": "v2",
                    "datanodeReplicas": 1,
                    "enable": "yes",
                    "namenodeImage": "ubix/hadoop",
                    "namenodeImageTag": "v2"
                  },
                  "imageRegistry": "registry.ubix.io",
                  "metastore": {
                    "enable": "yes",
                    "image": "ubix/hive",
                    "imageTag": "v3"
                  },
                  "mongo": {
                    "enable": "yes",
                    "image": "ubix/mongo",
                    "imageTag": "v1"
                  },
                  "portal": {
                    "enable": "yes",
                    "image": "ubix/portal",
                    "imageTag": "v3"
                  },
                  "postgres": {
                    "enable": "yes",
                    "image": "ubix/postgres",
                    "imageTag": "v7"
                  },
                  "spark": {
                    "enable": "yes",
                    "masterImage": "ubix/spark",
                    "masterImageTag": "v4",
                    "workerImage": "ubix/spark",
                    "workerImageTag": "v4",
                    "workers": 3
                  },
                  "ubixDomain": "crc.ubix.io",
                  "ubixNamespace": "ubix",
                  "ui": {
                    "enable": "yes",
                    "nginxImage": "ubix/openresty",
                    "nginxImageTag": "v4",
                    "uxFrameImage": "ubix/ux-frame",
                    "uxFrameImageTag": "v4"
                  },
                  "workspaceManager": {
                    "enable": "yes",
                    "image": "ubix/workspace-manager",
                    "imageTag": "v2",
                    "sparkCoresMax": "2"
                  },
                  "zookeeper": {
                    "enable": "yes",
                    "image": "ubix/zookeeper",
                    "imageTag": "v3"
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: AI/Machine Learning,Big Data
          description: Ubix platform
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Custom resource for ubix deployment
            displayName: Ubix custom resource
            kind: Ubix
            name: ubixes.ubixlabs.com
            version: v1alpha1
        description: Ubix platform
        displayName: Ubix Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Ubixlabs
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: ubix-operator
    provider:
      name: Ubixlabs
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: fuse-apicurito
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/fuse-apicurito
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: apicuritooperator.v7.5.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "apicur.io/v1alpha1",
              "kind": "Apicurito",
              "metadata": {
                "name": "apicurito-service"
              },
              "spec": {
                "size": 3,
                "image": "registry.redhat.io/fuse7-tech-preview/fuse-apicurito:1.5"
              }
            }]
          capabilities: Seamless Upgrades
          categories: Integration & Delivery
          certified: "false"
          containerImage: registry.redhat.io/fuse7-tech-preview/fuse-apicurito-operator
          createdAt: "2019-05-08T16:12:00Z"
          description: Manages the installation and upgrades of the Fuse API Designer.
          repository: https://github.com/Apicurio/apicurio-operators/tree/master/apicurito
          support: Apicurito Project
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: CRD for Apicurito
            displayName: Apicurito CRD
            kind: Apicurito
            name: apicuritos.apicur.io
            version: v1alpha1
        description: |
          The API Designer is a small/minimal version of Apicurio, a standalone API design studio that can be used to create new or edit existing API designs (using the OpenAPI specification).
          This operator supports the installation and upgrade of apicurito. Apicurito components are:
            - apicurito-ui (apicurito application)
            - apicurito route (to access apicurito from outside openshift)

          ### How to install
          When the operator is installed (you have created a subscription and the operator is running in the selected namespace) create a new CR of Kind Apicurito (click the Create New button). The CR spec contains all defaults.

          At the moment, following fields are supported as part of the CR:
            - size: how many pods your the apicurito operand will have.
            - image: the apicurito image. Changing this image in an existing installation will trigger an upgrade of the operand.
          ### How to upgrade
          Upgrades are trigered by updating the image field in the CR. This can be done manually via the Openshift console, or with kubeclt:
          ```
          $ cat apicurito_cr.yaml
          apiVersion: apicur.io/v1alpha1
            kind: Apicurito
            metadata:
              name: apicurito-service
            spec:
              size: 3
              image: registry.redhat.io/fuse7-tech-preview/fuse-apicurito-operator:version
          $ kubectl apply -f apicurito_cr.yaml
          ```
        displayName: API Designer
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 7.5.0
      name: alpha
    defaultChannel: alpha
    packageName: fuse-apicurito
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: IBM
      provider-url: ""
    name: multicloud-operators-subscription
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/multicloud-operators-subscription
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: multicloud-operators-subscription.v0.1.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "app.ibm.com/v1alpha1",
                "kind": "Subscription",
                "metadata": {
                  "name": "sub-nginx"
                },
                "spec": {
                  "channel": "my-operator/dev-helmrepo",
                  "name": "nginx-ingress",
                  "placement": {
                    "local": false
                  },
                  "packageFilter": {
                    "version": "=1.26.1"
                  },
                  "packageOverrides": [
                    {
                      "packageName": "nginx-ingress",
                      "packageOverrides": [
                        {
                          "path": "spec.values",
                          "value": "defaultBackend:\n  replicaCount: 3\n"
                        }
                      ]
                    }
                  ]
                }
              },
              {
                "apiVersion": "app.ibm.com/v1alpha1",
                "kind": "Channel",
                "metadata": {
                  "name": "dev-helmrepo"
                },
                "spec": {
                  "type": "HelmRepo",
                  "pathname": "http://kubernetes-charts.storage.googleapis.com/"
                },
                "status": {
                  "state": "N/A"
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Integration & Delivery,OpenShift Optional
          certified: "false"
          containerImage: quay.io/multicloudlab/multicloud-operators-subscription:b2aa45c1
          createdAt: "2020-02-02T11:59:59Z"
          description: An operator to subscribe resources from a channel
          repository: https://github.com/IBM/multicloud-operators-subscription
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Subscribe resources from a channel according to its package
              filters
            displayName: Subscription
            kind: Subscription
            name: subscriptions.app.ibm.com
            version: v1alpha1
          - description: Represent a helm chart repository
            displayName: Channel
            kind: Channel
            name: channels.app.ibm.com
            version: v1alpha1
          - description: Represent a helm chart selected by the subscription, for
              internal use only.
            displayName: Helm Release
            kind: HelmRelease
            name: helmreleases.app.ibm.com
            version: v1alpha1
          - description: Contain a k8s resource template for deployment, for internal
              use only.
            displayName: Deployable
            kind: Deployable
            name: deployables.app.ibm.com
            version: v1alpha1
        description: "A multicloud subscription operator subscribes resources from
          a Channel and get them deployed to kubernetes clusters\n## Prerequisites\n-
          Operator Lifecycle Manager (OLM) needs to be installed.\n\nOLM installation
          can be checked by running the command:\n  ```\n  $ kubectl get pods --all-namespaces
          | grep olm\n  olm        catalog-operator-7f54797f5f-z9l9n         1/1     Running
          \           0          46s\n  olm        olm-operator-65874bdb76-vs6gf             1/1
          \    Running            0          46s\n  olm        operatorhubio-catalog-8wp2d
          \              1/1     Running            0          36s\n  olm        packageserver-7fb4588767-8mxv4
          \           1/1     Running            0          34s\n  olm        packageserver-7fb4588767-d6h8j
          \           1/1     Running            0          25s\n  ```\n\n## How to
          Install\n- Install `Multicloud Subscription Operator` by following instructions
          in top right button `Install`.\n\nA new pod `multicloud-operators-subscription`
          is created in `my-multicloud-operators-subscription` namespace\n\n```\n$
          kubectl get pods --all-namespaces | grep my-multicloud-operators-subscription\nmy-multicloud-operators-subscription
          \ multicloud-operators-subscription-554c564476-fl98z     1/1     Running
          \  0          13s\n```\n\nThe operator is now providing new Custom Resources
          Definitions: `subscriptions.app.ibm.com`, `channels.app.ibm.com`\n\n## Using
          the IBM Multicloud Subscription Operator\nHere is an example to demonstrate
          how to create one IBM Multicloud Subscription CR to subscribe the helm chart
          nginx-ingress v1.26.1\n\n-  Create a channel `dev-helmrepo` in namespace
          `dev` to point to a helm repo `kubernetes-charts.storage.googleapis.com`\n\n```\napiVersion:
          v1\nkind: Namespace\nmetadata:\n  name: dev\n---\napiVersion: app.ibm.com/v1alpha1\nkind:
          Channel\nmetadata:\n  name: dev-helmrepo\n  namespace: dev\nspec:\n    type:
          HelmRepo\n    pathname: http://kubernetes-charts.storage.googleapis.com/\n
          \   configRef: \n      name: skip-cert-verify\n      apiVersion: v1\n      kind:
          ConfigMap\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: skip-cert-verify\n
          \ namespace: dev\ndata:\n  insecureSkipVerify: \"true\"\n```\n\n-  Create
          a subscription `simple` in the `default` namespace to subscribe the nginx-ingress
          v1.26.1 package. The subscription is disabled by default.\n\n```\napiVersion:
          app.ibm.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: simple\nspec:\n
          \ channel: dev/dev-helmrepo\n  name: nginx-ingress\n  placement:\n    local:
          false\n  packageFilter:\n    version: =1.26.1\n  packageOverrides:\n  -
          packageName: nginx-ingress\n    packageOverrides:\n    - path: spec.values\n
          \     value: |\n        defaultBackend:\n          replicaCount: 3\n```\n\n-
          Deploy the nginx-ingress v1.26.1 package by setting the subscription `spec.placement.local`
          to `true`\n\n```\nkubectl patch subscriptions.app.ibm.com simple --type='json'
          -p='[{\"op\": \"replace\", \"path\": \"/spec/placement/local\", \"value\":
          true}]'\n```\n\n- Make sure the subscribed nginx-ingress package being deployed
          to the default namespace\n\n```\n$ kubectl get pods -n default |grep nginx\nnginx-ingress-controller-775b4967cb-8nx9m
          \       1/1     Running   0          118s\nnginx-ingress-default-backend-659bd647bd-bfkn8
          \  1/1     Running   0          118s\nnginx-ingress-default-backend-659bd647bd-fpcnp
          \  1/1     Running   0          118s\nnginx-ingress-default-backend-659bd647bd-v4nbf
          \  1/1     Running   0          118s\n```\n"
        displayName: Multicloud Subscription Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: IBM
        version: 0.1.1
      name: alpha
    defaultChannel: alpha
    packageName: multicloud-operators-subscription
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: knative-camel-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/knative-camel-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: knative-camel-operator.v0.12.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "sources.eventing.knative.dev/v1alpha1",
                "kind": "CamelSource",
                "metadata": {
                  "name": "camel-timer-source"
                },
                "spec": {
                  "source": {
                    "flow": {
                      "from": {
                        "uri": "timer:tick?period=3s",
                        "steps": [
                          {
                            "set-body": {
                              "constant": "Hello World!"
                            }
                          }
                        ]
                      }
                    }
                  },
                  "sink": {
                    "ref": {
                      "apiVersion": "messaging.knative.dev/v1alpha1",
                      "kind": "InMemoryChannel",
                      "name": "camel-test"
                    }
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/openshift-knative/knative-eventing-sources-camel-source-controller:v0.12.1
          createdAt: "2020-02-04T08:40:00Z"
          description: The Knative Camel addon provides a collection of eventing sources
            from the popular integration framework Apache Camel.
          repository: https://github.com/knative/eventing-sources
          support: Camel
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Knative Source based on Apache Camel
            displayName: Camel Source
            kind: CamelSource
            name: camelsources.sources.eventing.knative.dev
            version: v1alpha1
          required:
          - description: Represents a Camel K Integration
            displayName: Camel K Integration
            kind: Integration
            name: integrations.camel.apache.org
            version: v1
        description: |
          The Knative Camel addon provides a collection of eventing sources from the popular integration framework [Apache Camel](http://camel.apache.org/).
          Sources are based on [Camel K integrations](https://github.com/apache/camel-k), a subproject of Apache Camel for running integration code in the cloud.

          For documentation on using Knative Camel Sources, see the
          [Camel Source section](https://knative.dev/docs/eventing/samples/apache-camel-source/) of the
          [Knative documentation site](https://www.knative.dev/docs).

          The operator requires Camel K 1.0.0-RC1 to be installed in any namespace where you want to run Camel eventing sources. Please, refer to the
          [Camel K documentation](https://github.com/apache/camel-k) for installation instructions.

          Knative Serving and Eventing are also required for installing this operator.
        displayName: Knative Apache Camel Operator
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.12.1
      name: alpha
    defaultChannel: alpha
    packageName: knative-camel-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: GT Software, Inc.
      provider-url: ""
    name: ivory-server-app
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ivory-server-app
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ivory-server-operator.v0.0.9
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "ivoryserver.gtsoftware.com/v1alpha1",
                "kind": "IvoryServer",
                "metadata": {
                  "name": "ivory-server"
                },
                "spec": {
                  "size": 1
                }
              }
            ]
          capabilities: Basic Install
          categories: Application Runtime
          certified: "false"
          containerImage: registry.connect.redhat.com/gtsoftware/ivory-service-architect-operator:v0.0.9
          createdAt: "2019-12-01 12:00:00"
          description: An operator to deploy the Ivory Server Component of the Ivory
            Service Architect Suite.
          support: GT Software, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Ivory Server instance
            displayName: Ivory Server Operator
            kind: IvoryServer
            name: ivoryservers.ivoryserver.gtsoftware.com
            version: v1alpha1
        description: The Ivory Service Architect Operator deploys the Ivory Server
          Operator Component and supporting FTP service in a Kubernetes environment.
        displayName: Ivory Service Architect Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: GT Software, Inc.
        version: 0.0.9
      name: alpha
    defaultChannel: alpha
    packageName: ivory-server-app
    provider:
      name: GT Software, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Portworx
      provider-url: ""
    name: portworx-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/portworx-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: portworx-operator.v1.2.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "core.libopenstorage.org/v1alpha1",
                "kind": "StorageCluster",
                "metadata": {
                  "name": "portworx",
                  "namespace": "test-operator",
                  "annotations": {
                    "portworx.io/is-openshift": "true"
                  }
                },
                "spec": {
                  "image": "registry.connect.redhat.com/portworx/px-monitor:2.3.4",
                  "userInterface": {
                    "enabled": true
                  },
                  "autopilot": {
                    "enabled": false
                  }
                }
              },
              {
                "apiVersion": "core.libopenstorage.org/v1alpha1",
                "kind": "StorageNode",
                "metadata": {
                  "name": "example",
                  "namespace": "test-operator"
                }
              }
            ]
          capabilities: Auto Pilot
          categories: Storage
          certified: "true"
          containerImage: registry.connect.redhat.com/portworx/openstorage-operator:1.2.0
          createdAt: "2020-02-03 08:00:00"
          description: Cloud native storage solution for production workloads
          repository: https://github.com/libopenstorage/operator
          support: Portworx, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Storage Cluster installs Portworx in the cluster. It has
              all the necessary configurations to setup and update a Portworx cluster.
            displayName: Storage Cluster
            kind: StorageCluster
            name: storageclusters.core.libopenstorage.org
            version: v1alpha1
          - description: DO NOT CREATE Storage Node as it is internally created by
              the operator. It represents the status of a Portworx node.
            displayName: Storage Node
            kind: StorageNode
            name: storagenodes.core.libopenstorage.org
            version: v1alpha1
        description: |
          Portworx-Enterprise is the most widely-used and reliable cloud-native
          storage solution for production workloads and provides high-availability,
          data protection and security for containerized applications.

          Portworx Enterprise enables you to migrate entire applications, including
          data, between clusters in a single data center or cloud, or between clouds,
          with a single kubectl command.

          The cloud native storage and data management platform that enterprises trust
          to manage data in containers now has an operator which simplifies the install,
          configuration, upgrades and manages the Portworx Enterprise cluster lifecycle.

          Learn more about the Portworx Enterprise
          [the data platform for Kubernetes](https://portworx.com/products/introduction)

          To learn more about the platform features, please visit our
          [product features page](https://portworx.com/products/features)

          ### About Portworx

          Portworx is the solution for running stateful containers in production,
          designed with DevOps in mind. With Portworx, users can manage any database
          or stateful service on any infrastructure using any container scheduler,
          including Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves
          the five most common problems DevOps teams encounter when running stateful
          services in production: persistence, high availability, data automation,
          security, and support for multiple data stores and infrastructure.

          ### How to install StorageCluster

          To customize your cluster's configuration (specification), use the
          [Openshift Spec Generator](https://openshift4.install.portworx.com/)

          ### Tutorials

          * [Portworx Enterprise on Openshift](https://portworx.com/openshift)

          * [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)

          * [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)

          * [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)

          * [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)

          ### Uninstall

          Deleting the StorageCluster object for Portworx cluster does not stop Portworx
          service running on the nodes, to avoid application downtime.

          To uninstall Portworx completely without wiping the data, you should add the
          following delete strategy to the StorageCluster spec:
          ```
          spec:
            deleteStrategy:
              type: Uninstall
          ```
          **Caution:** To uninstall Portworx and **wipe all the data**, you should use the following
          delete strategy:
          ```
          spec:
            deleteStrategy:
              type: UninstallAndWipe
          ```
        displayName: Portworx Enterprise
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Portworx
        version: 1.2.0
      name: alpha
    - currentCSV: portworx-operator.v1.2.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "core.libopenstorage.org/v1alpha1",
                "kind": "StorageCluster",
                "metadata": {
                  "name": "portworx",
                  "namespace": "test-operator",
                  "annotations": {
                    "portworx.io/is-openshift": "true"
                  }
                },
                "spec": {
                  "image": "registry.connect.redhat.com/portworx/px-monitor:2.3.4",
                  "userInterface": {
                    "enabled": true
                  },
                  "autopilot": {
                    "enabled": false
                  }
                }
              },
              {
                "apiVersion": "core.libopenstorage.org/v1alpha1",
                "kind": "StorageNode",
                "metadata": {
                  "name": "example",
                  "namespace": "test-operator"
                }
              }
            ]
          capabilities: Auto Pilot
          categories: Storage
          certified: "true"
          containerImage: registry.connect.redhat.com/portworx/openstorage-operator:1.2.0
          createdAt: "2020-02-03 08:00:00"
          description: Cloud native storage solution for production workloads
          repository: https://github.com/libopenstorage/operator
          support: Portworx, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Storage Cluster installs Portworx in the cluster. It has
              all the necessary configurations to setup and update a Portworx cluster.
            displayName: Storage Cluster
            kind: StorageCluster
            name: storageclusters.core.libopenstorage.org
            version: v1alpha1
          - description: DO NOT CREATE Storage Node as it is internally created by
              the operator. It represents the status of a Portworx node.
            displayName: Storage Node
            kind: StorageNode
            name: storagenodes.core.libopenstorage.org
            version: v1alpha1
        description: |
          Portworx-Enterprise is the most widely-used and reliable cloud-native
          storage solution for production workloads and provides high-availability,
          data protection and security for containerized applications.

          Portworx Enterprise enables you to migrate entire applications, including
          data, between clusters in a single data center or cloud, or between clouds,
          with a single kubectl command.

          The cloud native storage and data management platform that enterprises trust
          to manage data in containers now has an operator which simplifies the install,
          configuration, upgrades and manages the Portworx Enterprise cluster lifecycle.

          Learn more about the Portworx Enterprise
          [the data platform for Kubernetes](https://portworx.com/products/introduction)

          To learn more about the platform features, please visit our
          [product features page](https://portworx.com/products/features)

          ### About Portworx

          Portworx is the solution for running stateful containers in production,
          designed with DevOps in mind. With Portworx, users can manage any database
          or stateful service on any infrastructure using any container scheduler,
          including Kubernetes, Mesosphere DC/OS, and Docker Swarm. Portworx solves
          the five most common problems DevOps teams encounter when running stateful
          services in production: persistence, high availability, data automation,
          security, and support for multiple data stores and infrastructure.

          ### How to install StorageCluster

          To customize your cluster's configuration (specification), use the
          [Openshift Spec Generator](https://openshift4.install.portworx.com/)

          ### Tutorials

          * [Portworx Enterprise on Openshift](https://portworx.com/openshift)

          * [Stateful applications on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes/application-install-with-kubernetes)

          * [Portworx Enterprise on Kubernetes](https://docs.portworx.com/portworx-install-with-kubernetes)

          * [Kafka on Kubernetes](https://portworx.com/kafka-kubernetes)

          * [Elastisearch on Kubernetes](https://portworx.com/elasticsearch-kubernetes)

          ### Uninstall

          Deleting the StorageCluster object for Portworx cluster does not stop Portworx
          service running on the nodes, to avoid application downtime.

          To uninstall Portworx completely without wiping the data, you should add the
          following delete strategy to the StorageCluster spec:
          ```
          spec:
            deleteStrategy:
              type: Uninstall
          ```
          **Caution:** To uninstall Portworx and **wipe all the data**, you should use the following
          delete strategy:
          ```
          spec:
            deleteStrategy:
              type: UninstallAndWipe
          ```
        displayName: Portworx Enterprise
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Portworx
        version: 1.2.0
      name: stable
    defaultChannel: stable
    packageName: portworx-certified
    provider:
      name: Portworx
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: fuse-online
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/fuse-online
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: fuse-online-operator.v7.5.0
      currentCSVDesc:
        annotations:
          alm-examples: "[{\n    \"apiVersion\": \"syndesis.io/v1alpha1\",\n    \"kind\":
            \"Syndesis\",\n    \"metadata\": {\n    \t\"name\": \"app\"\n    },\n
            \   \"spec\": {\n    \t\"integration\": {\n    \t\t\"limit\": 0\n    \t}\n
            \   },\n    \"addons\": {\n      \"todo\": {\n        \"enabled\": \"false\"\n
            \     }\n    }\n}]\n"
          capabilities: Seamless Upgrades
          categories: Integration & Delivery
          certified: "false"
          containerImage: registry.redhat.io/fuse7/fuse-online-operator
          createdAt: "2019-10-08T16:12:00Z"
          description: Manages the installation of Fuse Online, a flexible and customizable
            open source platform that provides core integration capabilities as a
            service.
          repository: https://github.com/syndesisio/syndesis/
          support: Syndesis
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Syndesis CRD
            displayName: Syndesis CRD
            kind: Syndesis
            name: syndesises.syndesis.io
            version: v1alpha1
        description: |
          ### Fuse Online operator
          Fuse Online is a flexible and customizable, open source platform that provides core integration capabilities as a service.

          This operator installs as well as configures the following Fuse Online components:
          - syndesis-server
          - syndesis-meta
          - syndesis-ui
          - syndesis-db
          - syndesis-prometheus
          - syndesis-proxy
          - syndesis-oauthproxy

          ### Before you begin
          You must configure authentication to Red Hat container registry before you can import and use the Red Hat Fuse OpenShift Image Streams. Follow instruction given below to configure the registration to container registry.

          1. Log in to the OpenShift Server as an administrator, as follow:
              ```
              oc login -u system:admin
              ```
          2. Log in to the OpenShift project where you will be installing the operator.
              ```
              oc project fuse-online
              ```
          3. Create a docker-registry secret using either Red Hat Customer Portal account or Red Hat Developer Program account credentials.
              ```
              oc create secret docker-registry syndesis-pull-secret \
                --docker-server=registry.redhat.io \
                --docker-username=CUSTOMER_PORTAL_USERNAME \
                --docker-password=CUSTOMER_PORTAL_PASSWORD \
                --docker-email=EMAIL_ADDRESS
              ```
              NOTE: You need to create a docker-registry secret in every new namespace where the image streams reside and which use registry.redhat.io.

              If you do not wish to use your Red Hat account username and password to create the secret, it is recommended to create an authentication token using a [registry service account](https://access.redhat.com/terms-based-registry/).

          ### How to install
          - When the operator is installed (you have created a subscription and the operator is running in the selected namespace) and before you create a new CR of Kind Syndesis, you have to link the secret created in the previous section to the operator service account.
          ```
          oc secrets link syndesis-operator syndesis-pull-secret --for=pull
          ```

          - Create a new CR of Kind Syndesis (click the Create New button). The CR spec contains all defaults (see below).

          ### CR Defaults
          The CR definition is pretty simple and an empy CR will trigger a base installation.

          This version introduces the addons seccion which allows users to enable specific addons. The available addons at themoment are:
          - camelk: enables kamelk
          - jaeger: enable jaeger
          - ops: enables monitoring, requires extra CRDs
          - todo: a simple todo application

          To enable addons, set "addon_name": {"enabled": "true"} in the CR.
        displayName: Fuse Online Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 7.5.0
      name: alpha
    defaultChannel: alpha
    packageName: fuse-online
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: businessautomation-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/businessautomation-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: businessautomation-operator.1.3.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"app.kiegroup.org/v2","kind":"KieApp","metadata":{"name":"rhpam-trial"},"spec":{"environment":"rhpam-trial"}}]'
          capabilities: Seamless Upgrades
          categories: Integration & Delivery
          certified: "true"
          containerImage: registry.redhat.io/rhpam-7/rhpam-rhel8-operator:7.6.0
          createdAt: "2019-12-04 13:33:08"
          description: Business Automation Operator for deployment and management
            of RHPAM/RHDM environments.
          repository: https://github.com/kiegroup/kie-cloud-operator
          support: Red Hat, Inc.
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A project prescription running an RHPAM/RHDM environment.
            displayName: KieApp
            kind: KieApp
            name: kieapps.app.kiegroup.org
            version: v2
        description: Business Automation Operator for deployment and management of
          RHPAM/RHDM environments.
        displayName: Business Automation
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.3.0
      name: stable
    defaultChannel: stable
    packageName: businessautomation-operator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: quay
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/quay
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: quay.v1.0.2
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "redhatcop.redhat.io/v1alpha1",
                "kind": "QuayEcosystem",
                "metadata": {
                  "name": "example-quayecosystem"
                },
                "spec": {
                  "quay": {
                    "imagePullSecretName": "redhat-pull-secret"
                  }
                }
              },
              {
                "apiVersion": "redhatcop.redhat.io/v1alpha1",
                "kind": "QuayEcosystem",
                "metadata": {
                  "name": "example-quayecosystem"
                },
                "spec": {
                  "quay": {
                    "imagePullSecretName": "redhat-pull-secret",
                    "superuserCredentialsSecretName": "\u003csecret_name\u003e"
                  }
                }
              },
              {
                "apiVersion": "redhatcop.redhat.io/v1alpha1",
                "kind": "QuayEcosystem",
                "metadata": {
                  "name": "example-quayecosystem"
                },
                "spec": {
                  "clair": {
                    "enabled": true,
                    "imagePullSecretName": "redhat-pull-secret",
                    "updateInterval": "60m"
                  },
                  "quay": {
                    "configHostname": "example-quayecosystem-quay-enterprise.apps",
                    "configSecretName": "quay-config-app",
                    "database": {
                      "credentialsSecretName": "quay-database-credential",
                      "volumeSize": "10Gi"
                    },
                    "deploymentStrategy": "RollingUpdate",
                    "envVars": [
                      {
                        "name": "DEBUGLOG",
                        "value": "true"
                      }
                    ],
                    "imagePullSecretName": "redhat-pull-secret",
                    "registryStorage": {
                      "persistentVolumeAccessMode": [
                        "ReadWriteOnce"
                      ],
                      "persistentVolumeSize": "10Gi",
                      "persistentVolumeStorageClassName": "quay-storageclass"
                    },
                    "resources": {
                      "requests": {
                        "memory": "1000Mi"
                      }
                    },
                    "skipSetup": false,
                    "superuserCredentialsSecretName": "quay-super-user"
                  },
                  "redis": {
                    "credentialsSecretName": "quay-redis-password",
                    "imagePullSecretName": "redhat-pull-secret"
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/redhat-cop/quay-operator:v1.0.1
          createdAt: "2020-02-08"
          description: Red HatÂ® Quay is a private container registry that stores,
            builds, and deploys container images.
          repository: https://github.com/redhat-cop/quay-operator
          support: Red Hat Community of Practice
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Resources to support the Quay Container Registry and supporting
              components
            displayName: QuayEcosystem
            kind: QuayEcosystem
            name: quayecosystems.redhatcop.redhat.io
            version: v1alpha1
        description: |-
          [Red HatÂ® Quay](https://www.redhat.com/en/technologies/cloud-computing/quay) is a private container registry that stores, builds, and deploys container images. The Quay Operator streramlines the installation and configuration of Quay and its related ecosystem of components.

          ## Operator Features

          * Automated installation of Red Hat Quay
          * Provisions instance of Redis
          * Provisions Database(s) to support relational database for Quay and optional component Clair.
          * Generates SSL certificates for secure transport between components
          * Installation of Clair for container scanning and integration with Quay

          ## Prerequisites

          The Quay Operator and Quay ecosystem of components can be deployed into any namespace. By default, it is recommended that it be deployed to a namespace called `quay-enterprise`. Ensure the appropriate namespace exists prior to continuing.

          ### Access to Protected Registries

          Container images that support a the ecosystem of Quay components may originate from protected registries. Each component supports accessing images from these protected resources by specifying the name of a `Secret` containing credentials by specifying the `imagePullSecretName` property.

          ## Simplified Deployment

          The following example provisions a non persistent deployment of Quay, Redis and an ephemeral relational database:

          ```
          apiVersion: redhatcop.redhat.io/v1alpha1
          kind: QuayEcosystem
          metadata:
            name: example-quayecosystem
          spec:
            quay:
              imagePullSecretName: redhat-pull-secret
          ```

          More complex examples can be found in the [project documentation](https://github.com/redhat-cop/quay-operator).

          ## Documentation

          Documentation for the Quay Operator can be found on the [quay-operator](https://github.com/redhat-cop/quay-operator) project repository
        displayName: Quay
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.0.2
      name: stable
    defaultChannel: stable
    packageName: quay
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: local-storage-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/local-storage-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: local-storage-operator.4.2.18-202002031246
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "local.storage.openshift.io/v1",
                "kind": "LocalVolume",
                "metadata": {
                  "name": "example"
                },
                "spec": {
                  "storageClassDevices": [
                    {
                      "devicePaths": [
                          "/dev/vde",
                          "/dev/vdf"
                      ],
                      "fsType": "ext4",
                      "storageClassName": "foobar",
                      "volumeMode": "Filesystem"
                    }
                  ]
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Storage
          containerImage: quay.io/openshift/origin-local-storage-operator:4.2.0
          createdAt: "2019-08-14T00:00:00Z"
          description: Configure and use local storage volumes in kubernetes and Openshift
          repository: https://github.com/openshift/local-storage-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Manage local storage volumes for OpenShift
            displayName: Local Volume operator
            kind: LocalVolume
            name: localvolumes.local.storage.openshift.io
            version: v1
        description: Local Storage Operator
        displayName: Local Storage
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.2.18-202002031246
      name: "4.2"
    - currentCSV: local-storage-operator.4.2.18-202002031246-s390x
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "local.storage.openshift.io/v1",
                "kind": "LocalVolume",
                "metadata": {
                  "name": "example"
                },
                "spec": {
                  "storageClassDevices": [
                    {
                      "devicePaths": [
                          "/dev/vde",
                          "/dev/vdf"
                      ],
                      "fsType": "ext4",
                      "storageClassName": "foobar",
                      "volumeMode": "Filesystem"
                    }
                  ]
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Storage
          containerImage: quay.io/openshift/origin-local-storage-operator:4.2.0
          createdAt: "2019-08-14T00:00:00Z"
          description: Configure and use local storage volumes in kubernetes and Openshift
          repository: https://github.com/openshift/local-storage-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Manage local storage volumes for OpenShift
            displayName: Local Volume operator
            kind: LocalVolume
            name: localvolumes.local.storage.openshift.io
            version: v1
        description: Local Storage Operator
        displayName: Local Storage
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.2.18-202002031246
      name: 4.2-s390x
    - currentCSV: local-storage-operator.4.3.2-202002112006
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "local.storage.openshift.io/v1",
                "kind": "LocalVolume",
                "metadata": {
                  "name": "example"
                },
                "spec": {
                  "storageClassDevices": [
                    {
                      "devicePaths": [
                          "/dev/vde",
                          "/dev/vdf"
                      ],
                      "fsType": "ext4",
                      "storageClassName": "foobar",
                      "volumeMode": "Filesystem"
                    }
                  ]
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Storage
          containerImage: quay.io/openshift/origin-local-storage-operator:4.3.0
          createdAt: "2019-08-14T00:00:00Z"
          description: Configure and use local storage volumes in kubernetes and Openshift
          olm.skipRange: '>=4.3.0 <4.4.0'
          repository: https://github.com/openshift/local-storage-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Manage local storage volumes for OpenShift
            displayName: Local Volume operator
            kind: LocalVolume
            name: localvolumes.local.storage.openshift.io
            version: v1
        description: Local Storage Operator
        displayName: Local Storage
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.3.2-202002112006
      name: "4.3"
    defaultChannel: "4.3"
    packageName: local-storage-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: sriov-network-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/sriov-network-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: sriov-network-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetworkNodePolicy",
                "metadata": {
                  "name": "policy-1",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "resourceName": "intelnics",
                  "nodeSelector": {
                    "feature.node.kubernetes.io/sriov-capable": "true"
                  },
                  "priority": 99,
                  "mtu": 9000,
                  "numVfs": 6,
                  "nicSelector": {
                    "vendor": "8086",
                    "rootDevices": [
                      "0000:01:00.1"
                    ],
                    "pfNames": [
                      "eth1"
                    ]
                  },
                  "deviceType": "vfio-pci"
                }
              },
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetwork",
                "metadata": {
                  "name": "example-sriovnetwork",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "ipam": "{\n  \"type\": \"host-local\",\n  \"subnet\": \"10.56.217.0/24\",\n  \"rangeStart\": \"10.56.217.171\",\n  \"rangeEnd\": \"10.56.217.181\",\n  \"routes\": [{\n    \"dst\": \"0.0.0.0/0\"\n  }],\n  \"gateway\": \"10.56.217.1\"\n}\n",
                  "vlan": 0,
                  "spoofChk": true,
                  "trust": false,
                  "resourceName": "intelnics",
                  "networkNamespace": "default"
                }
              },
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetworkNodeState",
                "metadata": {
                  "name": "minikube",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "interfaces": []
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-sriov-network-operator@sha256:848cb34d880d41de49316a2dac31618877e6015fc6cf896c9b15ad7d95e50520
          createdAt: 2019/04/30
          description: An operator for configuring SR-IOV components and initializing
            SRIOV network devices in Openshift cluster.
          repository: https://github.com/openshift/sriov-network-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an policy of configuring SR-IOV components on
              nodes
            displayName: Sriov Network
            kind: SriovNetwork
            name: sriovnetworks.sriovnetwork.openshift.io
            version: v1
          - description: Represents an policy of configuring SR-IOV components on
              nodes
            displayName: Sriov Network Node Policy
            kind: SriovNetworkNodePolicy
            name: sriovnetworknodepolicies.sriovnetwork.openshift.io
            version: v1
          - description: Represents the state of SR-IOV network devices on a node
            displayName: Sriov Network Node State
            kind: SriovNetworkNodeState
            name: sriovnetworknodestates.sriovnetwork.openshift.io
            version: v1
        description: |
          # SR-IOV Network Operator for Openshift

          ## Introdution
          The sriov-network-operator is generally responsible for configuring the sriov components in a openshift cluster.

          ### Supported Features
          * Initialize the SR-IOV NICs on nodes.
          * provision SR-IOV device plugin on selected node.
          * provision SR-IOV CNI plugin on selected nodes.
          * manage configuration of SR-IOV device plugin.
          * generate net-att-def CRs for SR-IOV CNI plugin.

          This operator has to run in namespace 'sriov-network-operator'. An Operator Group is also required to install this operator:

          ```
          $ oc create -f - <<EOF
          apiVersion: v1
          kind: Namespace
          metadata:
            name: sriov-network-operator
            labels:
              openshift.io/run-level: "1"
          EOF

          $ oc create -f - <<EOF
          apiVersion: operators.coreos.com/v1
          kind: OperatorGroup
          metadata:
            name: sriov-network-operators
            namespace: sriov-network-operator
          spec:
            targetNamespaces:
            - sriov-network-operator
          EOF
          ```
        displayName: SR-IOV Network Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 0.0.1
      name: "4.2"
    - currentCSV: sriov-network-operator.v0.0.1-s390x
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetworkNodePolicy",
                "metadata": {
                  "name": "policy-1",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "resourceName": "intelnics",
                  "nodeSelector": {
                    "feature.node.kubernetes.io/sriov-capable": "true"
                  },
                  "priority": 99,
                  "mtu": 9000,
                  "numVfs": 6,
                  "nicSelector": {
                    "vendor": "8086",
                    "rootDevices": [
                      "0000:01:00.1"
                    ],
                    "pfNames": [
                      "eth1"
                    ]
                  },
                  "deviceType": "vfio-pci"
                }
              },
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetwork",
                "metadata": {
                  "name": "example-sriovnetwork",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "ipam": "{\n  \"type\": \"host-local\",\n  \"subnet\": \"10.56.217.0/24\",\n  \"rangeStart\": \"10.56.217.171\",\n  \"rangeEnd\": \"10.56.217.181\",\n  \"routes\": [{\n    \"dst\": \"0.0.0.0/0\"\n  }],\n  \"gateway\": \"10.56.217.1\"\n}\n",
                  "vlan": 0,
                  "spoofChk": true,
                  "trust": false,
                  "resourceName": "intelnics",
                  "networkNamespace": "default"
                }
              },
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetworkNodeState",
                "metadata": {
                  "name": "minikube",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "interfaces": []
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-sriov-network-operator@sha256:5f0591b5262cec97775ea251d23669be27231b634d88585648cdf915a4184e34
          createdAt: 2019/04/30
          description: An operator for configuring SR-IOV components and initializing
            SRIOV network devices in Openshift cluster.
          repository: https://github.com/openshift/sriov-network-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an policy of configuring SR-IOV components on
              nodes
            displayName: Sriov Network
            kind: SriovNetwork
            name: sriovnetworks.sriovnetwork.openshift.io
            version: v1
          - description: Represents an policy of configuring SR-IOV components on
              nodes
            displayName: Sriov Network Node Policy
            kind: SriovNetworkNodePolicy
            name: sriovnetworknodepolicies.sriovnetwork.openshift.io
            version: v1
          - description: Represents the state of SR-IOV network devices on a node
            displayName: Sriov Network Node State
            kind: SriovNetworkNodeState
            name: sriovnetworknodestates.sriovnetwork.openshift.io
            version: v1
        description: |
          # SR-IOV Network Operator for Openshift

          ## Introdution
          The sriov-network-operator is generally responsible for configuring the sriov components in a openshift cluster.

          ### Supported Features
          * Initialize the SR-IOV NICs on nodes.
          * provision SR-IOV device plugin on selected node.
          * provision SR-IOV CNI plugin on selected nodes.
          * manage configuration of SR-IOV device plugin.
          * generate net-att-def CRs for SR-IOV CNI plugin.

          This operator has to run in namespace 'sriov-network-operator'. An Operator Group is also required to install this operator:

          ```
          $ oc create -f - <<EOF
          apiVersion: v1
          kind: Namespace
          metadata:
            name: sriov-network-operator
            labels:
              openshift.io/run-level: "1"
          EOF

          $ oc create -f - <<EOF
          apiVersion: operators.coreos.com/v1
          kind: OperatorGroup
          metadata:
            name: sriov-network-operators
            namespace: sriov-network-operator
          spec:
            targetNamespaces:
            - sriov-network-operator
          EOF
          ```
        displayName: SR-IOV Network Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 0.0.1
      name: 4.2-s390x
    - currentCSV: sriov-network-operator.4.3.2-202002112006
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetworkNodePolicy",
                "metadata": {
                  "name": "policy-1",
                  "namespace": "openshift-sriov-network-operator"
                },
                "spec": {
                  "resourceName": "intelnics",
                  "nodeSelector": {
                    "feature.node.kubernetes.io/network-sriov.capable": "true"
                  },
                  "priority": 99,
                  "mtu": 9000,
                  "numVfs": 6,
                  "nicSelector": {
                    "vendor": "8086",
                    "rootDevices": [
                      "0000:01:00.1"
                    ],
                    "pfNames": [
                      "eth1"
                    ]
                  },
                  "deviceType": "vfio-pci"
                }
              },
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetwork",
                "metadata": {
                  "name": "example-sriovnetwork",
                  "namespace": "openshift-sriov-network-operator"
                },
                "spec": {
                  "ipam": "{\n  \"type\": \"host-local\",\n  \"subnet\": \"10.56.217.0/24\",\n  \"rangeStart\": \"10.56.217.171\",\n  \"rangeEnd\": \"10.56.217.181\",\n  \"routes\": [{\n    \"dst\": \"0.0.0.0/0\"\n  }],\n  \"gateway\": \"10.56.217.1\"\n}\n",
                  "vlan": 0,
                  "spoofChk": "on",
                  "trust": "off",
                  "resourceName": "intelnics",
                  "networkNamespace": "default"
                }
              },
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetworkNodeState",
                "metadata": {
                  "name": "minikube",
                  "namespace": "openshift-sriov-network-operator"
                },
                "spec": {
                  "interfaces": []
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-sriov-network-operator@sha256:804febca2d4b4c31471e520b16457f55c5d6d558dff7e898a7c421922ffc07cb
          createdAt: 2019/04/30
          description: An operator for configuring SR-IOV components and initializing
            SRIOV network devices in Openshift cluster.
          olm.skipRange: '>=4.3.0-0 <4.3.2-202002112006'
          repository: https://github.com/openshift/sriov-network-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an policy of configuring SR-IOV components on
              nodes
            displayName: Sriov Network
            kind: SriovNetwork
            name: sriovnetworks.sriovnetwork.openshift.io
            version: v1
          - description: Represents an policy of configuring SR-IOV components on
              nodes
            displayName: Sriov Network Node Policy
            kind: SriovNetworkNodePolicy
            name: sriovnetworknodepolicies.sriovnetwork.openshift.io
            version: v1
          - description: Represents the state of SR-IOV network devices on a node
            displayName: Sriov Network Node State
            kind: SriovNetworkNodeState
            name: sriovnetworknodestates.sriovnetwork.openshift.io
            version: v1
        description: |
          # SR-IOV Network Operator for Openshift

          ## Introdution
          The sriov-network-operator is generally responsible for configuring the sriov components in a openshift cluster.

          ### Supported Features
          * Initialize the SR-IOV NICs on nodes.
          * provision SR-IOV device plugin on selected node.
          * provision SR-IOV CNI plugin on selected nodes.
          * manage configuration of SR-IOV device plugin.
          * generate net-att-def CRs for SR-IOV CNI plugin.

          This operator has to run in namespace 'openshift-sriov-network-operator'. An Operator Group is also required to install this operator:

          ```
          $ oc create -f - <<EOF
          apiVersion: v1
          kind: Namespace
          metadata:
            name: openshift-sriov-network-operator
            labels:
              openshift.io/run-level: "1"
          EOF

          $ oc create -f - <<EOF
          apiVersion: operators.coreos.com/v1
          kind: OperatorGroup
          metadata:
            name: sriov-network-operators
            namespace: openshift-sriov-network-operator
          spec:
            targetNamespaces:
            - openshift-sriov-network-operator
          EOF
          ```
        displayName: SR-IOV Network Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.3.2-202002112006
      name: "4.3"
    - currentCSV: sriov-network-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetworkNodePolicy",
                "metadata": {
                  "name": "policy-1",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "resourceName": "intelnics",
                  "nodeSelector": {
                    "feature.node.kubernetes.io/sriov-capable": "true"
                  },
                  "priority": 99,
                  "mtu": 9000,
                  "numVfs": 6,
                  "nicSelector": {
                    "vendor": "8086",
                    "rootDevices": [
                      "0000:01:00.1"
                    ],
                    "pfNames": [
                      "eth1"
                    ]
                  },
                  "deviceType": "vfio-pci"
                }
              },
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetwork",
                "metadata": {
                  "name": "example-sriovnetwork",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "ipam": "{\n  \"type\": \"host-local\",\n  \"subnet\": \"10.56.217.0/24\",\n  \"rangeStart\": \"10.56.217.171\",\n  \"rangeEnd\": \"10.56.217.181\",\n  \"routes\": [{\n    \"dst\": \"0.0.0.0/0\"\n  }],\n  \"gateway\": \"10.56.217.1\"\n}\n",
                  "vlan": 0,
                  "spoofChk": true,
                  "trust": false,
                  "resourceName": "intelnics",
                  "networkNamespace": "default"
                }
              },
              {
                "apiVersion": "sriovnetwork.openshift.io/v1",
                "kind": "SriovNetworkNodeState",
                "metadata": {
                  "name": "minikube",
                  "namespace": "sriov-network-operator"
                },
                "spec": {
                  "interfaces": []
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-sriov-network-operator@sha256:848cb34d880d41de49316a2dac31618877e6015fc6cf896c9b15ad7d95e50520
          createdAt: 2019/04/30
          description: An operator for configuring SR-IOV components and initializing
            SRIOV network devices in Openshift cluster.
          repository: https://github.com/openshift/sriov-network-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an policy of configuring SR-IOV components on
              nodes
            displayName: Sriov Network
            kind: SriovNetwork
            name: sriovnetworks.sriovnetwork.openshift.io
            version: v1
          - description: Represents an policy of configuring SR-IOV components on
              nodes
            displayName: Sriov Network Node Policy
            kind: SriovNetworkNodePolicy
            name: sriovnetworknodepolicies.sriovnetwork.openshift.io
            version: v1
          - description: Represents the state of SR-IOV network devices on a node
            displayName: Sriov Network Node State
            kind: SriovNetworkNodeState
            name: sriovnetworknodestates.sriovnetwork.openshift.io
            version: v1
        description: |
          # SR-IOV Network Operator for Openshift

          ## Introdution
          The sriov-network-operator is generally responsible for configuring the sriov components in a openshift cluster.

          ### Supported Features
          * Initialize the SR-IOV NICs on nodes.
          * provision SR-IOV device plugin on selected node.
          * provision SR-IOV CNI plugin on selected nodes.
          * manage configuration of SR-IOV device plugin.
          * generate net-att-def CRs for SR-IOV CNI plugin.

          This operator has to run in namespace 'sriov-network-operator'. An Operator Group is also required to install this operator:

          ```
          $ oc create -f - <<EOF
          apiVersion: v1
          kind: Namespace
          metadata:
            name: sriov-network-operator
            labels:
              openshift.io/run-level: "1"
          EOF

          $ oc create -f - <<EOF
          apiVersion: operators.coreos.com/v1
          kind: OperatorGroup
          metadata:
            name: sriov-network-operators
            namespace: sriov-network-operator
          spec:
            targetNamespaces:
            - sriov-network-operator
          EOF
          ```
        displayName: SR-IOV Network Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 0.0.1
      name: alpha
    defaultChannel: "4.3"
    packageName: sriov-network-operator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Synopsys, Inc
      provider-url: ""
    name: synopsys-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/synopsys-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: synopsys-operator.2019.8.4
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "synopsys.com/v1",
                "kind": "OpsSight",
                "metadata": {
                  "name": "opssight-test"
                },
                "spec": {
                  "namespace": "opssight-test",
                  "perceptor": {
                    "name": "opssight-core",
                    "port": 3001,
                    "image": "docker.io/blackducksoftware/opssight-core:latest",
                    "checkForStalledScansPauseHours": 999999,
                    "stalledScanClientTimeoutHours": 999999,
                    "modelMetricsPauseSeconds": 15,
                    "unknownImagePauseMilliseconds": 15000,
                    "clientTimeoutMilliseconds": 100000,
                    "expose": ""
                  },
                  "scannerPod": {
                    "name": "opssight-scanner",
                    "scanner": {
                      "name": "opssight-scanner",
                      "port": 3003,
                      "image": "docker.io/blackducksoftware/opssight-scanner:latest",
                      "clientTimeoutSeconds": 600
                    },
                    "imageFacade": {
                      "name": "opssight-image-getter",
                      "port": 3004,
                      "internalRegistries": [],
                      "image": "docker.io/blackducksoftware/opssight-image-getter:latest",
                      "serviceAccount": "opssight-scanner",
                      "imagePullerType": "skopeo"
                    },
                    "replicaCount": 1
                  },
                  "perceiver": {
                    "enableImagePerceiver": false,
                    "enablePodPerceiver": true,
                    "port": 3002,
                    "imagePerceiver": {
                      "name": "opssight-image-processor",
                      "image": "docker.io/blackducksoftware/opssight-image-processor:latest"
                    },
                    "podPerceiver": {
                      "name": "opssight-pod-processor",
                      "image": "docker.io/blackducksoftware/opssight-pod-processor:latest"
                    },
                    "serviceAccount": "opssight-processor",
                    "annotationIntervalSeconds": 30,
                    "dumpIntervalMinutes": 30
                  },
                  "prometheus": {
                    "name": "prometheus",
                    "port": 9090,
                    "image": "docker.io/prom/prometheus:v2.1.0",
                    "expose": ""
                  },
                  "enableSkyfire": false,
                  "skyfire": {
                    "image": "gcr.io/saas-hub-stg/blackducksoftware/pyfire:master",
                    "name": "skyfire",
                    "port": 3005,
                    "prometheusPort": 3006,
                    "serviceAccount": "skyfire",
                    "hubClientTimeoutSeconds": 120,
                    "hubDumpPauseSeconds": 240,
                    "kubeDumpIntervalSeconds": 60,
                    "perceptorDumpIntervalSeconds": 60
                  },
                  "enableMetrics": true,
                  "defaultCPU": "300m",
                  "defaultMem": "1300Mi",
                  "scannerCPU": "300m",
                  "scannerMem": "1300Mi",
                  "logLevel": "debug",
                  "configMapName": "blackduck",
                  "secretName": "blackduck",
                  "blackduck": {
                    "connectionsEnvironmentVariableName": "blackduck.json",
                    "blackduckPassword": "<COMMON_BLACKDUCK_PWD_OPSSIGHT_TYPE_BASE64_ENCODED>",
                    "tlsVerification": false,
                    "initialCount": 0,
                    "maxCount": 0,
                    "blackduckSpec": {
                      "namespace": "",
                      "type": "opssight",
                      "size": "small",
                      "version": "2019.8.1",
                      "exposeService": "",
                      "livenessProbes": false,
                      "persistentStorage": false,
                      "certificateName": "default",
                      "licenseKey": "<BLACKDUCK_LICENSE_KEY>"
                    }
                  }
                }
              },
              {
                "apiVersion": "synopsys.com/v1",
                "kind": "Blackduck",
                "metadata": {
                  "name": "blackduck-ephemeral"
                },
                "spec": {
                  "namespace": "blackduck-ephemeral",
                  "size": "Small",
                  "type": "opssight",
                  "version": "2019.8.1",
                  "exposeService": "NONE",
                  "livenessProbes": false,
                  "persistentStorage": false,
                  "licenseKey": "<BLACKDUCK_LICENSE_KEY>",
                  "certificateName": "default",
                  "adminPassword": "<POSTGRES_ADMIN_USER_PASSWORD_BASE64_ENCODED>",
                  "postgresPassword": "<POSTGRES_POSTGRES_USER_PASSWORD_BASE64_ENCODED>",
                  "userPassword": "<POSTGRES_BLACKDUCK_USER_PASSWORD_BASE64_ENCODED>"
                }
              },
              {
                "apiVersion": "synopsys.com/v1",
                "kind": "Alert",
                "metadata": {
                  "clusterName": "",
                  "name": "alert-test"
                },
                "spec": {
                  "namespace": "alert-test",
                  "version": "5.0.0",
                  "exposeService": "",
                  "standAlone": true,
                  "port": 8443,
                  "encryptionPassword": "",
                  "encryptionGlobalSalt": "",
                  "environs": [ "ALERT_SERVER_PORT:8443", "PUBLIC_HUB_WEBSERVER_HOST:<<BLACK_DUCK_HOST>>", "PUBLIC_HUB_WEBSERVER_PORT:443" ],
                  "alertMemory": "2560Mi",
                  "cfsslMemory": "640Mi",
                  "persistentStorage": false,
                  "pvcSize": "",
                  "pvcStorageClass": ""
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Security, Monitoring
          certified: "false"
          containerImage: registry.connect.redhat.com/blackducksoftware/synopsys-operator:latest
          createdAt: "2018-12-14 17:09:00"
          description: Synopsys Operator is a cloud-native administration utility
            for Synopsys software.
          repository: https://github.com/blackducksoftware/synopsys-operator
          support: Synopsys, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Manages Black Duck instances
            displayName: Black Duck
            kind: Blackduck
            name: blackducks.synopsys.com
            version: v1
          - description: Manages the OpsSight Connector
            displayName: OpsSight Connector
            kind: OpsSight
            name: opssights.synopsys.com
            version: v1
          - description: Manages Black Duck Alert
            displayName: Black Duck Alert
            kind: Alert
            name: alerts.synopsys.com
            version: v1
        description: |-
          [Synopsys Operator](https://synopsys.atlassian.net/wiki/spaces/BDLM/pages/34373652/Synopsys+Operator) is a cloud-native administration utility for Synopsys software. Synopsys Operator assists in the deployment and management of Synopsys software like Black Duck, OpsSight Connector, and Black Duck Alert, in a Kubernetes and Red Hat OpenShift environment.

          The Synopsys Operator deployment can be managed by a utility called [synopsysctl](https://github.com/blackducksoftware/synopsys-operator/releases). synopsysctl makes it easy to deploy Synopsys Operator, and it eliminates the need to edit YML/JSON configuration files when deploying and managing software with Synopsys Operator. Synopsysctl provides a rich set of command-line parameters to allow for fast and highly-configurable deployments of Black Duck, OpsSight Connector, and Black Duck Alert. synopsysctlâ€™s interface mimics that of kubectl and oc, making it immediately familiar to anyone with Kubernetes and/or Red Hat OpenShift experience.

          ## Black Duck
          [Black Duck](https://www.synopsys.com/software-integrity/security-testing/software-composition-analysis.html) is a comprehensive solution for managing security, license compliance, and code quality risks that come from the use of open source in applications and containers. Black Duck gives you unmatched visibility into third-party code, enabling you to control it across your software supply chain and throughout the application life cycle.

          ### Supported Features

          * **Find and fix security vulnerabilities** at each stage in the SDLC, with detailed, vulnerability-specific remediation guidance and technical insight.

          * **Eliminate risk of open source license** noncompliance and safeguard your intellectual property by using the industryâ€™s largest open source knowledge base to identify which of 2,500+ licenses are relevant to the open source in your applications.

          * **Scan virtually any software, firmware, and source code** to generate acomprehensive bill of materials (BoM) of whatâ€™s inside.

          * **Automatically monitor for new vulnerabilities** hat affect your BoM, with custom policies and workflow triggers to accelerate remediation and reduce your risk exposure.

          ## OpsSight Connector
          [Synopsys Black Duck OpsSight](https://synopsys.atlassian.net/wiki/spaces/BDLM/pages/34242566/OpsSight) for Red Hat OpenShift Container Platform provides automated open source identification and monitoring in all container images in a Kubernetes or OpenShift cluster to give teams visibility into, and control over, the risks associated with open source components in those images.

          ### Supported Features

          * **Automated Detection** - OpsSightâ€™s automated multifactor open source detection inventories all the open source in container images as they are deployed.

          * **Security Intelligence** -  Black Duck Enhanced Vulnerability Data identifies all known vulnerabilities for the open source in your container images while actionable mitigation and remediation guidance helps minimize exploit risk.

          * **Policy Management** - OpsSight policy management allows teams to define open source use and security policies, which are evaluated with each scan and documented as metadata on your containers, allowing you to flag images that violate policies and prevent them from deploying to production.

          * **Continous Monitoring** - OpsSight continuously monitors for newly reported open source security vulnerabilities associated with open source in use, providing same-day alerts so teams can understand how newly discovered vulnerabilities affect their containers in production.

          ## Black Duck Alert
          Black Duck Alert is an additional deployment to Black Duck that enables you to see Security Vulnerability and Policy Management notifications through email, Slack or HipChat.

          ## About Synopsys

          [Synopsys Software Integrity](https://www.synopsys.com/software-integrity.html) helps development teams build secure, high-quality software, minimizing risks while maximizing speed and productivity. Synopsys, a recognized leader in application security, provides static analysis, software composition analysis, and dynamic analysis solutions that enable teams to quickly find and fix vulnerabilities and defects in proprietary code, open source components, and application behavior.
        displayName: Synopsys Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Synopsys, Inc
        version: 2019.8.4
      name: stable
    defaultChannel: stable
    packageName: synopsys-certified
    provider:
      name: Synopsys, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Dynatrace LLC
      provider-url: ""
    name: oneagent-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/oneagent-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: dynatrace-monitoring.v0.6.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
             "apiVersion": "dynatrace.com/v1alpha1",
             "kind": "OneAgent",
             "metadata": {
                "name": "oneagent",
                "namespace": "dynatrace"
             },
             "spec": {
                "apiUrl": "https://ENVIRONMENTID.live.dynatrace.com/api",
                "skipCertCheck": false,
                "tokens": "",
                "nodeSelector": {},
                "tolerations": [
                   {
                      "effect": "NoSchedule",
                      "key": "node-role.kubernetes.io/master",
                      "operator": "Exists"
                   }
                ],
                "image": "registry.connect.redhat.com/dynatrace/oneagent",
                "args": [
                   "APP_LOG_CONTENT_ACCESS=1"
                ],
                "env": []
             }
            }]
          capabilities: Deep Insights
          categories: Monitoring,Logging & Tracing,OpenShift Optional
          certified: "false"
          containerImage: registry.connect.redhat.com/dynatrace/dynatrace-oneagent-operator:v0.6.0
          createdAt: "2019-11-18T07:00:00Z"
          description: Install full-stack monitoring of OpenShift clusters with the
            Dynatrace OneAgent.
          repository: https://github.com/Dynatrace/dynatrace-oneagent-operator
          support: Dynatrace
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Dyantrace OneAgent for full-stack monitoring
            displayName: Dynatrace OneAgent
            kind: OneAgent
            name: oneagents.dynatrace.com
            version: v1alpha1
        description: |
          The Dynatrace OneAgent Operator allows users to easily deploy full-stack monitoring for [OpenShift clusters](https://www.dynatrace.com/technologies/openshift-monitoring/). The Dynatrace OneAgent automatically monitors the workload running in containers down to the code and request level.

          ### Installing the OneAgent
          Once you've installed the Operator, you can create OneAgent custom resources to monitor your environment.

          First, please add a Secret within the Project you've deployed the Dynatrace Operator to, which would contain your API and PaaS tokens. Create tokens of type *Dynatrace API* (`API_TOKEN`) and *Platform as a Service* (`PAAS_TOKEN`) and use their values in the following commands respectively.

          For assistance please refer to [Create user-generated access tokens](https://www.dynatrace.com/support/help/shortlink/token#create-user-generated-access-tokens).

          ``` $ oc -n <project> create secret generic oneagent --from-literal="apiToken=API_TOKEN" --from-literal="paasToken=PAAS_TOKEN" ```

          You may update this Secret at any time to rotate the tokens.

          Then please add an OneAgent object in the Project where the Operator has been deployed, configured to your needs.

          ### Required Parameters
          * `apiUrl` - provide the URL to the API of your Dynatrace environment. In Dynatrace SaaS it will look like `https://<ENVIRONMENTID>.live.dynatrace.com/api` . In Dynatrace Managed like `https://<YourDynatraceServerURL>/e/<ENVIRONMENTID>/api` .

          ### Advanced Options
          * **Image Override** - use a copy of the OneAgent container image from a registry other than Docker's or Red Hat's
          * **NodeSelectors** - select a subset of your cluster's nodes to run the Dynatrace OneAgent on, based on labels
          * **Tolerations** - add specific tolerations to the agent so that it can monitor all of the nodes in your cluster; we include the default toleration so that Dynatrace OneAgent also monitors the master nodes
          * **Priority Class Name** - define the priorityClassName for OneAgent pods
          * **Environment variables** - define environment variables for the OneAgent container
          * **Disable Certificate Checking** - disable any certificate validation that may interact poorly with proxies with in your cluster
          * **Disable OneAgent Update** - disable the Operator's auto-update feature for OneAgent pods
          * **Enable Istio Auto-config** - automatically create Istio objects for egress communication to the Dynatrace environment from the OneAgent

          For a complete list of supported parameters please consult the [Operator Deploy Guide](https://www.dynatrace.com/support/help/shortlink/openshift-deploy).

          ### Help
          You can find more about our instructions in our [documentation](https://www.dynatrace.com/support/help/shortlink/openshift-deploy#install-oneagent-operator).
        displayName: Dynatrace OneAgent
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Dynatrace LLC
        version: 0.6.0
      name: alpha
    defaultChannel: alpha
    packageName: oneagent-certified
    provider:
      name: Dynatrace LLC
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: PerceptiLabs
      provider-url: ""
    name: perceptilabs-operator-package
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/perceptilabs-operator-package
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: perceptilabs-operator.v1.0.10
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"perceptilabs.com/v1","kind":"PerceptiLabs","metadata":{"name":"example-perceptilabs"},"spec":{"namespace":"your-namespace","corePvc":"perceptilabs-pvc","coreGpus":0,"license_name":"demo","license_value":"demo"}}]'
          capabilities: Basic Install
          categories: AI/Machine Learning
          certified: "false"
          containerImage: registry.connect.redhat.com/perceptilabs/perceptilabs-operator:1.0.10
          createdAt: "2019-10-02T12:00:00Z"
          description: AI platform which lets you Build, Train and Analyze
          support: support@perceptilabs.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: The PerceptiLabs Modeling tool
            displayName: PerceptiLabs
            kind: PerceptiLabs
            name: perceptilabs.perceptilabs.com
            version: v1
        description: "The PerceptiLabs operator creates and maintains PerceptiLabs,
          a visual tool modeling for machine learning at warp speed.\n\nPerceptiLabs
          visual modeling tool provides a GUI for building,\ntraining, and assessing
          your models, while also enabling deeper\ndevelopment with code. You get
          faster iterations and better\nexplainability of your results.\n\nFor more
          information visit [http://perceptilabs.com](http://perceptilabs.com).\n\n#
          Features\n\n**Fast modeling**  \nMake changes, debug, and tune your model
          through the GUI of custom code\neditor where every component/layer is reprogrammable.
          Choose from\nmultiple neural network models as well as classical AI methods.\n\n\n**Transparency
          of Model Performance and Results**  \nGet instant feedback about your model's
          performance through the\nvisualization of the architecture, to better review
          and understand the\nresults. See real-time analytics in every operation
          and variable, and\ngranular previews of output from each model component.\n\n\n**Flexibility**
          \ \nCustomize your environment and statistics dashboard. Use high-level\nabstractions
          or low-level code. Execute any custom Python code or export\na fully trained
          TensorFlow model to perform inference in your projects.\n\n\n\n# Installation
          Instructions  \nFor your convenience, we've included an example quickstart
          for running PerceptiLabs in demo mode.\n\n## Prepare your namespace  \nChoose
          or create the namespace into which you'd like install PerceptiLabs. For
          example:  \n```\noc create namespace REPLACE_NAMESPACE\n```\n\n## Prepare
          storage for your data\n\nYou'll need to have a place on your cluster for
          storing training data and models.\n\nHere's an example configuration for
          creating storage on a cluster hosted on AWS that you can tailor to your
          needs:\n\n```\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n
          \ name: perceptilabs-example-sc\n  annotations:\n    description: Example
          Storage for PerceptiLabs\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n
          \ fsType: ext4\n  type: gp2\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n---\nkind:
          PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: REPLACE_PVC_NAME\n
          \ namespace: REPLACE_NAMESPACE\nspec:\n  storageClassName: perceptilabs-example-sc\n
          \ volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  resources:\n
          \   requests:\n      storage: 50Gi\n```\n\n## Create the service account\n\nChoose
          or create a service account to run PerceptiLabs. If you want a new service
          account, create it like so:\n\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n
          \ name: REPLACE_SERVICEACCOUNT_NAME\n  namespace: REPLACE_NAMESPACE\n```\n\n##
          Subscribe to the PerceptiLabs operator in your namespace\n\nIf you're using
          the OpenShift console webpage, just click the Install button on this operator.
          If not, you can customize and apply this configuration:\n\n```\napiVersion:
          operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: REPLACE_NAMESPACE-operatorgroup\n
          \ namespace: REPLACE_NAMESPACE\nspec:\n  targetNamespaces:\n  - REPLACE_NAMESPACE\n---\napiVersion:
          operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: perceptilabs-operator\n
          \ namespace: REPLACE_NAMESPACE\nspec:\n  channel: stable\n  name: perceptilabs-operator-package\n
          \ source: perceptilabs-operators\n  sourceNamespace: openshift-marketplace\n
          \ namespace: REPLACE_NAMESPACE\n```\n\nAfter this, you should see a `perceptilabs-operator`
          pod start up in your namespace. In that pod, the log for the `operator`
          container should eventually say \"starting to serve\".\n\n## Start a copy
          of PerceptiLabs\n\nThis is where you connect your storage and service account
          to a PerceptiLabs instance and run it. You can customize and apply the following
          configuration:\n\n```\napiVersion: perceptilabs.com/v1\nkind: PerceptiLabs\nmetadata:\n
          \ name: example-perceptilabs\n  namespace: REPLACE_NAMESPACE\nspec:\n  serviceAccountName:
          default  namespace: REPLACE_NAMESPACE\n  corePvc: REPLACE_PVC_NAME\n```\n\nAt
          this point two pods named 'perceptilabs-core-...' and 'perceptilabs-frontend-...`
          will start up in your namespace.\n\n## Copy data files to your cluster\n\nIf
          you've used the persistent storage configuration from above, then you have
          a read-write volume mounted in the pod at `/mnt/plabs`. Copy your files
          there:\n\n```\noc cp REPLACE_FILENAME --namespace=REPLACE_NAMESPACE REPLACE_CORE_POD_NAME:/mnt/plabs
          --container=core\n```\n\n## Get the URL of your PerceptiLabs\n\nOnce everything
          is up and running, you'll have two new routes in your namespace. Go to the
          routes for your namespace and follow the link named `perceptilabs-frontend`.
          Your browser will be connected to your instance of PerceptiLabs! Alternatively,
          you can get the URL from the command line:\n\n```\noc get routes --namespace
          REPLACE_NAMESPACE perceptilabs-frontend\n```"
        displayName: PerceptiLabs Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: PerceptiLabs
        version: 1.0.10
      name: stable
    defaultChannel: stable
    packageName: perceptilabs-operator-package
    provider:
      name: PerceptiLabs
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Citrix
      provider-url: ""
    name: cpx-cic-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cpx-cic-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: cpx-cic-operator.v130.47.103
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "charts.helm.k8s.io/v1alpha1",
                "kind": "CitrixCPXIngressController",
                "metadata": {
                  "name": "cpx-cic"
                },
                "spec": {
                  "cic": {
                    "image": "registry.connect.redhat.com/citrix/citrix-ingress-controller:latest",
                    "pullPolicy": "IfNotPresent",
                    "required": true
                  },
                  "cpx": {
                    "image": "registry.connect.redhat.com/citrix/cpx-13-0:latest",
                    "pullPolicy": "IfNotPresent"
                  },
                  "defaultSSLCert": null,
                  "exporter": {
                    "image": "registry.connect.redhat.com/citrix/citrix-adc-metrics-exporter:latest",
                    "ports": {
                      "containerPort": 8888
                    },
                    "pullPolicy": "IfNotPresent",
                    "required": false
                  },
                  "ingressClass": null,
                  "license": {
                    "accept": "no"
                  },
                  "lsIP": null,
                  "lsPort": null,
                  "nsNamespace": null,
                  "openshift": true,
                  "platform": null
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.connect.redhat.com/citrix/citrix-k8s-cpx-ingress-controller:latest
          createdAt: "2020-02-06"
          description: Citrix ADC CPX is a container-based application delivery controller
            that can be provisioned on a Docker host. Citrix ADC CPX enables customers
            to leverage Docker engine capabilities and use NetScaler load balancing
            and traffic management features for container-based applications. Citrix
            Ingress Controller which will be running in side-car mode with Citrix
            ADC CPX will automatically configures Citrix ADC CPX based on the Ingress
            resource configuration. This operator can be used deploy Citrix ADC CPX
            with Citrix ingress controller in an Openshift environment.
          repository: https://github.com/citrix/citrix-k8s-ingress-controller
          support: Citrix ADC CPX with Citrix Ingress Controller
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Deploys Citrix-CPX-with-Ingress-Controller in the Cluster.
            displayName: Citrix-CPX-with-Ingress-Controller
            kind: CitrixCPXIngressController
            name: citrixcpxingresscontrollers.charts.helm.k8s.io
            version: v1alpha1
        description: Citrix ADC CPX is a container-based application delivery controller
          that can be provisioned on a Docker host. Citrix ADC CPX enables customers
          to leverage Docker engine capabilities and use NetScaler load balancing
          and traffic management features for container-based applications. Citrix
          Ingress Controller which will be running in side-car mode with Citrix ADC
          CPX will automatically configures Citrix ADC CPX based on the Ingress resource
          configuration. This operator can be used deploy Citrix ADC CPX with Citrix
          ingress controller in an Openshift environment.
        displayName: Citrix ADC CPX with Ingress Controller
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Citrix
        version: 130.47.103
      name: alpha
    defaultChannel: alpha
    packageName: cpx-cic-operator
    provider:
      name: Citrix
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: IBM
      provider-url: ""
    name: event-streams-topic
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/event-streams-topic
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: event-streams-topic.v0.1.1
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion": "ibmcloud.ibm.com/v1alpha1", "kind": "Topic",
            "metadata": {"name": "mytopic"}, "spec": {"bindingFrom": {"name": "myes-binding"},
            "topicName": "myTopic"}}]'
          capabilities: Basic Install
          categories: Cloud Provider
          certified: "false"
          containerImage: cloudoperators/event-streams-topic:0.1.1
          createdAt: "2019-10-29T13:52:40Z"
          description: An operator for the life cycle management of Topics on Event
            Streams for IBM Cloud
          repository: https://github.com/IBM/event-streams-topic
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An operator for managing Topics for Event Streams on IBM
              Cloud
            displayName: Topic
            kind: Topic
            name: topics.ibmcloud.ibm.com
            version: v1alpha1
        description: |
          Event Streams Topic is an operator to manage the life cycle of Topics on Event Streams service on IBM Cloud. It is currently in preview. It will get updated as we release new versions of the [upstream repository](https://github.com/IBM/event-streams-topic).
          ## Requirements
          The operator can be installed on any OLM-enabled Kubernetes cluster with version >= 1.11.  Before installing, make sure you have installed [kubectl CLI](https://kubernetes.io/docs/tasks/tools/install-kubectl/) and it is configured to access your cluster.
          This operator can be used with IBM Event Streams instances provisioned by the IBM Cloud UI or CLI, or by the [IBM Cloud Operator](https://github.com/IBM/cloud-operators).
          ## Using the Event Streams Topic Operator
          Assuming you have created a Kubernetes secret, `binding-messagehub`, which contains credentials for the managed EventStreams service (apiKey, and kafkaAdminURL),  the following yaml deploys a Topic named `MyGreatTopic` with the configuration settings that are shown:

              apiVersion: ibmcloud.ibm.com/v1alpha1
              kind: Topic
              metadata:
                name: mytopic
              spec:
                apiKey:
                  secretKeyRef:
                    name: binding-messagehub
                    key: apikey
                kafkaAdminUrl:
                  secretKeyRef:
                    name: binding-messagehub
                    key: kafkaadminurl
                topicName: MyGreatTopic
                numPartitions: 3
                replicationFactor : 3
                configs :
                  - name: retentionMs
                    value: 2592000000



          For additional configuration options, samples and more information on using the operator, consult  the [Event Streams Topic documentation](https://github.com/IBM/event-streams-topic).
        displayName: Event Streams Topic
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: IBM
        version: 0.1.1
      name: alpha
    defaultChannel: alpha
    packageName: event-streams-topic
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: jenkins-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/jenkins-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: jenkins-operator.v0.3.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion":"jenkins.io/v1alpha2",
              "kind":"Jenkins",
              "metadata": {
                "annotations":{
                  "jenkins.io/openshift-mode":"true"
                },
              "generation":1,
              "name":"example"
              },
              "spec":{
                "backup":{
                  "action":{},
                  "containerName":"",
                  "interval":0,
                  "makeBackupBeforePodDeletion":false
                },
                "configurationAsCode":{
                  "configurations":null,
                  "secret":{"name":""}
                },
                "groovyScripts":{
                  "configurations":null,
                  "secret":{"name":""}
                },
                "master":{
                  "basePlugins":[
                    {"name":"kubernetes","version":"1.18.3"},
                    {"name":"workflow-job","version":"2.34"},
                    {"name":"workflow-aggregator","version":"2.6"},
                    {"name":"git","version":"3.12.0"},
                    {"name":"job-dsl","version":"1.76"},
                    {"name":"configuration-as-code","version":"1.29"},
                    {"name":"configuration-as-code-support","version":"1.19"},
                    {"name":"kubernetes-credentials-provider","version":"0.12.1"}
                  ],
                  "containers":[{
                    "command":["/usr/bin/go-init","-main","/usr/libexec/s2i/run"],
                    "env":[
                      {"name":"OPENSHIFT_ENABLE_OAUTH","value":"true"},
                      {"name":"OPENSHIFT_ENABLE_REDIRECT_PROMPT","value":"true"},
                      {"name":"DISABLE_ADMINISTRATIVE_MONITORS","value":"false"},
                      {"name":"KUBERNETES_MASTER","value":"https://kubernetes.default:443"},
                      {"name":"KUBERNETES_TRUST_CERTIFICATES","value":"true"},
                      {"name":"JENKINS_SERVICE_NAME","value":"jenkins-operator-http-example"},
                      {"name":"JNLP_SERVICE_NAME","value":"jenkins-operator-slave-example"},
                      {"name":"JENKINS_UC_INSECURE","value":"false"},
                      {"name":"JENKINS_HOME","value":"/var/lib/jenkins"},
                      {"name":"JAVA_OPTS","value":"-XX:+UnlockExperimentalVMOptions -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:MaxRAMFraction=1 -Djenkins.install.runSetupWizard=false -Djava.awt.headless=true"}
                    ],
                    "image":"quay.io/openshift/origin-jenkins:latest",
                    "imagePullPolicy":"Always",
                    "livenessProbe":{
                      "httpGet":{
                        "path":"/login",
                        "port":8080,
                        "scheme":"HTTP"
                      },
                      "initialDelaySeconds":420,
                      "periodSeconds":360,
                      "timeoutSeconds":240
                    },
                    "name":"jenkins-master",
                    "readinessProbe":{
                      "httpGet":{
                        "path":"/login",
                        "port":8080,
                        "scheme":"HTTP"
                      },
                      "initialDelaySeconds":3,
                      "periodSeconds":0,
                      "timeoutSeconds":240
                    },
                    "resources":{
                      "limits":{"cpu":"600m","memory":"4Gi"},
                      "requests":{"cpu":"500m","memory":"3Gi"}
                    }
                  }],
                  "securityContext":{"runAsUser":1000700001}},
                  "restore":{"action":{},"containerName":""},
                  "service":{"port":8080,"type":"ClusterIP"},
                  "slaveService":{"port":50000,"type":"ClusterIP"}
                }
              }]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/redhat-developer/openshift-jenkins-operator:latest
          description: Kubernetes native operator which fully manages Jenkins on Kubernetes.
          repository: https://github.com/redhat-developer/jenkins-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Jenkins
            displayName: Jenkins
            kind: Jenkins
            name: jenkins.jenkins.io
            version: v1alpha2
        description: |+
          ##What's the Jenkins Operator?

          Jenkins operator is a Kubernetes native operator which fully manages Jenkins on Kubernetes. It was built with immutability and declarative configuration as code in mind.


          Out of the box it provides:


          - Integration with Kubernetes

          - Pipelines as code

          - Extensibility via groovy scripts or configuration as code plugin

          - Security and hardening

          - Problem statement and goals

          The main reason why we decided to implement the Jenkins Operator is the fact that we faced a lot of problems with standard Jenkins deployment. We want to make Jenkins more robust, suitable for dynamic and multi-tenant environments.


          Some of the problems we want to solve:

          - Installing plugins with incompatible versions or security vulnerabilities

          - Better configuration as code

          - Lack of end to end tests

          - Handle graceful shutdown properly

          - Security and hardening out of the box

          - Orphaned jobs with no jnlp connection

          - Make errors more visible for end users

          - Backup and restore for jobs history

        displayName: Jenkins Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.3.0
      name: alpha
    defaultChannel: alpha
    packageName: jenkins-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Tufin
      provider-url: ""
    name: orca
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/orca
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: orca-operator.v0.1.153
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"tufin.io/v1alpha1","kind":"Orca","metadata":{"name":"orca"},"spec":{"components":{"conntrack":true,"dns":false,"istio":false,"kube-network-policy":true,"pusher":true,"syslog":false,"watcher":true},"domain":"generic-bank","endpoints":{"guru":"guru.tufin.io:443","orca":"https://orca.tufin.io","registry":"registry.tufin.io"},"ignored_config_maps":["kube-system/ingress-gce-lock","istio-system/istio-ingress-controller-leader-istio"],"images":{"kite":"registry.connect.redhat.com/tufin/kite","monitor":"registry.connect.redhat.com/tufin/monitor"},"kube_platform":"Openshift","namespace":"tufin-system","project":"openshift4"},"status":{"phase":null}},{"apiVersion":"networking.tufin.io/v1","kind":"Policy","metadata":{"name":"orca","namespace":"placeholder"},"spec":{"mode":"learning","networkPolicyAllowAll":true,"rules":[]},"status":{"phase":null}}]'
          capabilities: Basic Install
          categories: Security
          certified: "true"
          containerImage: registry.connect.redhat.com/tufin/orca-operator
          createdAt: ""
          description: Installs the Orca Agent on the cluster. Orca is a cloud-based
            security monitoring and enforcement platform for Kubernetes.
          repository: https://github.com/Tufin/orca-operator
          support: Tufin
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Orca Agent configuration
            displayName: Orca
            kind: Orca
            name: orcas.tufin.io
            version: v1alpha1
          - description: Orca Policy configuration
            displayName: Policy
            kind: Policy
            name: policies.networking.tufin.io
            version: v1
        description: |-
          Tufin Orca is a cloud-based security monitoring and enforcement platform for Kubernetes clusters, containers and microservices.
          The Orca Operator installs Orca resources into your cluster. These work with the Orca cloud application to give the following functionality:
          * Monitor the cluster configuration: namespaces, containers, pods, services, network policies etc.
          * Learn and visualize the cluster connectivity
          * Build a connectivity policy (whitelist)
          * Generate audit reports
          * Alert on unauthorized connections
          * Enforce the connectivity policy in the cluster and on enterpise firewalls surrounding the cluster
          * Integrate into the CI/CD pipeline to learn and update the policy continuously
          * Scan images for vulnerabilities
          ## Deploying the agent
          * Sign up to Orca [here] (https://www.tufin.com/products/tufin-orca#s6)
          * You will be sent an email containing your deployment parameters
          * Complete the deployment following the instructions for generating the agent's secret [here] (https://github.com/Tufin/orca-operator/blob/master/README.md)
        displayName: Tufin Orca Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Tufin
        version: 0.1.153
      name: beta
    defaultChannel: beta
    packageName: orca
    provider:
      name: Tufin
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: OpsMx
      provider-url: ""
    name: spinnaker-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/spinnaker-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: spinnaker-operator.v1.17.4
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"SpinnakerOperator","metadata":{"name":"spin-install"},"spec":{"halyard":{"spinnakerVersion":"1.17.4","image":{"repository":"opsmx11/operator-halyard","tag":"1.24.0"}},"dockerRegistries":[{"name":"dockerhub","address":"index.docker.io","repositories":["library/alpine","library/ubuntu","library/centos","library/nginx"]}],"spinnakerFeatureFlags":["artifacts","artifacts-rewrite","chaos","gremlin","infrastructure-stages","mine-canary","travis","wercker","pipeline-templates","managed-pipeline-templates-v2-ui"],"minio":{"enabled":true,"imageTag":"RELEASE.2018-06-09T02-18-09Z","serviceType":"ClusterIP","accessKey":"spinnakeradmin","secretKey":"spinnakeradmin","bucket":"spinnaker","nodeSelector":{},"persistence":{"enabled":false}},"rbac":{"create":false},"serviceAccount":{"create":false,"halyardName":"spinnaker-operator","spinnakerName":null}}}]'
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: docker.io/opsmx11/spinnaker-operator:v0.1
          createdAt: "2019-04-29 19:49:35"
          description: Spinnaker is an Open Source, multi-cloud Continuous delivery
            platform to perform software releases with high velocity and confidence.
          support: OpsMx
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: SpinnakerOperator
            displayName: spinnakeroperator
            kind: SpinnakerOperator
            name: spinnakeroperators.charts.helm.k8s.io
            version: v1alpha1
        description: "## Spinnaker as an Operator\nSpinnaker Operator allows users
          to spin up Spinnaker to manage continuous deployments. With the help of
          Spinnaker Operator, users will have the convenience and confidence of simple
          but powerful approach to execute CI/CD process with high velocity while
          managing risk and improving quality of deployments to multiple cloud environments.
          \n## What is Spinnaker?\n* Spinnaker is an Open Source, multi-cloud Continuous
          delivery platform to perform software releases with high velocity and confidence.\n*
          Spinnaker helps user to create deployment pipelines that run integration
          and system tests, spin up and down server groups, and monitor your rollouts.\n###
          List of Features\n* Multi-Cloud Deployment - Deploy your VM or Containers
          or functions across most public and private cloud including AWS EC2, ECS,
          EKS, Lambda, Kubernetes, Google Compute Engine, Google Kubernetes Engine,
          Google App Engine, Microsoft Azure, OpenStack, with Oracle Bare Metal and
          DC/OS.\n* Automated Releases with Pipelines - Create deployment pipelines
          that run integration and system tests, spin up and down server groups, and
          monitor your rollouts. Trigger pipelines via git events, Jenkins, Travis
          CI, Docker, CRON, or other Spinnaker pipelines\n* Pipeline-as-code - Manage
          the pipeline as code (JSON) or interact with pipeline using API or UI.\n*
          Safe Deployment Strategies - Deploy using Canary or Red/Black (Blue/Green)
          or Rolling update and enable automated Canary analysis to ensure safety
          of the new updates before full-rollout to production\n* 1-click Rollback
          - Rolling back new deployments is never been easier with a 1-click rollback
          of images/configurations.\n* See more spinnaker.io or docs.opsmx.com\n"
        displayName: Spinnaker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: OpsMx
        version: 1.17.4
      name: alpha
    defaultChannel: alpha
    packageName: spinnaker-operator
    provider:
      name: OpsMx
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Containous
      provider-url: ""
    name: traefikee-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/traefikee-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: traefikee-operator.v0.4.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "containo.us/v1alpha1",
                "kind": "Traefikee",
                "metadata": {
                  "name": "example-traefikee",
                  "namespace": "traefikee"
                },
                "spec": {
                  "cluster": "traefikee",
                  "controllers": 1,
                  "image": "store/containous/traefikee:v2.0.0",
                  "proxies": 2
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: containous/traefikee-operator:latest
          createdAt: "2019-12-12T09:01:00Z"
          description: Traefik Enterprise Edition
          repository: https://github.com/containous/traefikee-operator
          support: https://docs.containo.us
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a TraefikEE installation
            displayName: TraefikEE
            kind: Traefikee
            name: traefikees.containo.us
            version: v1alpha1
        description: |
          TraefikEE is a distributed, and highly available edge routing solution built on top of the open source Traefik and natively integrates with Openshift to:

          * Load balance any applications and easily scale out to meet production traffic needs
          * Secure services with end-to-end network and application encryption
          * Provide end-to-end monitoring and real-time tracing for better insight into application uptime and performance

          ## Before You Start

          To start using the operator you''ll need a license key

          Request your [30-days free trial](https://containo.us/traefikee)

          * Create a Secret with your License key
          ```
          kubectl create namespace traefikee
          kubectl create -n traefikee secret generic license --from-literal=license=${TRAEFIKEE_LICENSE_KEY}
          ```

          * Generate the client credentials
          ```
          export CLUSTER=test
          teectl setup --cluster="${CLUSTER}" --kubernetes --force
          kubectl create secret -n traefikee generic "${CLUSTER}-mtls" --from-file=bundle.zip=./bundle.zip
          kubectl label secret -n traefikee "${CLUSTER}-mtls" app=traefikee
          kubectl label secret -n traefikee "${CLUSTER}-mtls" release="${CLUSTER}"
          ```
        displayName: Traefikee Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Containous
        version: 0.4.1
      name: alpha
    defaultChannel: alpha
    packageName: traefikee-operator
    provider:
      name: Containous
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Turbonomic, Inc.
      provider-url: ""
    name: kubeturbo-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kubeturbo-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kubeturbo-operator.v6.4.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"Kubeturbo","metadata":{"name":"kubeturbo-release"},"spec":{"restAPIConfig":{"opsManagerPassword":"Turbo_password","opsManagerUserName":"Turbo_username"},"serverMeta":{"turboServer":"https://Turbo_server_URL","version":6.4}}}]'
          capabilities: Basic Install
          categories: Monitoring
          certified: "false"
          containerImage: registry.connect.redhat.com/turbonomic/kubeturbo-operator:6.4
          createdAt: "2019-05-01 00:00:00"
          description: Turbonomic Workload Automation for Multicloud simultaneously
            optimizes performance, compliance, and cost in real-time. Workloads are
            precisely resourced, automatically, to perform while satisfying business
            constraints.
          repository: https://github.com/turbonomic/kubeturbo/tree/master/deploy/kubeturbo-operator
          support: Turbonomic, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Turbonomic Workload Automation for Multicloud simultaneously
              optimizes performance, compliance, and cost in real-time. Workloads
              are precisely resourced, automatically, to perform while satisfying
              business constraints.
            displayName: Kubeturbo Operator
            kind: Kubeturbo
            name: kubeturbos.charts.helm.k8s.io
            version: v1alpha1
        description: |-
          ### Decision Automation for Kubernetes
          Turbonomic makes workloads smartâ€”enabling them to self-manageâ€”and determines the specific actions that will drive continuous health:

          * Continuous placement for Pods (rescheduling)
          * Continuous scaling for applications and  the underlying cluster.

          It assures application performance by giving workloads the resources they need when they need them.

          ### How does it work?
          Turbonomic uses a container â€” KubeTurbo â€” that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.
          KubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.
          Turbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.
        displayName: Kubeturbo Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Turbonomic, Inc.
        version: 6.4.0
      name: alpha
    defaultChannel: alpha
    packageName: kubeturbo-certified
    provider:
      name: Turbonomic, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: datagrid
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/datagrid
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: datagrid-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "infinispan.org/v1",
                "kind": "Infinispan",
                "metadata": {
                  "name": "example-infinispan"
                },
                "spec": {
                  "replicas": 1
                }
              }
            ]
          capabilities: Basic Install
          categories: Database
          certified: "false"
          containerImage: registry.redhat.io/jboss-datagrid-7-tech-preview/datagrid-operator:1.0
          createdAt: "2019-07-16 10:30:00"
          description: Create and manage Red Hat Data Grid clusters.
          repository: https://github.com/infinispan/infinispan-operator
          support: Red Hat, Inc.
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Infinispan cluster instance.
            displayName: Infinispan
            kind: Infinispan
            name: infinispans.infinispan.org
            version: v1
        description: |
          Red Hat Data Grid is a distributed, in-memory data store that increases application performance and delivers open-source capabilites to handle demanding use cases.

          The Red Hat Data Grid Operator provides operational intelligence to simplify deploying Red Hat Data Grid on Kubernetes clusters.

          ### Core Capabilities
          * **Schemaless data structure:** Store different objects as key-value pairs.
          * **Grid-based data storage:** Distribute and replicate data across clusters.
          * **Elastic scaling:** Dynamically adjust the number of nodes to meet demand without service disruption.
          * **Data interoperability:** Store, retrieve, and query data in the grid from different endpoints.
          * **High availability:** Always have access to data.

          ### Red Hat Data Grid Operator Features
          * HTTP endpoint at `8080`.
          * HotRod endpoint at `11222`.
          * Supports custom configuration for different use cases.

          The default user for accessing data is `developer`.
          The Red Hat Data Grid Operator generates a password for the default user and stores it in a Secret on startup.

          Each Red Hat Data Grid pod requests `0.5` CPUs and 512MiB of memory.
          You can adjust resource allocation with the `.spec.container.cpu` and `.spec.container.memory` fields.
        displayName: Data Grid
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: datagrid
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: knative-eventing-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/knative-eventing-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: knative-eventing-operator.v0.12.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"eventing.knative.dev/v1alpha1","kind":"KnativeEventing","metadata":{"name":"knative-eventing"},"spec":{}}]'
          capabilities: Basic Install
          categories: Networking,Integration & Delivery,Cloud Provider,Developer Tools
          certified: "false"
          containerImage: quay.io/openshift-knative/knative-eventing-operator:v0.12.0
          createdAt: "2019-05-07T17:00:00Z"
          description: |-
            Knative Eventing is a system that is designed to address a
            common need for cloud native development and provides composable
            primitives to enable late-binding event sources and event
            consumers.
          repository: https://github.com/openshift-knative/knative-eventing-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an installation of a particular version of Knative
              Eventing
            displayName: Knative Eventing
            kind: KnativeEventing
            name: knativeeventings.eventing.knative.dev
            version: v1alpha1
        description: |
          Knative Eventing is designed to address a common need for cloud
          native development:

          1. Services are loosely coupled during development and deployed independently
          1. A producer can generate events before a consumer is listening, and a consumer
             can express an interest in an event or class of events that is not yet being
             produced.
          1. Services can be connected to create new applications
             - without modifying producer or consumer, and
             - with the ability to select a specific subset of events from a particular
               producer.

          For complete Knative Eventing documentation, see
          [Knative eventing](https://www.knative.dev/docs/eventing/) or
          [Knative docs](https://www.knative.dev/docs/) to learn about Knative.
        displayName: Knative Eventing Operator
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.12.0
      name: alpha
    defaultChannel: alpha
    packageName: knative-eventing-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Infinispan
      provider-url: ""
    name: infinispan
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/infinispan
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: infinispan-operator.v1.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "infinispan.org/v1",
                "kind": "Infinispan",
                "metadata": {
                  "name": "example-infinispan"
                },
                "spec": {
                  "replicas": 1
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Database
          certified: "false"
          containerImage: docker.io/jboss/infinispan-operator:1.1.0
          createdAt: "2020-01-17T09:07:20Z"
          description: Create and manage Infinispan clusters.
          repository: https://github.com/infinispan/infinispan-operator
          support: Infinispan
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Infinispan cluster instance.
            displayName: Infinispan
            kind: Infinispan
            name: infinispans.infinispan.org
            version: v1
        description: |
          Infinispan is a distributed, in-memory data store that increases application performance and delivers open-source capabilites to handle demanding use cases.

          The Infinispan Operator provides operational intelligence to simplify deploying Infinispan on Kubernetes clusters.

          ### Core Capabilities
          * **Schemaless data structure:** Store different objects as key-value pairs.
          * **Grid-based data storage:** Distribute and replicate data across clusters.
          * **Elastic scaling:** Dynamically adjust the number of nodes to meet demand without service disruption.
          * **Data interoperability:** Store, retrieve, and query data in the grid from different endpoints.
          * **High availability:** Always have access to data.

          ### Infinispan Operator Features
          * Single port HTTP/HotRod endpoint at `11222`.
          * Supports custom configuration for different use cases.

          The default user for accessing data is `developer`.
          The Infinispan Operator generates a password for the default user and stores it in a Secret on startup.

          Each Infinispan pod requests `0.5` CPUs, 512MiB of memory and 1Gi of ReadWriteOnce persistent volume.
          You can adjust resource allocation with the `.spec.container.cpu` and `.spec.container.memory` fields.
        displayName: infinispan
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Infinispan
        version: 1.1.0
      name: dev-preview
    - currentCSV: infinispan-operator.v1.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "infinispan.org/v1",
                "kind": "Infinispan",
                "metadata": {
                  "name": "example-infinispan"
                },
                "spec": {
                  "replicas": 1
                }
              }
            ]
          capabilities: Basic Install
          categories: Database
          certified: "false"
          containerImage: docker.io/jboss/infinispan-operator:1.0.1
          createdAt: "2019-10-28T16:55:39Z"
          description: Create and manage Infinispan clusters.
          repository: https://github.com/infinispan/infinispan-operator
          support: Infinispan
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Infinispan cluster instance.
            displayName: Infinispan
            kind: Infinispan
            name: infinispans.infinispan.org
            version: v1
        description: |
          Infinispan is a distributed, in-memory data store that increases application performance and delivers open-source capabilites to handle demanding use cases.

          The Infinispan Operator provides operational intelligence to simplify deploying Infinispan on Kubernetes clusters.

          ### Core Capabilities
          * **Schemaless data structure:** Store different objects as key-value pairs.
          * **Grid-based data storage:** Distribute and replicate data across clusters.
          * **Elastic scaling:** Dynamically adjust the number of nodes to meet demand without service disruption.
          * **Data interoperability:** Store, retrieve, and query data in the grid from different endpoints.
          * **High availability:** Always have access to data.

          ### Infinispan Operator Features
          * Single port HTTP/HotRod endpoint at `11222`.
          * Supports custom configuration for different use cases.

          The default user for accessing data is `developer`.
          The Infinispan Operator generates a password for the default user and stores it in a Secret on startup.

          Each Infinispan pod requests `0.5` CPUs, 512MiB of memory and 1Gi of ReadWriteOnce persistent volume.
          You can adjust resource allocation with the `.spec.container.cpu` and `.spec.container.memory` fields.
        displayName: infinispan
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Infinispan
        version: 1.0.1
      name: stable
    defaultChannel: dev-preview
    packageName: infinispan
    provider:
      name: Infinispan
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Eclipse Foundation
      provider-url: ""
    name: eclipse-che
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/eclipse-che
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: eclipse-che.v7.8.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "org.eclipse.che/v1",
                "kind": "CheCluster",
                "metadata": {
                   "name": "eclipse-che"
                },
                "spec": {
                   "server": {
                      "cheImageTag": "",
                      "devfileRegistryImage": "",
                      "pluginRegistryImage": "",
                      "tlsSupport": false,
                      "selfSignedCert": false
                   },
                   "database": {
                      "externalDb": false,
                      "chePostgresHostName": "",
                      "chePostgresPort": "",
                      "chePostgresUser": "",
                      "chePostgresPassword": "",
                      "chePostgresDb": ""
                   },
                   "auth": {
                      "openShiftoAuth": true,
                      "identityProviderImage": "",
                      "externalIdentityProvider": false,
                      "identityProviderURL": "",
                      "identityProviderRealm": "",
                      "identityProviderClientId": ""
                   },
                   "storage": {
                      "pvcStrategy": "per-workspace",
                      "pvcClaimSize": "1Gi",
                      "preCreateSubPaths": true
                   }
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Developer Tools, OpenShift Optional
          certified: "false"
          containerImage: quay.io/eclipse/che-operator:7.8.0
          createdAt: "2020-02-03T07:34:57Z"
          description: A Kube-native development solution that delivers portable and
            collaborative developer workspaces in OpenShift.
          repository: https://github.com/eclipse/che-operator
          support: Eclipse Foundation
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Eclipse Che cluster with DB and Auth Server
            displayName: Eclipse Che Cluster
            kind: CheCluster
            name: checlusters.org.eclipse.che
            version: v1
        description: |
          A collaborative Kubernetes-native development solution that delivers OpenShift workspaces and in-browser IDE for rapid cloud application development.
          This operator installs PostgreSQL, Keycloak, and the Eclipse Che server, as well as configures all three services.

          ## How to Install

          Press the **Install** button, choose the upgrade strategy, and wait for the **Installed** Operator status.

          When the operator is installed, create a new CR of Kind CheCluster (click the **Create New** button).
          The CR spec contains all defaults (see below).

          You can start using Eclipse Che when the CR status is set to **Available**, and you see a URL to Eclipse Che.

          ## Defaults

          By default, the operator deploys Eclipse Che with:

          * Bundled PostgreSQL and Keycloak

          * Per-Workspace PVC strategy

          * Auto-generated passwords

          * HTTP mode (non-secure routes)

          * Regular login extended with OpenShift OAuth authentication

          ## Installation Options

          Eclipse Che operator installation options include:

          * Connection to external database and Keycloak

          * Configuration of default passwords and object names

          * TLS mode

          * PVC strategy (once shared PVC for all workspaces, PVC per workspace, or PVC per volume)

          * Authentication options

          ### External Database and Keycloak

          To instruct the operator to skip deploying PostgreSQL and Keycloak and connect to an existing DB and Keycloak instead:

          * set respective fields to `true` in a custom resource spec

          * provide the operator with connection and authentication details:



            `externalDb: true`


            `chePostgresHostname: 'yourPostgresHost'`


            `chePostgresPort: '5432'`


            `chePostgresUser: 'myuser'`


            `chePostgresPassword: 'mypass'`


            `chePostgresDb: 'mydb'`


            `externalIdentityProvider: true`


            `identityProviderURL: 'https://my-keycloak.com'`


            `identityProviderRealm: 'myrealm'`


            `identityProviderClientId: 'myClient'`


          ### TLS Mode

          To activate TLS mode, set the respective field in the CR spec to `true` (in the `server` block):


          ```
          tlsSupport: true
          ```

          #### Self-signed Certificates

          To use Eclipse Che with TLS enabled, but the OpenShift router does not use certificates signed by a public authority, you can use self-signed certificates, which the operator can fetch for you:


          ```
          selfSignedCert: true
          ```


          You can also manually create a secret:



          ```
          oc create secret self-signed-certificate generic --from-file=/path/to/certificate/ca.crt -n=$codeReadyNamespace
          ```
        displayName: Eclipse Che
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Eclipse Foundation
        version: 7.8.0
      name: stable
    defaultChannel: stable
    packageName: eclipse-che
    provider:
      name: Eclipse Foundation
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Starburst Data
      provider-url: ""
    name: presto-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/presto-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: presto-operator.0.19.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "starburstdata.com/v1",
                "kind": "Presto",
                "metadata": {
                  "name": "example-presto"
                },
                "spec": {
                  "nameOverride": "example-presto",
                  "service": {
                    "name": "example-presto"
                  }
                }
              }
            ]
          capabilities: Auto Pilot
          categories: Big Data
          certified: "true"
          containerImage: starburstdata/presto-operator:323-e.5-k8s-0.19-ubi8
          createdAt: "2020-02-19T18:22:35Z"
          description: Starburst Presto Operator to orchestrate Presto clusters
          support: Starburst Data, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Presto worker/coordinator
            displayName: Presto
            kind: Presto
            name: prestos.starburstdata.com
            version: v1
        description: Starburst Presto Operator to orchestrate Presto clusters
        displayName: Presto Operator
        installModes:
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: OwnNamespace
        provider:
          name: Starburst Data
        version: 0.19.0
      name: alpha
    defaultChannel: alpha
    packageName: presto-operator
    provider:
      name: Starburst Data
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: kiali-ossm
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kiali-ossm
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kiali-operator.v1.0.9
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "kiali.io/v1alpha1",
                "kind": "Kiali",
                "metadata": {
                  "name": "kiali"
                },
                "spec": {
                  "installation_tag": "My Kiali",
                  "istio_namespace": "istio-system",
                  "deployment": {
                    "namespace": "istio-system",
                    "verbose_mode": "4",
                    "view_only_mode": false
                  },
                  "external_services": {
                    "grafana": {
                      "url": ""
                    },
                    "prometheus": {
                      "url": ""
                    },
                    "tracing": {
                      "url": ""
                    }
                  },
                  "server": {
                    "web_root": "/mykiali"
                  }
                }
              },
              {
                "apiVersion": "monitoring.kiali.io/v1alpha1",
                "kind": "MonitoringDashboard",
                "metadata": {
                  "name": "myappdashboard"
                },
                "spec": {
                  "title": "My App Dashboard",
                  "items": [
                    {
                      "chart": {
                        "name": "My App Processing Duration",
                        "unit": "seconds",
                        "spans": 6,
                        "metricName": "my_app_duration_seconds",
                        "dataType": "histogram",
                        "aggregations": [
                          {
                            "label": "id",
                            "displayName": "ID"
                          }
                        ]
                      }
                    }
                  ]
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Monitoring,Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift-service-mesh/kiali-rhel7-operator:1.0.9
          createdAt: "2019-12-17T21:15:24Z"
          description: 'Kiali project provides answers to the questions: What microservices
            are part of my Istio service mesh and how are they connected?'
          repository: https://github.com/kiali/kiali
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A configuration file for a Kiali installation.
            displayName: Kiali
            kind: Kiali
            name: kialis.kiali.io
            version: v1alpha1
          - description: A configuration file for defining an individual metric dashboard.
            displayName: Monitoring Dashboard
            kind: MonitoringDashboard
            name: monitoringdashboards.monitoring.kiali.io
            version: v1alpha1
        description: |-
          ## About the managed application

          A Microservice Architecture breaks up the monolith into many smaller pieces
          that are composed together. Patterns to secure the communication between
          services like fault tolerance (via timeout, retry, circuit breaking, etc.)
          have come up as well as distributed tracing to be able to see where calls
          are going.

          A service mesh can now provide these services on a platform level and frees
          the application writers from those tasks. Routing decisions are done at the
          mesh level.

          Kiali works with Istio, in OpenShift or Kubernetes, to visualize the service
          mesh topology, to provide visibility into features like circuit breakers,
          request rates and more. It offers insights about the mesh components at
          different levels, from abstract Applications to Services and Workloads.

          See [https://www.kiali.io](https://www.kiali.io) to read more.

          ### Accessing the UI

          By default, the Kiali operator exposes the Kiali UI as a Route on OpenShift
          or Ingress on Kubernetes.

          On OpenShift, the default root context path is '/' and on Kubernetes it is
          '/kiali' though you can change this by configuring the 'web_root' setting in
          the Kiali CR.

          ## About this Operator

          ### Kiali Custom Resource Configuration Settings

          For quick descriptions of all the settings you can configure in the Kiali
          Custom Resource (CR), see the file
          [kiali_cr.yaml](https://github.com/kiali/kiali/blob/v1.7.0/operator/deploy/kiali/kiali_cr.yaml)

          Note that the Kiali operator can be told to restrict Kiali's access to
          specific namespaces, or can provide to Kiali cluster-wide access to all
          namespaces.

          ## Prerequisites for enabling this Operator

          Today Kiali works with Istio. So before you install Kiali, you must have
          already installed Istio. Note that Istio can come pre-bundled with Kiali
          (specifically if you installed the Istio demo helm profile or if you
          installed Istio with the helm option '--set kiali.enabled=true'). If you
          already have the pre-bundled Kiali in your Istio environment and you want to
          install Kiali via the Kiali Operator, uninstall the pre-bundled Kiali first.
          You can do this via this command,

              kubectl delete --ignore-not-found=true all,secrets,sa,templates,configmaps,deployments,clusterroles,clusterrolebindings,ingresses,customresourcedefinitions --selector="app=kiali" -n istio-system

          When you install Kiali in a non-OpenShift Kubernetes environment, the
          authentication strategy will default to `login`. When using the
          authentication strategy of `login`, you are required to create a Kubernetes
          Secret with a `username` and `passphrase` that you want users to provide in
          order to successfully log into Kiali. Here is an example command you can
          execute to create such a secret (with a username of `admin` and a passphrase
          of `admin`),

              kubectl create secret generic kiali -n istio-system --from-literal "username=admin" --from-literal "passphrase=admin"

          If you wish to use the "ldap" authentication strategy, you must have an LDAP
          server available and accessible to Kiali.
        displayName: Kiali Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.0.9
      name: stable
    defaultChannel: stable
    packageName: kiali-ossm
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Containers & PaaS CoP
      provider-url: ""
    name: namespace-configuration-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/namespace-configuration-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: namespace-configuration-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "redhatcop.redhat.io/v1alpha1",
                "kind": "NamespaceConfig",
                "metadata": {
                  "name": "example-namespaceconfig"
                },
                "spec": {
                  "size": 3
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Security
          certified: "false"
          containerImage: quay.io/redhat-cop/namespace-configuration-operator:latest
          createdAt: 5/28/2019
          description: This operator provides a facility to define and enforce namespace
            configurations
          repository: https://github.com/redhat-cop/namespace-configuration-operator
          support: Best Effort
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represent the desired configuration for a set of namespaces
              selected via labels
            displayName: Namespace Configuration
            kind: NamespaceConfig
            name: namespaceconfigs.redhatcop.redhat.io
            version: v1alpha1
        description: "The namespace configuration operator helps keeping a namespace's
          configuration aligned with one of more policies specified as a CRs.\n\nThe
          `NamespaceConfig` CR allows specifying one or more objects that will be
          created in the selected namespaces.\n\nFor example using this operator an
          administrator can enforce a specific ResourceQuota or LimitRange on a set
          of namespaces. For example with the following snippet:\n\n```\napiVersion:
          redhatcop.redhat.io/v1alpha1\nkind: NamespaceConfig\nmetadata:\n  name:
          small-size\nspec:\n  selector:\n    matchLabels:\n      size: small  \n
          \ resources:\n  - apiVersion: v1\n    kind: ResourceQuota\n    metadata:\n
          \     name: small-size  \n    spec:\n      hard:\n        requests.cpu:
          \"4\"\n        requests.memory: \"2Gi\"\n```\n\nwe are enforcing that all
          the namespaces with label: `size=small` receive the specified resource quota.
          \ \n"
        displayName: Namespace Configuration Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Containers & PaaS CoP
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: namespace-configuration-operator
    provider:
      name: Containers & PaaS CoP
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Helm Community
      provider-url: ""
    name: cockroachdb
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cockroachdb
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: cockroachdb.v2.1.11
      currentCSVDesc:
        annotations:
          alm-examples: '[{ "apiVersion": "charts.helm.k8s.io/v1alpha1", "kind": "Cockroachdb",
            "metadata": { "name": "example" }, "spec": { "Name": "cdb", "Image": "cockroachdb/cockroach",
            "ImageTag": "v19.1.3", "ImagePullPolicy": "Always", "Replicas": 3, "MaxUnavailable":
            1, "Component": "cockroachdb", "InternalGrpcPort": 26257, "ExternalGrpcPort":
            26257, "InternalGrpcName": "grpc", "ExternalGrpcName": "grpc", "InternalHttpPort":
            8080, "ExternalHttpPort": 8080, "HttpName": "http", "Resources": { "requests":
            { "cpu": "500m", "memory": "512Mi" } }, "InitPodResources": { }, "Storage":
            "10Gi", "StorageClass": null, "CacheSize": "25%", "MaxSQLMemory": "25%",
            "ClusterDomain": "cluster.local", "NetworkPolicy": { "Enabled": false,
            "AllowExternal": true }, "Service": { "type": "ClusterIP", "annotations":
            { } }, "PodManagementPolicy": "Parallel", "UpdateStrategy": { "type":
            "RollingUpdate" }, "NodeSelector": { }, "Tolerations": { }, "Secure":
            { "Enabled": false, "RequestCertsImage": "cockroachdb/cockroach-k8s-request-cert",
            "RequestCertsImageTag": "0.4", "ServiceAccount": { "Create": true } }
            } }]'
          capabilities: Basic Install
          categories: Database
          certified: "false"
          containerImage: quay.io/helmoperators/cockroachdb:2.1.1
          createdAt: 2019-01-24T15-33-43Z
          description: CockroachDB Operator based on the CockroachDB helm chart
          repository: https://github.com/dmesser/cockroachdb-operator
          support: a-robinson
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a CockroachDB cluster
            displayName: CockroachDB
            kind: Cockroachdb
            name: cockroachdbs.charts.helm.k8s.io
            version: v1alpha1
        description: |
          CockroachDB is a scalable, survivable, strongly-consistent SQL database.

          ## About this Operator

          This Operator is based on a Helm chart for CockroachDB. It supports reconfiguration for some parameters, but notably does not handle scale down of the replica count in a seamless manner. Scale up works great.

          ## Core capabilities
          * **StatefulSet** - Sets up a dynamically scalable CockroachDB cluster using a Kubernetes StatefulSet
          * **Expand Replicas** - Supports expanding the set of replicas by simply editing your object
          * **Dashboard** - Installs the CockroachDB user interface to administer your cluster. Easily expose it via an Ingress rule.

          Review all of the [confiuguration options](https://github.com/helm/charts/tree/master/stable/cockroachdb#configuration) to best run your database instance. The example configuration is derived from the chart's [`values.yaml`](https://github.com/helm/charts/blob/master/stable/cockroachdb/values.yaml).

          ## Using the cluster

          The resulting cluster endpoint can be consumed from a `Service` that follows the pattern: `<StatefulSet-name>-public`. For example to connect using the command line client, use something like the following to obtain the name of the service:

          ```
          kubectl get service -l chart=cockroachdb-2.0.11
          NAME                                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)              AGE
          example-9f8ngwzrxbxrulxqmdqfhn51h-cdb          ClusterIP   None             <none>        26257/TCP,8080/TCP   24m
          example-9f8ngwzrxbxrulxqmdqfhn51h-cdb-public   ClusterIP   10.106.249.134   <none>        26257/TCP,8080/TCP   24m
          ```

          Then you can use the CockroachDB command line client to connect to the database cluster:

          ```
          kubectl run -it --rm cockroach-client --image=cockroachdb/cockroach --restart=Never --command -- ./cockroach sql --insecure --host example-9f8ngwzrxbxrulxqmdqfhn51h-cdb-public
          ```

          ## Before you start

          This Operator requires a cluster with PV support in order to run correctly.
        displayName: CockroachDB
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Helm Community
        version: 2.1.11
      name: stable
    defaultChannel: stable
    packageName: cockroachdb
    provider:
      name: Helm Community
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Tremolo Security, Inc.
      provider-url: ""
    name: myvirtualdirectory
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/myvirtualdirectory
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: myvirtualdirectory.1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "myvirtualdirectory.tremolo.io/v1",
                "kind": "MyVirtualDirectory",
                "metadata": {
                  "name":"myvirtualdirectory"
                },
                "spec": {
                  "image": "docker.io/tremolosecurity/myvirtualdirectory:latest",
                  "replicas": 1,
                  "dest_secret": "myvd-secret",
                  "dest_cfg_map": "myvd-configmap",
                  "source_secret": "myvd-secrets-source",
                  "secret_data": [
                    "MYVD_TLS_KEY_PASSWORD"
                  ],
                  "non_secret_data": [
                    {
                      "name": "ROOT",
                      "value": "o=Tremolo"
                    }
                  ],
                  "myvd_props": "#Global AuthMechConfig\nserver.globalChain=accesslog,attrCleaner\nserver.globalChain.accesslog.className=com.tremolosecurity.proxy.myvd.log.AccessLog\nserver.globalChain.attrCleaner.className=net.sourceforge.myvd.inserts.mapping.AttributeCleaner\nserver.globalChain.attrCleaner.config.clearAttributes=true\nserver.globalChain.dump.className=net.sourceforge.myvd.inserts.DumpTransaction\nserver.globalChain.dump.config.logLevel=info\nserver.globalChain.dump.config.label=global\n\nserver.nameSpaces=rootdse,myvdroot\nserver.rootdse.chain=dse\nserver.rootdse.nameSpace=\nserver.rootdse.weight=0\nserver.rootdse.dse.className=net.sourceforge.myvd.inserts.RootDSE\nserver.rootdse.dse.config.namingContexts=#[ROOT]\nserver.myvdroot.chain=root\nserver.myvdroot.nameSpace=#[ROOT]\nserver.myvdroot.weight=0\nserver.myvdroot.root.className=net.sourceforge.myvd.inserts.RootObject",
                  "myvd_log4j2_xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<!--\n    Copyright 2018 Tremolo Security, Inc.\n    \n    Licensed under the Apache License, Version 2.0 (the \"License\");\n    you may not use this file except in compliance with the License.\n    You may obtain a copy of the License at\n    \n        http://www.apache.org/licenses/LICENSE-2.0\n    \n    Unless required by applicable law or agreed to in writing, software\n    distributed under the License is distributed on an \"AS IS\" BASIS,\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n    See the License for the specific language governing permissions and\n    limitations under the License.\n-->\n\n<Configuration>\n    <Appenders>\n    <Console name=\"STDOUT\" target=\"SYSTEM_OUT\">\n        <PatternLayout pattern=\"[%d][%t] %-5p %c{1} - %m%n\"/>\n    </Console>\n    </Appenders>\n    <Loggers>\n    \n    <Root level=\"info\">\n        <AppenderRef ref=\"STDOUT\"/>\n    </Root>\n    </Loggers>\n</Configuration>",
                  "myvd_network_configuration": {
                    "open_port": 10389,
                    "secure_port": 10636,
                    "secure_key_alias": "myvd-tls",
                    "client_auth": "none",
                    "allowed_client_names": [],
                    "ciphers": [
                      "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
                      "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256",
                      "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA",
                      "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
                      "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384",
                      "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA"
                    ],
                    "path_to_deployment": "/etc/myvd-config",
                    "path_to_env_file": "/etc/myvd"
                  },
                  "key_store": {
                    "static_keys": [],
                    "trusted_certificates": [],
                    "key_pairs": {
                      "create_keypair_template": [
                        {
                          "name": "ou",
                          "value": "k8s"
                        },
                        {
                          "name": "o",
                          "value": "Tremolo Security"
                        },
                        {
                          "name": "l",
                          "value": "Alexandria"
                        },
                        {
                          "name": "st",
                          "value": "Virginia"
                        },
                        {
                          "name": "c",
                          "value": "US"
                        }
                      ],
                      "keys": [
                        {
                          "name": "myvd-tls",
                          "tls_secret_name": "myvd-tls-secret",
                          "import_into_ks": "keypair",
                          "create_data": {
                            "sign_by_k8s_ca" : false,
                            "server_name": "${k8s_obj.metadata.name + '.' + k8s_namespace + '.svc.cluster.local'}","subject_alternative_names": [],
                            "key_size": 2048,
                            "ca_cert": false
                          }
                        }
                      ]
                    }
                  }
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Security
          certified: "false"
          containerImage: docker.io/tremolosecurity/myvd-k8s-operator
          createdAt: "2019-04-15"
          description: MyVirtualDirectory Operator to provide LDAP virtual directory
            services and LDAP proxy services
          repository: https://github.com/TremoloSecurity/myvd-k8s-operator
          support: https://github.com/TremoloSecurity/myvd-k8s-operator
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A running MyVirtualDirectory instance
            displayName: MyVirtualDirectory
            kind: MyVirtualDirectory
            name: myvirtualdirectories.myvirtualdirectory.tremolo.io
            version: v1
        description: "The MyVirtualDirectory operator enables the deployment of MyVirtualDirectory
          inside of your Kubernetes cluster.  \n\n## Before You Start\n\nBefore you
          can deploy MyVirtualDirectory you'll need to create a source secret with
          information that shouldn't be stored in the `MyVirtualDirectory` custom
          resource, referenced by the `source_secret` attribute.  The minimal data
          in this secret should be at least `MYVD_TLS_KEY_PASSWORD`, the password
          for the keystore the operator creates.  An example of creating a secret
          used by the example configuration is:\n  \n```\n$ mkdir secret\n$ cd secret\n$
          echo 'my_secret_password' | base64 > MYVD_TLS_KEY_PASSWORD\n$ oc create
          secret generic myvd-secrets-source --from-file=.\n```\n  \nThe default configuration
          will generate a self signed certificate for TLS."
        displayName: MyVirtualDirectory
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Tremolo Security, Inc.
        version: 1.0.0
      name: beta
    defaultChannel: beta
    packageName: myvirtualdirectory
    provider:
      name: Tremolo Security, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Open Data Hub
      provider-url: ""
    name: opendatahub-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/opendatahub-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: opendatahub-operator.v0.5.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "opendatahub.io/v1alpha1",
                "kind": "OpenDataHub",
                "metadata": {
                  "name": "example-opendatahub"
                },
                "spec": {
                  "aicoe-jupyterhub": {
                    "odh_deploy": true,
                    "notebook_cpu": 1,
                    "notebook_memory": "2Gi",
                    "registry": "quay.io",
                    "repository": "odh-jupyterhub",
                    "notebook_images": {
                      "deploy_all_notebooks": false,
                      "deploy_cuda_notebooks": false
                    },
                    "storage_class": null,
                    "db_memory": "1Gi",
                    "jupyterhub_memory": "1Gi",
                    "notebook_image": "s2i-minimal-notebook:3.6",
                    "s3_endpoint_url": "",
                    "gpu_mode": "",
                    "user_pvc_size": "2Gi",
                    "spark_configmap_template": "jupyterhub-spark-operator-configmap",
                    "spark_pyspark_submit_args": "--conf spark.cores.max=6 --conf spark.executor.instances=2 --conf spark.executor.memory=3G --conf spark.executor.cores=3 --conf spark.driver.memory=4G --packages com.amazonaws:aws-java-sdk:1.7.4,org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell",
                    "spark_pyspark_driver_python": "jupyter",
                    "spark_pyspark_driver_python_opts": "notebook",
                    "spark_home": "/opt/app-root/lib/python3.6/site-packages/pyspark/",
                    "spark_pythonpath": "$PYTHONPATH:/opt/app-root/lib/python3.6/site-packages/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/:/opt/app-root/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.8.2.1-src.zip",
                    "spark": {
                      "image": "quay.io/opendatahub/spark-cluster-image:spark22python36",
                      "worker": {
                        "instances": 2,
                        "resources": {
                          "limits": {
                            "memory": "4Gi",
                            "cpu": 3
                          },
                          "requests": {
                            "memory": "1Gi",
                            "cpu": "500m"
                          }
                        }
                      },
                      "master": {
                        "instances": 1,
                        "resources": {
                          "limits": {
                            "memory": "1Gi",
                            "cpu": 1
                          },
                          "requests": {
                            "memory": "512Mi",
                            "cpu": "500m"
                          }
                        }
                      }
                    }
                  },
                  "spark-operator": {
                    "odh_deploy": true,
                    "worker": {
                      "instances": 0,
                      "resources": {
                        "limits": {
                          "memory": "2Gi",
                          "cpu": 1
                        },
                        "requests": {
                          "memory": "1Gi",
                          "cpu": "500m"
                        }
                      }
                    },
                    "master": {
                      "instances": 0,
                      "resources": {
                        "limits": {
                          "memory": "1Gi",
                          "cpu": 1
                        },
                        "requests": {
                          "memory": "512Mi",
                          "cpu": "500m"
                        }
                      }
                    }
                  },
                  "seldon": {
                    "odh_deploy": false
                  },
                  "kafka": {
                    "odh_deploy": false,
                    "kafka_cluster_name": "odh-message-bus",
                    "kafka_broker_replicas": 3,
                    "kafka_zookeeper_replicas": 3
                  },
                  "monitoring": {
                    "odh_deploy": false,
                    "enable_pushgateway": false
                  },
                  "beakerx": {
                    "odh_deploy": false
                  },
                  "ai-library": {
                    "odh_deploy": false
                  },
                  "argo": {
                    "odh_deploy": false
                  },
                  "superset": {
                    "odh_deploy": false,
                    "image": "quay.io/aiops/superset:v0.30",
                    "superset_admin": {
                      "admin_usr": "userKPJ",
                      "admin_psw": "7ujmko0",
                      "admin_fname": "admin",
                      "admin_lname": "admin",
                      "admin_email": "admin@fab.org"
                    },
                    "secret_key": "thisISaSECRET_1234",
                    "data_volume_size": "512Mi",
                    "sqlalchemy_db_uri": "sqlite:////var/lib/superset/superset.db"
                  },
                  "data-catalog": {
                    "odh_deploy": false,
                    "aws_access_key_id": "changeme",
                    "aws_secret_access_key": "changeme",
                    "s3_endpoint": "s3.foo.com",
                    "s3_port": 8080,
                    "s3_is_secure": false,
                    "spark-cluster": {
                      "master_node_count": 1,
                      "master_memory": "1Gi",
                      "master_cpu": 1,
                      "worker_node_count": 2,
                      "worker_memory": "2Gi",
                      "worker_cpu": 2,
                      "spark_cluster_name": "spark-cluster-data-catalog",
                      "spark_image": "quay.io/opendatahub/spark-cluster-image:spark24"
                    },
                    "hive-metastore": {
                      "database": {
                        "image": "registry.access.redhat.com/rhscl/postgresql-96-rhel7",
                        "username": "changeme",
                        "password": "changeme",
                        "memory_limit": "1Gi",
                        "volume_capacity": "10Gi",
                        "driver": "org.postgresql.Driver"
                      },
                      "warehouse_volume_capacity": "10Gi"
                    },
                    "thrift-server": {
                      "spark_cluster_port": 7077,
                      "spark_max_cores": 6
                    },
                    "hue": {
                      "database": {
                        "username": "changeme",
                        "password": "changeme",
                        "root_password": "changeme",
                        "volume_capacity": "10Gi",
                        "image": "registry.access.redhat.com/rhscl/mysql-57-rhel7",
                        "memory_limit": "1Gi"
                      },
                      "hue_secret_key": "changeme"
                    }
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: AI/Machine Learning, Big Data
          certified: "false"
          containerImage: quay.io/opendatahub/opendatahub-operator:v0.5.1
          createdAt: "2019-12-04T01:00:13Z"
          description: The Open Data Hub is a machine-learning-as-a-service platform
            built on Red Hat's Kubernetes-based OpenShiftÂ® Container Platform, Ceph
            Object Storage, and integrating a collection of open source projects.
          repository: https://gitlab.com/opendatahub/opendatahub-operator
          support: Open Data Hub
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Deployment of components from the Open Data Hub community
            displayName: Open Data Hub
            kind: OpenDataHub
            name: opendatahubs.opendatahub.io
            version: v1alpha1
        description: |
          The Open Data Hub is a machine-learning-as-a-service platform built on Red Hat's Kubernetes-based OpenShiftÂ® Container Platform, Ceph Object Storage, and integrating a collection of open source projects.

          Open Data Hub is a meta-project that integrates open source projects into a practical solution. It aims to foster collaboration between communities, vendors, user-enterprises, and academics following open source best practices. The open source community can experiment and develop intelligent applications without incurring high costs and having to master the complexity of modern machine learning and artificial intelligence software stacks.

          ### Components
          * JupyterHub v3.0.7 - Open source multi-user notebook platform w/ GPU support
          * Apache Spark v2.2.3 - Unified analytics engine for large-scale data processing
          * Prometheus v2.3 - Monitoring and alerting tool
          * Grafana v4.7 - Data visualization and monitoring
          * Two Sigma BeakerX v1.4.0 - Jupyter notebook extension that includes tools for plotting, creating tables and forms and many more.
          * AI-Library v1.0 - An open source collection of machine learning algorithms
          * Argo v2.4.2 - Container-native Workflow Engine
          * Seldon Core v0.2.7 - Open source platform for deploying machine learning models
          * Apache Superset v0.34.0 - Open source application for data exploration and visualization
          * Data Catalog - Deployment of Hue v4.4.1 and Hive Thrift Server to browse S3 Object storage and querying data using HiveSQL in the Cloudera Hue WebUI
          * Apache Kafka v0.11.1 - Open source stream processing platform. A prerequisite for Kafka is to install Strimzi Apache Kafka Operator prior to deploying Open Data Hub

          If deploying Argo, Seldon or Apache Kafka as part of Open Data Hub, you will need to make sure that each component's CustomResourceDefinition has been added to the cluster by a user with cluster-admin privileges. See documentation on [opendatahub.io](https://opendatahub.io) for more information.
        displayName: Open Data Hub Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Open Data Hub
        version: 0.5.1
      name: alpha
    defaultChannel: alpha
    packageName: opendatahub-operator
    provider:
      name: Open Data Hub
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: ProphetStor Data Services, Inc.
      provider-url: ""
    name: federatorai-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/federatorai-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: federatorai.v4.2.605
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "federatorai.containers.ai/v1alpha1",
                "kind": "AlamedaService",
                "metadata": {
                  "name": "my-alamedaservice"
                },
                "spec": {
                  "keycode": {
                    "codeNumber": "GRV7J-LA4TX-KPPIT-S6GRS-NK4EB-ILFRQ"
                  },
                  "selfDriving": false,
                  "enableExecution": true,
                  "enableGui": true,
                  "version": "v4.2.605",
                  "prometheusService": "https://prometheus-k8s.openshift-monitoring:9091",
                  "storages": [
                    {
                      "usage": "log",
                      "type": "ephemeral"
                    },
                    {
                      "usage": "data",
                      "type":"ephemeral"
                    }
                  ]
                }
              }
            ]
          capabilities: Auto Pilot
          categories: AI/Machine Learning, OpenShift Optional
          certified: "true"
          containerImage: registry.connect.redhat.com/prophetstor/federatorai-operator:4.2.605
          createdAt: "2020-02-06T17:00:00Z"
          description: Federator.ai Operator provides easy configuration and management
            of AI-based Kubernetes resource orchestrator
          repository: https://access.redhat.com/containers/#/registry.connect.redhat.com/prophetstor/federatorai-operator
          support: ProphetStor Data Services, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An instance of Alameda.
            displayName: AlamedaService
            kind: AlamedaService
            name: alamedaservices.federatorai.containers.ai
            version: v1alpha1
        description: |
          **Federator.ai**, ProphetStor's Artificial Intelligence for IT Operations (AIOps) platform, provides intelligence to orchestrate container resources on top of VMs (virtual machines) or bare metal, allowing users to operate applications without the need to manage the underlying computing resources. It aims to provide optimal resource planning recommendations that will help enterprises make better decisions. The benefits of **Federator.ai** include:
          - Up to 60% resource savings
          - Increased operational efficiency
          - Reduced manual configuration time with digital intelligence

          For more information, visit our [website](https://www.prophetstor.com/federator-ai/federator-ai-for-openshift/) and [github](https://github.com/containers-ai/federatorai-operator).

          **Federator.ai Operator** is an Operator that manages **Federator.ai** components for an OpenShift cluster. Once installed, it provides the following features:
          - **Create/Clean up**: Launch **Federator.ai** components using the Operator.
          - **Easy Configuration**: Easily configure data source of Prometheus and enable/disable add-on components, such as GUI, and predictive autoscaling.
          - **Pod Scaling Recommendation/Autoscaling**: Use provided CRD to setup target pods and desired policies for scaling recommendation and autoscaling.

          ### Prerequisite
          **Federator.ai** requires a Prometheus datasource to get historical metrics of pods and nodes. When launching **Federator.ai** components, Prometheus connection settings need to be provided.

          ### Common Configurations
              apiVersion: federatorai.containers.ai/v1alpha1
              kind: AlamedaService
              metadata:
                name: my-alamedaservice
              spec:
                keycode:
                  codeNumber: GRV7J-LA4TX-KPPIT-S6GRS-NK4EB-ILFRQ
                # experimental feature, set true to enable
                selfDriving: false
                enableExecution: true
                enableGui: true
                version: v4.2.605
                prometheusService: https://prometheus-k8s.openshift-monitoring:9091
                # storages is optional. Omit this field if not needed.
                storages:
                  - usage: log       # the supported usages are log and data
                    type: ephemeral  # the supported types are ephemeral and pvc
                  - usage: data
                    type: ephemeral
        displayName: Federator.ai
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: ProphetStor Data Services, Inc.
        version: 4.2.605
      name: stable
    defaultChannel: stable
    packageName: federatorai-certified
    provider:
      name: ProphetStor Data Services, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: CognitiveScale
      provider-url: ""
    name: cortex-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cortex-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: cortex-operator.v0.1.2
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "charts.helm.k8s.io/v1alpha1",
                "kind": "Cortex5",
                "metadata": {
                  "name": "no-infra-cortex5"
                },
                "spec": {
                  "accounts": {
                    "tag": "5.0.492-g05eb988-ubi7"
                  },
                  "actions": {
                    "tag": "5.0.577-g2081776-ubi7"
                  },
                  "adminui": {
                    "tag": "5.0.5-g564032f-ubi7"
                  },
                  "agents": {
                    "tag": "5.0.399-g92ae742-ubi7"
                  },
                  "catalog": {
                    "tag": "5.0.296-ge185713-ubi7"
                  },
                  "connections": {
                    "tag": "5.0.288-g5eba827-ubi7"
                  },
                  "connectiontypeloader": {
                    "tag": "5.0.32-gc4e0c74-ubi7"
                  },
                  "cortex": {
                    "imageRegistry": "610527985415.dkr.ecr.us-west-2.amazonaws.com",
                    "managedContent": {
                      "S3_BUCKET": "colt-clean-test-content",
                      "S3_ENDPOINT": "https://s3.amazonaws.com",
                      "S3_REGION": "us-east-1",
                      "accesskey": "{{.AWS_ACCESS_KEY_ID}}",
                      "secretkey": "{{.AWS_SECRET_ACCESS_KEY}}"
                    },
                    "mongodb": {
                      "graphUri": "mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@performance-testing-etur0.mongodb.net/cortex-graph?authSource=admin\u0026replicaSet=rs0",
                      "metricsUri": "mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@performance-testing-etur0.mongodb.net/cortex-metrics?authSource=admin\u0026replicaSet=rs0",
                      "mongoUri": "mongodb://$(MONGODB_USERNAME):$(MONGODB_PASSWORD)@performance-testing-etur0.mongodb.net/cortex-admin?authSource=admin\u0026replicaSet=rs0",
                      "password": "L7HlijT6aCJi8tE2",
                      "username": "cortex"
                    },
                    "postgresql": {
                      "password": ""
                    },
                    "redis": {
                      "env": {
                        "REDIS_TYPE": "node"
                      },
                      "password": "something",
                      "redisUri": "redis://:$(REDIS_PASSWORD)@cortex-redis-master-0.cortex-redis-headless.cortex.svc.cluster.local:6379"
                    }
                  },
                  "datasets": {
                    "tag": "5.0.86-gb3a1b0b-ubi7"
                  },
                  "deployModel": {
                    "type": "openshift"
                  },
                  "docker-registry": {
                    "enabled": false
                  },
                  "docs": {
                    "tag": "5.1.52-g45a7f49-ubi7"
                  },
                  "elasticsearch": {
                    "enabled": false
                  },
                  "filebeat": {
                    "enabled": false
                  },
                  "graph": {
                    "tag": "5.0.72-gc8917a5-ubi7"
                  },
                  "kibana": {
                    "enabled": false
                  },
                  "kong": {
                    "tag": "5.0.88-gea2a5d8-ubi7"
                  },
                  "logWriteAccess": {
                    "enabled": false
                  },
                  "logs": {
                    "env": {
                      "ELASTICSEARCH_HOST": "ecd7bc77797941978322593d049dfd8d.us-east-1.aws.found.io:9243",
                      "ELASTICSEARCH_PASS": "U48HPRNgPdR8TQ368E02lFuE",
                      "ELASTICSEARCH_PROTO": "https",
                      "ELASTICSEARCH_USER": "elastic"
                    },
                    "tag": "5.0.65-gc7ab2b2-ubi7"
                  },
                  "metrics": {
                    "tag": "5.0.77-g02efa69-ubi7"
                  },
                  "minio": {
                    "enabled": false
                  },
                  "models": {
                    "tag": "5.0.52-g880e03e-ubi7"
                  },
                  "mongodb-replicaset": {
                    "enabled": false
                  },
                  "nginx-ingress": {
                    "enabled": false
                  },
                  "postgresql": {
                    "enabled": false
                  },
                  "processorgateway": {
                    "tag": "5.0.175-g36c3ad1-ubi7"
                  },
                  "rabbitmq": {
                    "enabled": false,
                    "persistence": {
                      "enabled": false
                    },
                    "volumePermissions": {
                      "enabled": false
                    }
                  },
                  "redis": {
                    "enabled": false
                  },
                  "sessions": {
                    "tag": "5.0.35-gf1bf851-ubi7"
                  }
                }
              }
            ]
          capabilities: Basic Install
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Cortex5 app cluster.
            displayName: Cortex5
            kind: Cortex5
            name: cortex5s.charts.helm.k8s.io
            version: v1alpha1
        description: Placeholder description
        displayName: Cortex Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: CognitiveScale
        version: 0.1.2
      name: alpha
    defaultChannel: alpha
    packageName: cortex-operator
    provider:
      name: CognitiveScale
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Citrix
      provider-url: ""
    name: citrix-cpx-istio-sidecar-injector-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/citrix-cpx-istio-sidecar-injector-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: citrix-cpx-istio-sidecar-injector-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"Citrixcpxistiosidecarinjector","metadata":{"name":"sidecar-inject"},"spec":{"cpxProxy":{"EULA":false,"cpxSidecarMode":true,"image":"quay.io/citrix/citrix-k8s-cpx-ingress","imagePullPolicy":"IfNotPresent","licenseServerPort":27000,"mgmtHttpPort":10080,"mgmtHttpsPort":10443,"tag":"13.0-41.28"},"istioAdaptor":{"ADMIP":null,"image":"quay.io/citrix/citrix-istio-adaptor","imagePullPolicy":"IfNotPresent","netscalerUrl":"http://127.0.0.1","proxyType":"sidecar","secureConnect":true,"tag":"1.0.1"},"istioPilot":{"SAN":"spiffe://cluster.local/ns/istio-system/sa/istio-pilot-service-account","insecureGrpcPort":15010,"name":"istio-pilot","namespace":"istio-system","secureGrpcPort":15011},"sidecarWebHook":{"imagePullPolicy":"IfNotPresent","webhookImage":"gcr.io/istio-release/sidecar_injector","webhookImageVersion":"1.0.0"},"webhook":{"injectionLabelName":"cpx-injection"}}}]'
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.connect.redhat.com/citrix/citrixadcsidecar:1.0.0
          createdAt: "2019-12-04 14:16:17"
          description: In Istio service mesh, a sidecar proxy runs alongside application
            pods and it intercepts and manages incoming and outgoing traffic for applications.
            Citrix ADC CPX can be deployed as the sidecar proxy in application pods.
            A sidecar proxy applies the configured routing policies or rules to the
            ingress and egress traffic from the pod.
          repository: https://github.com/citrix/citrix-istio-adaptor
          support: citrix-cpx-istio-sidecar-injector-operator
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: This is CRD for CPX Istio Sidecar Injector Operator
            displayName: Citrix ADC CPX Istio Sidecar Injector Operator
            kind: Citrixcpxistiosidecarinjector
            name: citrixcpxistiosidecarinjectors.charts.helm.k8s.io
            version: v1alpha1
        description: In Istio service mesh, a sidecar proxy runs alongside application
          pods and it intercepts and manages incoming and outgoing traffic for applications.
          Citrix ADC CPX can be deployed as the sidecar proxy in application pods.
          A sidecar proxy applies the configured routing policies or rules to the
          ingress and egress traffic from the pod.
        displayName: Citrix ADC CPX Istio Sidecar Injector Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Citrix
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: citrix-cpx-istio-sidecar-injector-operator
    provider:
      name: Citrix
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: servicemeshoperator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/servicemeshoperator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: servicemeshoperator.v1.0.7
      currentCSVDesc:
        annotations:
          alm-examples: "[\n  {\n    \"apiVersion\": \"maistra.io/v1\",\n    \"kind\":
            \"ServiceMeshControlPlane\",\n    \"metadata\": {\n      \"name\": \"basic-install\"\n
            \   },\n    \"spec\": {\n      \"istio\": {\n        \"gateways\": {\n
            \         \"istio-egressgateway\": {\n            \"autoscaleEnabled\":
            false\n          },\n          \"istio-ingressgateway\": {\n            \"autoscaleEnabled\":
            false\n          }\n        },\n        \"mixer\": {\n          \"policy\":
            {\n            \"autoscaleEnabled\": false\n          },\n          \"telemetry\":
            {\n            \"autoscaleEnabled\": false\n          }\n        },\n
            \       \"pilot\": {\n          \"autoscaleEnabled\": false,\n          \"traceSampling\":
            100.0\n        },\n        \"kiali\": {\n          \"enabled\": true\n
            \       },\n        \"grafana\": {\n          \"enabled\": true\n        },\n
            \       \"tracing\": {\n          \"enabled\": true,\n          \"jaeger\":
            {\n            \"template\": \"all-in-one\"\n          }\n        }\n
            \     }\n    }\n  },\n  {\n    \"apiVersion\": \"maistra.io/v1\",\n    \"kind\":
            \"ServiceMeshMemberRoll\",\n    \"metadata\": {\n      \"name\": \"default\"\n
            \   },\n    \"spec\": {\n      \"members\": [\n        \"your-project\",\n
            \       \"another-of-your-projects\" \n      ]\n    }\n  }\n]"
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Integration & Delivery
          certified: "false"
          containerImage: registry.redhat.io/openshift-service-mesh/istio-rhel8-operator:1.0.7
          createdAt: 2020-02-05T23:37:22UTC
          description: The OpenShift Service Mesh Operator enables you to install,
            configure, and manage an instance of Red Hat OpenShift Service Mesh. OpenShift
            Service Mesh is based on the open source Istio project.
          repository: https://github.com/maistra/istio-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A list of namespaces in Service Mesh
            displayName: Istio Service Mesh Member Roll
            kind: ServiceMeshMemberRoll
            name: servicemeshmemberrolls.maistra.io
            version: v1
          - description: An Istio control plane installation
            displayName: Istio Service Mesh Control Plane
            kind: ServiceMeshControlPlane
            name: servicemeshcontrolplanes.maistra.io
            version: v1
          required:
          - description: A configuration file for a Kiali installation.
            displayName: Kiali
            kind: Kiali
            name: kialis.kiali.io
            version: v1alpha1
          - description: A configuration file for a Jaeger installation.
            displayName: Jaeger
            kind: Jaeger
            name: jaegers.jaegertracing.io
            version: v1
        description: 'Red Hat OpenShift Service Mesh is a platform that provides behavioral
          insight and operational control over the service mesh, providing a uniform
          way to connect, secure, and monitor microservice applications. '
        displayName: Red Hat OpenShift Service Mesh
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.0.7
      name: "1.0"
    defaultChannel: "1.0"
    packageName: servicemeshoperator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: IBM
      provider-url: ""
    name: composable-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/composable-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: composable-operator.v0.1.3
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion": "ibmcloud.ibm.com/v1alpha1", "kind": "Composable",
            "metadata": {"name": "comp"}, "spec": {"template": {"apiVersion": "ibmcloud.ibm.com/v1alpha1",
            "kind": "Service"}}}]'
          capabilities: Basic Install
          categories: Cloud Provider
          certified: "false"
          containerImage: cloudoperators/composable-controller:0.1.3
          createdAt: "2019-11-16T20:39:31Z"
          description: Operator that can wrap any resource to make it dynamically
            configurable
          repository: https://github.com/IBM/composable
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Overlay operator that allows its underlying object to be
              dynamically configurable
            displayName: Composable
            kind: Composable
            name: composables.ibmcloud.ibm.com
            version: v1alpha1
        description: "Kubernetes object specifications often require constant values
          for their fields. When deploying an entire application with many different
          resources, this limitation often results in the need for staged deployments,
          because some resources have to be deployed first in order to determine what
          data to provide for the specifications of dependent resources. This undermines
          the declarative nature of Kubernetes object specification and requires workflows,
          manual step-by-step instructions and/or brittle automated scripts for the
          deployment of applications as a whole.\nThe Composable operator alleviates
          this problem by wrapping any resource (native Kubernetes or CRD instance)
          and allowing it to be specified with references to fields of other objects.
          These references are resolved dynamically by the Compsable controller when
          the data becomes available. This allows the yaml for the entire application
          to be deployed at once regardless of dependencies, and leverages Kubernetes
          native mechanisms to stage the deployment of different resources. \nFor
          example, consider a Knative KafkaSource resource:\n\n    apiVersion: sources.eventing.knative.dev/v1alpha1\n
          \   kind: KafkaSource\n    metadata:\n      name: kafka-source\n    spec:\n
          \     consumerGroup: knative-group\n      bootstrapServers: my-cluster-kafka-bootstrap.kafka:9092,my-cluster0-kafka-bootstrap.kafka:9093\n
          \     topics: knative-demo-topic\n    sink:\n      apiVersion: serving.knative.dev/v1\n
          \     kind: Service\n      name: event-display\n      \nThe KafkaSource
          resource requires a field bootstrapServers whose value can only be known
          if a Kafka service has already been deployed successfully. So one must first
          deploy Kafka, obtain this data, then create the above yaml and deploy it.\n
          \nWith the Composable operator, this yaml can be deployed at the same time
          as the rest of the application, as follows:\n\n\n    apiVersion: ibmcloud.ibm.com/v1alpha1\n
          \   kind: Composable\n    metadata:\n      name: kafka-source\n    spec:\n
          \     template:\n        apiVersion: sources.eventing.knative.dev/v1alpha1\n
          \       kind: KafkaSource\n        metadata:\n          name: kafka-source\n
          \       spec:\n          consumerGroup: knative-group\n          bootstrapServers:\n
          \           getValueFrom:\n              kind: Secret\n              name:
          my-kafka-binding\n              path: '{.data.kafka_brokers_sasl}'\n              format-transformers:\n
          \             - \"Base64ToString\" \n              - \"JsonToObject\"\n
          \             - \"ArrayToCSString\"\n          topics: knative-demo-topic\n
          \         sink:\n            apiVersion: serving.knative.dev/v1\n            kind:
          Service\n            name: event-display\n            \nWith Composable
          as the wrapper object, the field `bootstrapServers` can be specified with
          a reference `getValueFrom` to another object, in this case a secret named
          `my-kafka-binding` that contains binding information for the Kafka service
          (created by a different operator). When the Composable object is deployed,
          the Composable controller tries to resolve this value and keeps trying if
          the secret has not been created yet. Once the secret is created and the
          data becomes available, the Composable operator then deploys the underlying
          object.\n \nOften there is data formatting mismatch between a source and
          a referencing field, so Composable also provides a series of handy data
          transformers that can be piped together in order to obtain the correct format.
          In this case, the base64 secret data is first decoded  to obtain a Json
          string, which is then parsed producing an array of urls. Finally this array
          is transformed into a comma-separated string.\n \nThe Composable operator
          allows all yamls of an application to be deployed at once, in one step,
          by supporting cross-resource references that are resolved dynamically, and
          leverages native Kubernetes mechanisms to stage the deployment of a collection
          of resources.\n \nThe Composable Operator is currently in preview. It will
          get updated as we release new versions of the [upstream repository](https://github.com/IBM/composable).\n##
          Requirements\nThe operator can be installed on any OLM-enabled Kubernetes
          cluster with version >= 1.11. \n## Using the Composable Operator\nFor additional
          configuration options, samples and more information on using the operator,
          consult  the [Composable Operator documentation](https://github.com/IBM/composable).\n"
        displayName: Composable
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: IBM
        version: 0.1.3
      name: alpha
    defaultChannel: alpha
    packageName: composable-operator
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: apicast-community-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/apicast-community-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: apicast-community-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIcast","metadata":{"name":"example-apicast"},"spec":{"adminPortalCredentialsRef":{"name":"mysecretname"}}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIcast","metadata":{"name":"example-apicast"},"spec":{"embeddedConfigurationSecretRef":{"name":"mysecretname"}}}]'
          capabilities: Full Lifecycle
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/3scale/apicast-operator:v0.1.0
          createdAt: "2019-10-27T22:40:00Z"
          description: APIcast is an API gateway built on top of NGINX. It is part
            of the Red Hat 3scale API Management Platform
          repository: https://github.com/3scale/apicast-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: APIcast is an API gateway
            displayName: APIcast
            kind: APIcast
            name: apicasts.apps.3scale.net
            version: v1alpha1
        description: |
          The APIcast Operator creates and maintains the Red Hat 3scale API Gateway in several deployment configurations.

          APIcast is an API gateway built on top of [NGINX](https://www.nginx.com/). It is part of the [Red Hat 3scale API Management Platform](https://www.redhat.com/en/technologies/jboss-middleware/3scale).

          ### Supported Features
          * **Installer** A way to install a APIcast gateway.on

          ### Upgrading your installation
          The APIcast Operator understands how to run and upgrade between a set of APIcast versions.
          See [the upgrade guide](https://github.com/3scale/apicast-operator) for more information.

          ### Documentation
          Documentation can be found on our [website](https://github.com/3scale/apicast-operator).

          ### Getting help
          If you encounter any issues while using operator, you can create an issue on our [website](https://github.com/3scale/apicast-operator) for bugs, enhancements, or other requests.

          ### Contributing
          You can contribute by:

          * Raising any issues you find using APIcast Operator
          * Fixing issues by opening [Pull Requests](https://github.com/3scale/apicast-operator/pulls)
          * Improving [documentation](https://github.com/3scale/apicast-operator)
          * Talking about APIcast Operator

          All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/apicast-operator/issues).

          ### License
          APIcast Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/apicast-operator/blob/master/LICENSE)
        displayName: APIcast
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.1.0
      name: alpha
    - currentCSV: apicast-community-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIcast","metadata":{"name":"example-apicast"},"spec":{"adminPortalCredentialsRef":{"name":"mysecretname"}}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIcast","metadata":{"name":"example-apicast"},"spec":{"embeddedConfigurationSecretRef":{"name":"mysecretname"}}}]'
          capabilities: Full Lifecycle
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/3scale/apicast-operator:v0.1.0
          createdAt: "2019-10-27T22:40:00Z"
          description: APIcast is an API gateway built on top of NGINX. It is part
            of the Red Hat 3scale API Management Platform
          repository: https://github.com/3scale/apicast-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: APIcast is an API gateway
            displayName: APIcast
            kind: APIcast
            name: apicasts.apps.3scale.net
            version: v1alpha1
        description: |
          The APIcast Operator creates and maintains the Red Hat 3scale API Gateway in several deployment configurations.

          APIcast is an API gateway built on top of [NGINX](https://www.nginx.com/). It is part of the [Red Hat 3scale API Management Platform](https://www.redhat.com/en/technologies/jboss-middleware/3scale).

          ### Supported Features
          * **Installer** A way to install a APIcast gateway.on

          ### Upgrading your installation
          The APIcast Operator understands how to run and upgrade between a set of APIcast versions.
          See [the upgrade guide](https://github.com/3scale/apicast-operator) for more information.

          ### Documentation
          Documentation can be found on our [website](https://github.com/3scale/apicast-operator).

          ### Getting help
          If you encounter any issues while using operator, you can create an issue on our [website](https://github.com/3scale/apicast-operator) for bugs, enhancements, or other requests.

          ### Contributing
          You can contribute by:

          * Raising any issues you find using APIcast Operator
          * Fixing issues by opening [Pull Requests](https://github.com/3scale/apicast-operator/pulls)
          * Improving [documentation](https://github.com/3scale/apicast-operator)
          * Talking about APIcast Operator

          All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/apicast-operator/issues).

          ### License
          APIcast Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/apicast-operator/blob/master/LICENSE)
        displayName: APIcast
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.1.0
      name: stable
    defaultChannel: stable
    packageName: apicast-community-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Appsody
      provider-url: ""
    name: appsody-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/appsody-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: appsody-operator.v0.3.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"appsody.dev/v1beta1","kind":"AppsodyApplication","metadata":{"name":"example-appsodyapplication"},"spec":{"applicationImage":"APPLICATION_IMAGE","stack":"java-microprofile"}}]'
          capabilities: Seamless Upgrades
          categories: Application Runtime
          certified: "true"
          containerImage: registry.connect.redhat.com/ibm/appsody-application-operator:0.3.0
          createdAt: "2019-12-02 09:00:00"
          description: Deploys Appsody based applications
          repository: https://github.com/appsody/appsody-operator
          support: Appsody
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Describes application deployment
            displayName: Appsody Application
            kind: AppsodyApplication
            name: appsodyapplications.appsody.dev
            version: v1beta1
        description: |
          The Appsody Operator allows easy deployment of applications developed with [Appsody](https://appsody.dev) to Kubernetes environments. The operator provides the following key features:

          #### Routing

          Expose your application to external users via a single toggle.

          #### High Availability

          Run multiple instances of your application for high availability. Either specify a static number of replicas or easily configure auto scaling to create (and delete) instances based on resource consumption.

          #### Persistence

          Enable persistence for your application by specifying storage requirements.

          #### Service Binding

          Easily bind to available services in your cluster.

          #### Knative

          Deploy your serverless application on [Knative](https://knative.dev) using a single toggle.

          #### Kubernetes Application Navigator (kAppNav)

          Automatically configures the Kubernetes resources for integration with [kAppNav](https://kappnav.io/).

          See our [**documentation**](https://github.com/appsody/appsody-operator/tree/master/doc/) for more information.
        displayName: Appsody Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Appsody
        version: 0.3.0
      name: beta
    defaultChannel: beta
    packageName: appsody-operator-certified
    provider:
      name: Appsody
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Argo CD Community
      provider-url: ""
    name: argocd-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/argocd-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: argocd-operator.v0.0.4
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"argoproj.io/v1alpha1","kind":"ArgoCD","metadata":{"name":"example-argocd"},"spec":{}},{"apiVersion":"argoproj.io/v1alpha1","kind":"ArgoCDExport","metadata":{"name":"example-argocdexport"},"spec":{"argocd":"example-argocd"}},{"apiVersion":"argoproj.io/v1alpha1","kind":"Application","metadata":{"name":"guestbook"},"spec":{"destination":{"namespace":"argocd","server":"https://kubernetes.default.svc"},"project":"default","source":{"path":"guestbook","repoURL":"https://github.com/argoproj/argocd-example-apps.git","targetRevision":"HEAD"}}},{"apiVersion":"argoproj.io/v1alpha1","kind":"AppProject","metadata":{"name":"example-project"},"spec":{"sourceRepos":
            ["*"]}}]'
          capabilities: Deep Insights
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/jmckind/argocd-operator:v0.0.4
          createdAt: "2020-01-22 19:49:32"
          description: Argo CD is a declarative, GitOps continuous delivery tool for
            Kubernetes.
          repository: https://github.com/argoproj-labs/argocd-operator
          support: Argo CD
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Application is a group of Kubernetes resources as defined
              by a manifest.
            displayName: Application
            kind: Application
            name: applications.argoproj.io
            version: v1alpha1
          - description: An AppProject is a logical grouping of Argo CD Applications.
            displayName: AppProject
            kind: AppProject
            name: appprojects.argoproj.io
            version: v1alpha1
          - description: ArgoCDExport describes the desired state for the export of
              a given Argo CD deployment.
            displayName: ArgoCDExport
            kind: ArgoCDExport
            name: argocdexports.argoproj.io
            version: v1alpha1
          - description: ArgoCD is the representation of an Argo CD deployment.
            displayName: ArgoCD
            kind: ArgoCD
            name: argocds.argoproj.io
            version: v1alpha1
        description: "## Overview\n\nThe Argo CD Operator is intended to manage the
          full lifecycle for Argo CD and it's components. The operator's goal \nis
          to automate the tasks required when operating Argo CD. Beyond installation,
          the operator attempts to automate \nthe process of upgrading, backing up
          and restoring as needed and remove the human as much as possible.\n\nIn
          addition, the operator aims to provide deep insights into the Argo CD environment
          by configuring Prometheus and \nGrafana to expose, aggregate and visualize
          the metrics already exported by Argo CD.\n\nThe operator aims to provide
          the following.\n\n* Easy configuration and installation of the Argo CD components
          with sane defaults to get up and running quickly.\n* Provide seamless upgrades
          to the Argo CD components.\n* Ablity to back up and restore an Argo CD deployment
          from a point in time.\n* Expose and aggregate the metrics for Argo CD and
          the operator itself using Prometheus and Grafana.\n* Autoscale the Argo
          CD components as necessary to handle increased load.\n\n## Usage\n\nDeploy
          a basic Argo CD cluster by creating a new ArgoCD resource in the namespace
          where the operator is installed.\n\n```\napiVersion: argoproj.io/v1alpha1\nkind:
          ArgoCD\nmetadata:\n  name: example-argocd\nspec: {}\n```\n\n## Backup\n\nBackup
          the cluster above by creating a new ArgoCDExport resource in the namespace
          where the operator is installed.\n\n```\napiVersion: argoproj.io/v1alpha1\nkind:
          ArgoCDExport\nmetadata:\n  name: example-argocdexport\nspec:\n  argocd:
          example-argocd\n```\n\nSee the documentation and examples at the [official
          github repository](https://github.com/argoproj-labs/argocd-operator) \nfor
          more information.\n"
        displayName: Argo CD
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Argo CD Community
        version: 0.0.4
      name: alpha
    defaultChannel: alpha
    packageName: argocd-operator
    provider:
      name: Argo CD Community
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Percona
      provider-url: ""
    name: percona-xtradb-cluster-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/percona-xtradb-cluster-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: percona-xtradb-cluster-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"kind":"PerconaXtraDBCluster","spec":{"secretsName":"my-cluster-secrets","sslSecretName":"my-cluster-ssl","pmm":{"enabled":false},"pxc":{"allowUnsafeConfigurations":
            true,"size":3,"volumeSpec":{"persistentVolumeClaim":{"resources":{"requests":{"storage":"6Gi"}}}},"image":"registry.connect.redhat.com/percona/percona-xtradb-cluster-operator-containers:1.0.0-pxc"},"backup":{"storages":{"fs-pvc":{"volume":{"persistentVolumeClaim":{"resources":{"requests":{"storage":"6Gi"}}}},"type":"filesystem"}},"image":"registry.connect.redhat.com/percona/percona-xtradb-cluster-operator-containers:1.0.0-backup","schedule":[{"keep":5,"storageName":"fs-pvc","name":"sat-night-backup","schedule":"0
            0 * * 6"}]},"sslInternalSecretName":"my-cluster-ssl-internal","proxysql":{"image":"registry.connect.redhat.com/percona/percona-xtradb-cluster-operator-containers:1.0.0-proxysql","enabled":true,"volumeSpec":{"persistentVolumeClaim":{"resources":{"requests":{"storage":"1Gi"}}}},"size":1}},"apiVersion":"pxc.percona.com/v1","metadata":{"name":"cluster1","finalizers":["delete-pxc-pods-in-order"]}},{"apiVersion":"pxc.percona.com/v1","kind":"PerconaXtraDBClusterRestore","metadata":{"name":"restore1"},"spec":{"backupName":"backup1","pxcCluster":"cluster1"}},{"apiVersion":"pxc.percona.com/v1","kind":"PerconaXtraDBClusterBackup","metadata":{"name":"backup1"},"spec":{"pxcCluster":"cluster1","storageName":"fs-pvc"}},{"apiVersion":"pxc.percona.com/v1alpha1","kind":"PerconaXtraDBBackup","metadata":{"name":"backup1"},"spec":{"pxcCluster":"cluster1","storageName":"s3-us-west"}}]'
          capabilities: Full Lifecycle
          categories: Database
          certified: "true"
          containerImage: registry.connect.redhat.com/percona/percona-xtradb-cluster-operator:v1.0.0
          createdAt: "2019-05-13 21:36:39"
          description: Percona XtraDB Cluster Operator manages the lifecycle of Percona
            XtraDB cluster instances.
          repository: https://github.com/percona/percona-xtradb-cluster-operator
          support: Percona
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Instance of a Percona XtraDB Cluster
            displayName: PerconaXtraDBCluster
            kind: PerconaXtraDBCluster
            name: perconaxtradbclusters.pxc.percona.com
            version: v1
          - description: Instance of a Percona XtraDB Cluster Backup
            displayName: PerconaXtraDBClusterBackup
            kind: PerconaXtraDBClusterBackup
            name: perconaxtradbclusterbackups.pxc.percona.com
            version: v1
          - description: Instance of a Percona XtraDB Cluster Restore
            displayName: PerconaXtraDBClusterRestore
            kind: PerconaXtraDBClusterRestore
            name: perconaxtradbclusterrestores.pxc.percona.com
            version: v1
          - description: (Legacy) Instance of a Percona XtraDB Cluster Backup
            displayName: PerconaXtraDBBackup
            kind: PerconaXtraDBBackup
            name: perconaxtradbbackups.pxc.percona.com
            version: v1alpha1
        description: |2-

          ## Percona is Cloud Native

          The Percona Kubernetes Operator for Percona XtraDB Cluster automates the creation, alteration, or deletion of nodes in your cluster environment. It can be used to instantiate a new database cluster or to scale an existing database cluster. The Operator contains all necessary Kubernetes settings to provide a proper and consistent Percona XtraDB Cluster instance.

          Consult the [documentation](https://percona.github.io/percona-xtradb-cluster-operator/) on the Percona Kubernetes Operator for Percona XtraDB Cluster for complete details on capabilities and options.

          ### Supported Features

          * **Scale Your Cluster** â€“ change the `size` parameter to [add or remove members](https://percona.github.io/percona-xtradb-cluster-operator/install/scaling) of the cluster. Three is the minimum recommended size for a functioning cluster.

          * **Manage Your Users** â€“ [add, remove, or change](https://percona.github.io/percona-xtradb-cluster-operator/configure/users) the privileges of database users

          * **Automate Your Backups** â€“ [configure cluster backups](https://percona.github.io/percona-xtradb-cluster-operator/configure/operator) to run on a scheduled basis. Backups are stored on a persistent volume.

          ### Common Configurations

          * **Set Resource Limits** - set limitation on requests to CPU and memory resources.

          * **Customize Storage** - set the desired Storage Class and Access Mode for your database cluster data.

          * **Control Scheduling** - define how your PXC Pods are scheduled onto the K8S cluster with tolerations, pod disruption budgets, node selector and affinity settings.

          ### Before You Start

          Add the PXC user `Secret` to Kubernetes. User information must be placed in the data section of the `secrets.yaml`
          file with Base64-encoded logins and passwords for the user accounts.

          Below is a sample `secrets.yaml` file for the correct formatting.

          apiVersion: v1
          kind: Secret
          metadata:
            name: my-cluster-secrets
          type: Opaque
          data:
            root: cm9vdF9wYXNzd29yZA==
            xtrabackup: YmFja3VwX3Bhc3N3b3Jk
            monitor: bW9uaXRvcg==
            clustercheck: Y2x1c3RlcmNoZWNrcGFzc3dvcmQ=
            proxyadmin: YWRtaW5fcGFzc3dvcmQ=
            pmmserver: c3VwYXxefHBheno=
        displayName: Percona XtraDB Cluster Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Percona
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: percona-xtradb-cluster-operator-certified
    provider:
      name: Percona
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Joget, Inc
      provider-url: ""
    name: joget-openshift-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/joget-openshift-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: joget-openshift-operator.v0.0.4
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"app.joget.com/v1alpha1","kind":"Joget","metadata":{"name":"example-joget"},"spec":{"size":1}}]'
          capabilities: Seamless Upgrades
          categories: Application Runtime,Developer Tools
          certified: "true"
          containerImage: registry.connect.redhat.com/joget/joget-v6-operator
          createdAt: "2019-07-23 12:59:59"
          description: No-code/low-code application platform to visually build, run
            and maintain apps
          repository: github.com/jogetworkflow/jw-community
          support: Joget, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Joget on JBoss EAP 7
            displayName: Joget on JBoss EAP 7
            kind: Joget
            name: joget.app.joget.com
            version: v1alpha1
        description: "Joget is an open source no-code / low-code application platform
          that combines the best of Rapid Application Development, Business Process
          Automation and Workflow Management. \nJoget empowers business users, non-coders
          or coders with a single platform to easily build, deliver, monitor and maintain
          enterprise applications.\n\nThis operator installs a Joget cluster running
          on JBoss EAP 7.\n\n### Features\n* Build full-fledged apps e.g. CRM, HR,
          Healthcare, etc\n* Drag and drop forms, lists, UI\n* Add workflow to automate
          processes\n* Extend via plugins\n* Apps are mobile optimized and cloud ready\n*
          Download ready-made apps from the Joget Marketplace\n\n### Before You Start\nDeploy
          a [MySQL](https://docs.openshift.com/online/pro/using_images/db_images/mysql.html)
          or [MariaDB](https://docs.openshift.com/online/pro/using_images/db_images/mariadb.html)
          database.\n\n### Post Deployment\nAccess the service URL and complete the
          one-time [Database Setup](https://dev.joget.org/community/display/KBv6/Setting+Up+Database)\n\n###
          More Information\nMore information about Joget on JBoss EAP 7 is available
          on the [OpenShift Blog](https://blog.openshift.com/how-to-automatically-scale-low-code-apps-with-joget-and-jboss-eap-on-openshift/)\n"
        displayName: Joget Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Joget, Inc
        version: 0.0.4
      name: alpha
    defaultChannel: alpha
    packageName: joget-openshift-operator
    provider:
      name: Joget, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Gigaspaces
      provider-url: ""
    name: insightedge-enterprise-operator2
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/insightedge-enterprise-operator2
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: insightedge-enterprise-operator.v15.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "insightedge.gigaspaces.com/v1alpha1",
                "kind": "Insightedge",
                "metadata": {
                  "name": "example-insightedge"
                },
                "spec": {
                  "manager": {
                    "antiAffinity": {
                      "enabled": false
                    },
                    "ha": false,
                    "hs": {
                      "lus": 4174,
                      "zkClient": 2181,
                      "zkLeaderElection": 3888,
                      "zkServer": 2888
                    },
                    "image": {
                      "pullPolicy": "IfNotPresent",
                      "repository": "registry.connect.redhat.com/gigaspaces/insightedge:V15.0.1"
                    },
                    "java": {
                      "heap": "limit-150Mi",
                      "options": null
                    },
                    "nameOverride": "insightedge-manager",
                    "resources": {
                      "limits": {
                        "memory": "300Mi"
                      }
                    },
                    "service": {
                      "api": {
                        "enabled": true,
                        "nodePort": null,
                        "port": 8090
                      },
                      "lrmi": {
                        "enabled": false,
                        "nodePort": null,
                        "port": 8200
                      },
                      "lus": {
                        "enabled": true,
                        "nodePort": null,
                        "port": 4174
                      },
                      "type": "LoadBalancer"
                    },
                    "terminationGracePeriodSeconds": 30
                  },
                  "pu": {
                    "antiAffinity": {
                      "enabled": false
                    },
                    "ha": null,
                    "image": {
                      "pullPolicy": "IfNotPresent",
                      "repository": "registry.connect.redhat.com/gigaspaces/insightedge:V15.0.1"
                    },
                    "instances": null,
                    "java": {
                      "heap": "limit-150Mi",
                      "options": null
                    },
                    "license": "tryme",
                    "livenessProbe": {
                      "enabled": false,
                      "failureThreshold": 3,
                      "initialDelaySeconds": 30,
                      "periodSeconds": 5
                    },
                    "manager": {
                      "discoveryTimeoutSeconds": 60,
                      "name": null,
                      "ports": {
                        "api": 8090
                      }
                    },
                    "memoryXtendVolume": {
                      "enabled": false,
                      "volumeClaimTemplate": {
                        "accessModes": "ReadWriteOnce",
                        "persistentVolumeReclaimPolicy": "Delete",
                        "storage": null,
                        "storageClassName": null
                      },
                      "volumeMount": {
                        "mountPath": "/opt/gigaspaces/work/memoryxtend",
                        "name": "mx-volume"
                      }
                    },
                    "nameOverride": "insightedge-pu",
                    "partitions": null,
                    "properties": null,
                    "readinessProbe": {
                      "enabled": false,
                      "failureThreshold": 3,
                      "initialDelaySeconds": 30,
                      "periodSeconds": 5
                    },
                    "resourceUrl": null,
                    "resources": {
                      "limits": {
                        "memory": "400Mi"
                      }
                    },
                    "schema": null,
                    "service": {
                      "lrmi": {
                        "enabled": false,
                        "initialNodePort": null,
                        "port": 8200
                      },
                      "type": "LoadBalancer"
                    },
                    "terminationGracePeriodSeconds": 30
                  },
                  "zeppelin": {
                    "image": {
                      "pullPolicy": "IfNotPresent",
                      "repository": "registry.connect.redhat.com/gigaspaces/insightedge:V15.0.1"
                    },
                    "nameOverride": "insightedge-zeppelin",
                    "service": {
                      "api": {
                        "nodePort": null,
                        "port": 9090
                      },
                      "type": "LoadBalancer"
                    }
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: AI/Machine Learning, Big Data, Database, Streaming & Messaging
          certified: "true"
          containerImage: registry.connect.redhat.com/gigaspaces/insightedge-operator:v15.0.1-04022020
          createdAt: "2020-02-04 14:59:59"
          description: |
            InsightEdge is an in-memory real-time analytics platform for instant insights to action. It's an always-on platform for mission-critical applications across cloud, on-premise or hybrid. InsightEdge operationalizes machine learning and transactional processing, at scale; analyzing data as it's born, enriching it with historical context, for instant insights.
          support: Gigaspaces Ltd
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: |
              InsightEdge is an in-memory real-time analytics platform for instant insights to action. It's an always-on platform for mission-critical applications across cloud, on-premise or hybrid. InsightEdge operationalizes machine learning and transactional processing, at scale; analyzing data as it's born, enriching it with historical context, for instant insights.
            displayName: Insightedge Enterprise Operator
            kind: Insightedge
            name: insightedges.insightedge.gigaspaces.com
            version: v1alpha1
        description: |
          InsightEdge is an in-memory real-time analytics platform for instant insights to action. It's an always-on platform for mission-critical applications across cloud, on-premise or hybrid. InsightEdge operationalizes machine learning and transactional processing, at scale; analyzing data as it's born, enriching it with historical context, for instant insights.
        displayName: Insightedge Enterprise Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Gigaspaces
        version: 15.0.1
      name: alpha
    defaultChannel: alpha
    packageName: insightedge-enterprise-operator2
    provider:
      name: Gigaspaces
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: StorageOS, Inc
      provider-url: ""
    name: storageos
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/storageos
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: storageosoperator.v1.5.3
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "storageos.com/v1",
                "kind": "StorageOSCluster",
                "metadata": {
                  "name": "example-storageos",
                  "namespace": "openshift-operators"
                },
                "spec": {
                  "namespace": "kube-system",
                  "secretRefName": "storageos-api",
                  "secretRefNamespace": "openshift-operators",
                  "k8sDistro": "openshift",
                  "csi": {
                    "enable": true,
                    "deploymentStrategy": "deployment"
                  }
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "Job",
                "metadata": {
                  "name": "example-job",
                  "namespace": "default"
                },
                "spec": {
                  "image": "registry.connect.redhat.com/storageos/cluster-operator:latest",
                  "args": ["/var/lib/storageos"],
                  "mountPath": "/var/lib",
                  "hostPath": "/var/lib",
                  "completionWord": "done"
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "StorageOSUpgrade",
                "metadata": {
                  "name": "example-upgrade",
                  "namespace": "default"
                },
                "spec": {
                  "newImage": "registry.connect.redhat.com/storageos/node:latest"
                }
              },
              {
                "apiVersion": "storageos.com/v1",
                "kind": "NFSServer",
                "metadata": {
                  "name": "example-nfsserver",
                  "namespace": "default"
                },
                "spec": {
                  "resources": {
                    "requests": {
                      "storage": "1Gi"
                    }
                  }
                }
              }
            ]
          capabilities: Deep Insights
          categories: Storage
          certified: "true"
          containerImage: registry.connect.redhat.com/storageos/cluster-operator:1.5.3
          createdAt: "2020-01-09T18:11:00Z"
          description: Cloud-native, persistent storage for containers.
          repository: https://github.com/storageos/cluster-operator
          support: StorageOS, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: StorageOS Cluster installs StorageOS in the cluster. It contains
              all the configuration for setting up a StorageOS cluster and also shows
              the status of the running StorageOS cluster.
            displayName: StorageOS Cluster
            kind: StorageOSCluster
            name: storageosclusters.storageos.com
            version: v1
          - description: StorageOS Job creates special pods that run on all the node
              and perform an administrative task. This could be used for cluster maintenance
              tasks.
            displayName: StorageOS Job
            kind: Job
            name: jobs.storageos.com
            version: v1
          - description: StorageOS Upgrade automatically upgrades an existing StorageOS
              cluster as per the upgrade configuration.
            displayName: StorageOS Upgrade
            kind: StorageOSUpgrade
            name: storageosupgrades.storageos.com
            version: v1
          - description: StorageOS NFS Server provides support for shared volumes.
              The StorageOS control plane will automatically create and manage NFS
              Server instances when a PersistentVolumeClaim requests a volume with
              AccessMode=ReadWriteMany.
            displayName: NFS Server
            kind: NFSServer
            name: nfsservers.storageos.com
            version: v1
        description: |
          StorageOS is a cloud native, software-defined storage platform that
          transforms commodity server or cloud based disk capacity into
          enterprise-class persistent storage for containers. StorageOS is ideal for
          deploying databases, message busses, and other mission-critical stateful
          solutions, where rapid recovery and fault tolerance are essential.

          The StorageOS Operator installs and manages StorageOS within a cluster.
          Cluster nodes may contribute local or attached disk-based storage into a
          distributed pool, which is then available to all cluster members via a
          global namespace.

          Volumes are available across the cluster so if a container gets moved to
          another node it has immediate access to re-attach its data. Data can be
          protected with synchronous replication. Compression, caching, and QoS are
          enabled by default, and all volumes are thinly-provisioned.

          No other hardware or software is required.

          StorageOS is free to use up to 50GB of presented storage, increasing to
          500GB after registration.  For additional capacity and support plans contact
          sales@storageos.com.

          ## Supported Features

          * **Rapid volume failover** - If a container gets re-scheduled to another
          node, any StorageOS volumes can be re-attached immediately, irrespective of
          where the data is located within the cluster.

          * **Data protection** - Data is replicated synchronously, up to 6 copies.

          * **Block checksums** - Each block is protected by a checksum which
          automatically detects any corruption of data in the underlying storage
          media.

          * **Thin provisioning** - Only consume the space you need in a storage pool.

          * **Data reduction** - Transparent inline data compression to reduce the
          amount of storage used in a backing store as well as reducing the network
          bandwidth requirements for replication.

          * **In-memory caching** - Speed up access to data even when accessed
          remotely.

          * **Quality of service** - Prioritize data traffic and address the â€œnoisy
          neighborsâ€ problem.

          * **Flexible configuration** - Use labels to automate data placement and
          enforce data policy such as replication. Ideal for compliance and infosec
          teams to enforce policies and rules while still enabling self-service
          storage by developers and DevOps teams.

          * **Access control** - Support multiple namespace â€“ individual users are
          permissioned to specific storage namespaces.

          * **Observability & instrumentation** - Log streams for observability and
          Prometheus support for instrumentation.

          * **Deployment flexibility** - Scale up or scale out storage based on
          application requirements.  Works with any infrastructure â€“ on-premises, VM,
          bare metal or cloud.

          * **Small footprint** - Lightweight container requires minimum 1 core with
          2GB of RAM.  Runs in user-space on any 64-bit Linux with no custom kernel
          modules.

          ## Prerequisites

          * Ensure port 5705 is open on the worker nodes.

          ## Required Parameters

          * `secretRefName` - the name of a secret that contains two keys for the
          `apiUsername` and `apiPassword` to be used as api credentials
          ([documentation](https://docs.storageos.com/docs/reference/cluster-operator/examples))
          * `secretRefNamespace` - the namespace where the api credentials secret is
          stored

          ## About StorageOS

          StorageOS gives users total control of their own storage environment â€“
          whether in the cloud or on-premises. Our customers take advantage of storage
          on any platform while maintaining full control of business requirements
          around availability, data mobility, performance, security, data residency
          compliance and business continuity.
        displayName: StorageOS
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: StorageOS, Inc
        version: 1.5.3
      name: stable
    defaultChannel: stable
    packageName: storageos
    provider:
      name: StorageOS, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: ArangoDB GmbH
      provider-url: ""
    name: kube-arangodb
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kube-arangodb
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kube-arangodb.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"database.arangodb.com/v1","kind":"ArangoDeployment","metadata":{"name":"arangodb"},"spec":{"image":"registry.connect.redhat.com/arangodb/enterprise:3.5.3-ubi"}}]'
          capabilities: Full Lifecycle
          categories: Database, Application Runtime
          certified: "true"
          containerImage: registry.connect.redhat.com/arangodb/kube-arangodb:0.4.2-ubi
          createdAt: "2019-11-30 22:40:00"
          description: ArangoDB Kubernetes Operator
          repository: https://github.com/arangodb/kube-arangodb
          support: redhat@arangodb.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Deployment of ArangoDB
            displayName: ArangoDeployment
            kind: ArangoDeployment
            name: arangodeployments.database.arangodb.com
            version: v1
          - description: Arango Backup Policy
            displayName: ArangoBackupPolicy
            kind: ArangoBackupPolicy
            name: arangobackuppolicies.backup.arangodb.com
            version: v1
          - description: Arango Backup
            displayName: ArangoBackup
            kind: ArangoBackup
            name: arangobackups.backup.arangodb.com
            version: v1
          - description: Arango Deployment Replication
            displayName: ArangoDeploymentReplication
            kind: ArangoDeploymentReplication
            name: arangodeploymentreplications.replication.database.arangodb.com
            version: v1
        description: |+
          **ArangoDB**

          Three major data models along with a full-text search and ranking engine in one open-source core.

          **One engine. One query language. Multiple models.**

          When youâ€™re building your application, ultimately what matters most is having the right data model available for the task at hand.

          By uniting graph, document, and key/value in a single core with the same query language, along with a full-text search and ranking engine, ArangoDB provides the flexibility to easily apply the data models you need.

          **A query language that feels like coding**

          As a declarative query language, those coming from SQL will feel right at home with AQL.

          With AQL, you can use JOINs, traversals, filters, geo-spatial operations, and aggregations & combine them in a single query.



        displayName: kube-arangodb
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: ArangoDB GmbH
        version: 0.1.0
      name: Enterprise
    defaultChannel: Enterprise
    packageName: kube-arangodb
    provider:
      name: ArangoDB GmbH
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: AppDynamics LLC
      provider-url: ""
    name: appdynamics-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/appdynamics-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: appdynamics-operator.v0.4.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "appdynamics.com/v1alpha1",
                "kind": "Adam",
                "metadata": {
                  "name": "example-adam",
                  "namespace": "appdynamics"
                },
                "spec": {
                "controllerUrl": "https://saas.appdynamics.com"
                }
              },
              {
                "apiVersion": "appdynamics.com/v1alpha1",
                "kind": "Clusteragent",
                "metadata": {
                  "name": "k8s-cluster-agent",
                  "namespace": "appdynamics"
                },
                "spec": {
                  "account": "customer1",
                  "appName": "Cluster1",
                  "controllerUrl": "https://saas.appdynamics.com",
                  "stdoutLogging": "true"
                }
              },
              {
                "apiVersion": "appdynamics.com/v1alpha1",
                "kind": "InfraViz",
                "metadata": {
                  "name": "appd-infraviz",
                  "namespace": "appdynamics"
                },
                "spec": {
                  "account": "customer1",
                  "controllerUrl": "https://saas.appdynamics.com",
                  "enableDockerViz": "false",
                  "enableMasters": true,
                  "globalAccount": "customer1_12345",
                  "netVizPort": 3892,
                  "stdoutLogging": true
                }
              }
            ]
          capabilities: Basic Install
          categories: Monitoring
          certified: "false"
          containerImage: registry.connect.redhat.com/appdynamics/cluster-agent-operator
          createdAt: ""
          description: End to end monitoring of applications on Kubernetes and OpenShift
            clusters with AppDynamics.
          support: help@appdynamics.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Adam is the Schema for the adams API
            displayName: Adam
            kind: Adam
            name: adams.appdynamics.com
            version: v1alpha1
          - description: Clusteragent is the Schema for the clusteragents API
            displayName: Clusteragent
            kind: Clusteragent
            name: clusteragents.appdynamics.com
            version: v1alpha1
          - description: InfraViz is the Schema for the infravizs API
            displayName: InfraViz
            kind: InfraViz
            name: infravizs.appdynamics.com
            version: v1alpha1
        description: AppDynamics Operator manages deployments and upgrades of the
          AppDynamics agents for monitoring applications on Kubernetes and OpenShift.
        displayName: Appdynamics Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: AppDynamics LLC
        version: 0.4.0
      name: alpha
    defaultChannel: alpha
    packageName: appdynamics-operator
    provider:
      name: AppDynamics LLC
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Anchore Inc.
      provider-url: ""
    name: anchore-engine
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/anchore-engine
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: anchore-engine-operator.v0.1.4
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "anchore.com/v1alpha1",
                "kind": "AnchoreEngine",
                "metadata": {
                  "name": "example-anchoreengine"
                },
                "spec": {
                  "anchore-feeds-db": {
                    "externalEndpoint": null,
                    "extraEnv": [
                      {
                        "name": "POSTGRESQL_USER",
                        "value": "anchoreengine"
                      },
                      {
                        "name": "POSTGRESQL_PASSWORD",
                        "value": "anchore-postgres,123"
                      },
                      {
                        "name": "POSTGRESQL_DATABASE",
                        "value": "anchore-feeds"
                      },
                      {
                        "name": "PGUSER",
                        "value": "postgres"
                      },
                      {
                        "name": "LD_LIBRARY_PATH",
                        "value": "/opt/rh/rh-postgresql96/root/usr/lib64"
                      },
                      {
                        "name": "PATH",
                        "value": "/opt/rh/rh-postgresql96/root/usr/bin:/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
                      }
                    ],
                    "image": "registry.access.redhat.com/rhscl/postgresql-96-rhel7",
                    "imageTag": "latest",
                    "persistence": {
                      "resourcePolicy": "nil",
                      "size": "20Gi"
                    },
                    "postgresDatabase": "anchore-feeds",
                    "postgresPassword": "anchore-postgres,123",
                    "postgresUser": "anchoreengine"
                  },
                  "anchore-ui-redis": {
                    "cluster": {
                      "enabled": false
                    },
                    "externalEndpoint": null,
                    "password": "anchore-redis,123",
                    "persistence": {
                      "enabled": false
                    }
                  },
                  "anchoreAnalyzer": {
                    "affinity": {},
                    "annotations": {},
                    "concurrentTasksPerWorker": 1,
                    "configFile": {
                      "retrieve_files": {
                        "file_list": [
                          "/etc/passwd"
                        ]
                      },
                      "secret_search": {
                        "match_params": [
                          "MAXFILESIZE=10000",
                          "STOREONMATCH=n"
                        ],
                        "regexp_match": [
                          "AWS_ACCESS_KEY=(?i).*aws_access_key_id( *=+ *).*(?\u003c![A-Z0-9])[A-Z0-9]{20}(?![A-Z0-9]).*",
                          "AWS_SECRET_KEY=(?i).*aws_secret_access_key( *=+ *).*(?\u003c![A-Za-z0-9/+=])[A-Za-z0-9/+=]{40}(?![A-Za-z0-9/+=]).*",
                          "PRIV_KEY=(?i)-+BEGIN(.*)PRIVATE KEY-+",
                          "DOCKER_AUTH=(?i).*\"auth\": *\".+\"",
                          "API_KEY=(?i).*api(-|_)key( *=+ *).*(?\u003c![A-Z0-9])[A-Z0-9]{20,60}(?![A-Z0-9]).*"
                        ]
                      }
                    },
                    "containerPort": 8084,
                    "cycleTimers": {
                      "image_analyzer": 5
                    },
                    "extraEnv": [],
                    "labels": {},
                    "layerCacheMaxGigabytes": 0,
                    "nodeSelector": {},
                    "replicaCount": 1,
                    "tolerations": []
                  },
                  "anchoreApi": {
                    "affinity": {},
                    "annotations": {},
                    "extraEnv": [],
                    "labels": {},
                    "nodeSelector": {},
                    "replicaCount": 1,
                    "service": {
                      "annotations": {},
                      "label": {},
                      "port": 8228,
                      "type": "ClusterIP"
                    },
                    "tolerations": []
                  },
                  "anchoreCatalog": {
                    "affinity": {},
                    "annotations": {},
                    "archive": {
                      "compression": {
                        "enabled": true,
                        "min_size_kbytes": 100
                      },
                      "storage_driver": {
                        "config": {},
                        "name": "db"
                      }
                    },
                    "cycleTimers": {
                      "analyzer_queue": 1,
                      "image_watcher": 3600,
                      "notifications": 30,
                      "policy_eval": 3600,
                      "repo_watcher": 60,
                      "service_watcher": 15,
                      "vulnerability_scan": 14400
                    },
                    "events": {
                      "notification": {
                        "enabled": false,
                        "level": [
                          "error"
                        ]
                      }
                    },
                    "extraEnv": [],
                    "labels": {},
                    "nodeSelector": {},
                    "replicaCount": 1,
                    "service": {
                      "annotations": {},
                      "labels": {},
                      "port": 8082,
                      "type": "ClusterIP"
                    },
                    "tolerations": []
                  },
                  "anchoreEnterpriseFeeds": {
                    "affinity": {},
                    "annotations": {},
                    "cycleTimers": {
                      "driver_sync": 7200
                    },
                    "dbConfig": {
                      "connectionPoolMaxOverflow": 100,
                      "connectionPoolSize": 30,
                      "ssl": false,
                      "sslMode": "verify-full",
                      "sslRootCertName": null,
                      "timeout": 120
                    },
                    "enabled": true,
                    "extraEnv": [],
                    "labels": {},
                    "nodeSelector": {},
                    "service": {
                      "annotations": {},
                      "labels": {},
                      "port": 8448,
                      "type": "ClusterIP"
                    },
                    "tolerations": []
                  },
                  "anchoreEnterpriseGlobal": {
                    "enabled": false,
                    "image": "docker.io/anchore/enterprise:v0.6.1",
                    "imagePullPolicy": "IfNotPresent",
                    "imagePullSecretName": "anchore-enterprise-pullcreds",
                    "licenseSecretName": "anchore-enterprise-license"
                  },
                  "anchoreEnterpriseNotifications": {
                    "affinity": {},
                    "annotations": {},
                    "cycleTimers": {
                      "notifications": 30
                    },
                    "enabled": true,
                    "extraEnv": [],
                    "labels": {},
                    "nodeSelector": {},
                    "service": {
                      "port": 8668
                    },
                    "tolerations": []
                  },
                  "anchoreEnterpriseRbac": {
                    "enabled": true,
                    "extraEnv": [],
                    "service": {
                      "apiPort": 8229,
                      "authPort": 8089
                    }
                  },
                  "anchoreEnterpriseReports": {
                    "affinity": {},
                    "annotations": {},
                    "cycleTimers": {
                      "reports_data_load": 600,
                      "reports_data_refresh": 7200,
                      "reports_metrics": 3600
                    },
                    "enableDataIngress": true,
                    "enableGraphql": true,
                    "enabled": true,
                    "extraEnv": [],
                    "labels": {},
                    "nodeSelector": {},
                    "service": {
                      "port": 8558
                    },
                    "tolerations": []
                  },
                  "anchoreEnterpriseUi": {
                    "affinity": {},
                    "annotations": {},
                    "enableProxy": false,
                    "enableSharedLogin": true,
                    "enableSsl": false,
                    "enabled": true,
                    "extraEnv": [],
                    "image": "docker.io/anchore/enterprise-ui:v0.6.1",
                    "imagePullPolicy": "IfNotPresent",
                    "labels": {},
                    "ldapsRootCaCertName": null,
                    "nodeSelector": {},
                    "policyHubUri": "http://hub.anchore.io",
                    "redisFlushdb": true,
                    "service": {
                      "annotations": {},
                      "labels": {},
                      "port": 80,
                      "sessionAffinity": "ClientIP",
                      "type": "ClusterIP"
                    },
                    "tolerations": []
                  },
                  "anchoreGlobal": {
                    "allowECRUseIAMRole": false,
                    "certStoreSecretName": null,
                    "cleanupImages": true,
                    "dbConfig": {
                      "connectionPoolMaxOverflow": 100,
                      "connectionPoolSize": 30,
                      "ssl": false,
                      "sslMode": "verify-full",
                      "sslRootCertName": null,
                      "timeout": 120
                    },
                    "defaultAdminEmail": "example@email.com",
                    "defaultAdminPassword": "foobar",
                    "enableMetrics": false,
                    "existingSecret": null,
                    "extraEnv": [],
                    "hashedPasswords": false,
                    "image": "docker.io/anchore/anchore-engine:v0.6.1",
                    "imageAnalyzeTimeoutSeconds": 36000,
                    "imagePullPolicy": "IfNotPresent",
                    "internalServicesSsl": {
                      "certSecretCertName": null,
                      "certSecretKeyName": null,
                      "enabled": false,
                      "verifyCerts": false
                    },
                    "labels": {},
                    "logLevel": "INFO",
                    "metricsAuthDisabled": false,
                    "oauthEnabled": false,
                    "oauthTokenExpirationSeconds": 3600,
                    "openShiftDeployment": true,
                    "saml": {
                      "privateKeyName": null,
                      "publicKeyName": null,
                      "secret": null
                    },
                    "scratchVolume": {
                      "details": {
                        "emptyDir": {}
                      },
                      "mountPath": "/analysis_scratch"
                    },
                    "serviceDir": "/anchore_service",
                    "webhooks": {
                      "general": {},
                      "ssl_verify": true,
                      "webhook_pass": null,
                      "webhook_user": null
                    },
                    "webhooksEnabled": false
                  },
                  "anchorePolicyEngine": {
                    "affinity": {},
                    "annotations": {},
                    "cycleTimers": {
                      "feed_sync": 14400,
                      "feed_sync_checker": 3600
                    },
                    "extraEnv": [],
                    "labels": {},
                    "nodeSelector": {},
                    "replicaCount": 1,
                    "service": {
                      "annotations": {},
                      "labels": {},
                      "port": 8087,
                      "type": "ClusterIP"
                    },
                    "tolerations": []
                  },
                  "anchoreSimpleQueue": {
                    "affinity": {},
                    "annotations": {},
                    "extraEnv": [],
                    "labels": {},
                    "nodeSelector": {},
                    "replicaCount": 1,
                    "service": {
                      "annotations": {},
                      "labels": {},
                      "port": 8083,
                      "type": "ClusterIP"
                    },
                    "tolerations": []
                  },
                  "cloudsql": {
                    "enabled": false,
                    "image": {
                      "pullPolicy": "IfNotPresent",
                      "repository": "gcr.io/cloudsql-docker/gce-proxy",
                      "tag": 1.12
                    },
                    "instance": ""
                  },
                  "ingress": {
                    "annotations": {
                      "kubernetes.io/ingress.class": "nginx"
                    },
                    "apiPath": "/v1/",
                    "enabled": false,
                    "labels": {},
                    "tls": [],
                    "uiPath": "/"
                  },
                  "postgresql": {
                    "externalEndpoint": null,
                    "extraEnv": [
                      {
                        "name": "POSTGRESQL_USER",
                        "value": "anchoreengine"
                      },
                      {
                        "name": "POSTGRESQL_PASSWORD",
                        "value": "anchore-postgres,123"
                      },
                      {
                        "name": "POSTGRESQL_DATABASE",
                        "value": "anchore"
                      },
                      {
                        "name": "PGUSER",
                        "value": "postgres"
                      },
                      {
                        "name": "LD_LIBRARY_PATH",
                        "value": "/opt/rh/rh-postgresql96/root/usr/lib64"
                      },
                      {
                        "name": "PATH",
                        "value": "/opt/rh/rh-postgresql96/root/usr/bin:/opt/app-root/src/bin:/opt/app-root/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"
                      }
                    ],
                    "image": "registry.access.redhat.com/rhscl/postgresql-96-rhel7",
                    "imageTag": "latest",
                    "persistence": {
                      "resourcePolicy": "nil",
                      "size": "20Gi"
                    },
                    "postgresDatabase": "anchore",
                    "postgresPassword": "anchore-postgres,123",
                    "postgresUser": "anchoreengine"
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: Security
          certified: "true"
          containerImage: registry.connect.redhat.com/anchore/engine-operator:v0.1.4-r0
          createdAt: "2019-12-13T07:00:00Z"
          description: Anchore Engine - container image scanning service for policy-based
            security, best-practice and compliance enforcement.
          repository: https://github.com/anchore/engine-operator
          support: anchore.slack.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Deploys Anchore Engine
            displayName: Anchore Engine Operator
            kind: AnchoreEngine
            name: anchoreengines.anchore.com
            version: v1alpha1
        description: "Anchore Engine is an open source software system that provides
          a centralized service for analyzing container images, \nscanning for security
          vulnerabilities, and enforcing deployment policies. Anchore Engine is provided
          as a Docker \ncontainer image that can be run standalone or with an orchestration
          platform such as Kubernetes. Anchore Engine allows \nusers to perform detailed
          analysis on their container images, run queries, produce reports, and define
          policies that \ncan be used in CI/CD pipelines. Users can extend Anchore
          Engine with plugins that add new queries, image analysis, and \npolicies.
          Anchore Engine can be accessed directly through a RESTful API or via the
          Anchore CLI.\n\nThe Anchore Engine Operator is based on the official stable
          [Helm Chart](https://github.com/helm/charts/tree/master/stable/anchore-engine)."
        displayName: Anchore Engine Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Anchore Inc.
        version: 0.1.4
      name: alpha
    defaultChannel: alpha
    packageName: anchore-engine
    provider:
      name: Anchore Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: KubeVirt project
      provider-url: ""
    name: kubevirt-hyperconverged
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kubevirt-hyperconverged
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kubevirt-hyperconverged-operator.v2.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "hco.kubevirt.io/v1alpha1",
                "kind": "HyperConverged",
                "metadata": {
                  "name": "kubevirt-hyperconverged",
                  "namespace": "kubevirt-hyperconverged"
                },
                "spec": {
                  "BareMetalPlatform": "false"
                }
              },
              {
                "apiVersion": "cdi.kubevirt.io/v1alpha1",
                "kind": "CDI",
                "metadata": {
                  "name": "cdi"
                }
              },
              {
                "apiVersion": "kubevirt.io/v1alpha3",
                "kind": "KubeVirt",
                "metadata": {
                  "name": "kubevirt",
                  "namespace": "kubevirt"
                }
              },
              {
                "apiVersion": "networkaddonsoperator.network.kubevirt.io/v1alpha1",
                "kind": "NetworkAddonsConfig",
                "metadata": {
                  "name": "network-addons"
                }
              },
              {
                "apiVersion": "kubevirtcommontemplatesbundles.kubevirt.io/v1",
                "kind": "KubevirtCommonTemplatesBundle",
                "metadata": {
                  "name": "common-templates"
                }
              },
              {
                "apiVersion": "kubevirtnodelabellerbundles.kubevirt.io/v1",
                "kind": "KubevirtNodeLabellerBundle",
                "metadata": {
                  "name": "node-labeller"
                }
              },
              {
                "apiVersion": "kubevirttemplatevalidators.kubevirt.io/v1",
                "kind": "KubevirtTemplateValidator",
                "metadata": {
                  "name": "template-validator"
                }
              },
              {
                "apiVersion": "kubevirtmetricsaggregations.kubevirt.io/v1",
                "kind": "KubevirtMetricsAggregation",
                "metadata": {
                  "name": "metrics-aggregation"
                }
              },
              {
                "apiVersion": "kubevirt.io/v1alpha1",
                "kind": "NodeMaintenance",
                "metadata": {
                  "name": "node-maintenance"
                },
                "spec": {
                  "nodeName": "nodename",
                  "reason": "node maintenance reason"
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: OpenShift Optional
          certified: "false"
          containerImage: registry.redhat.io/container-native-virtualization/hyperconverged-cluster-operator:v2.1.0-22
          createdAt: "2019-10-17 09:34:37"
          description: Creates and maintains a HyperConverged KubeVirt Deployment
          repository: https://github.com/kubevirt/hyperconverged-cluster-operator
          support: "false"
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents the deployment of a KubeVirt HyperConverged Cluster
              Operator
            displayName: KubeVirt HyperConverged Cluster Operator Deployment
            kind: HyperConverged
            name: hyperconvergeds.hco.kubevirt.io
            version: v1alpha1
          - description: Cluster Network Addons
            displayName: Cluster Network Addons
            kind: NetworkAddonsConfig
            name: networkaddonsconfigs.networkaddonsoperator.network.kubevirt.io
            version: v1alpha1
          - description: Represents a KubeVirt deployment
            displayName: KubeVirt deployment
            kind: KubeVirt
            name: kubevirts.kubevirt.io
            version: v1alpha3
          - description: Represents a deployment of the predefined VM templates
            displayName: KubeVirt common templates
            kind: KubevirtCommonTemplatesBundle
            name: kubevirtcommontemplatesbundles.kubevirt.io
            version: v1
          - description: Provide aggregation rules for core kubevirt metrics
            displayName: KubeVirt Metric Aggregation
            kind: KubevirtMetricsAggregation
            name: kubevirtmetricsaggregations.kubevirt.io
            version: v1
          - description: Represents a deployment of Node labeller component
            displayName: KubeVirt Node labeller
            kind: KubevirtNodeLabellerBundle
            name: kubevirtnodelabellerbundles.kubevirt.io
            version: v1
          - description: Represents a deployment of admission control webhook to validate
              the KubeVirt templates
            displayName: KubeVirt Template Validator admission webhook
            kind: KubevirtTemplateValidator
            name: kubevirttemplatevalidators.kubevirt.io
            version: v1
          - description: Represents a CDI deployment
            displayName: CDI deployment
            kind: CDI
            name: cdis.cdi.kubevirt.io
            version: v1alpha1
          - description: Represents a deployment of node maintenance crd
            displayName: KubeVirt node maintenance
            kind: NodeMaintenance
            name: nodemaintenances.kubevirt.io
            version: v1alpha1
        description: |-
          **Container-native virtualization** extends Red Hat OpenShift Container Platform, allowing you to host and manage virtualized workloads on the same platform as container-based workloads. From the OpenShift Container Platform web console, you can import a VMware virtual machine from vSphere, create new or clone existing VMs, perform live migrations between nodes, and more. You can use container-native virtualization to manage both Linux and Windows VMs.

          The technology behind container-native virtualization is developed in the [KubeVirt](https://kubevirt.io) open source community. The KubeVirt project extends [Kubernetes](https://kubernetes.io) by adding additional virtualization resource types through [Custom Resource Definitions](https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/) (CRDs). Administrators can use Custom Resource Definitions to manage `VirtualMachine` resources alongside all other resources that Kubernetes provides.
        displayName: Container-native virtualization Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: KubeVirt project
        version: 2.1.0
      name: "2.1"
    - currentCSV: kubevirt-hyperconverged-operator.v2.2.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "hco.kubevirt.io/v1alpha1",
                "kind": "HyperConverged",
                "metadata": {
                  "name": "kubevirt-hyperconverged",
                  "namespace": "kubevirt-hyperconverged"
                },
                "spec": {
                  "BareMetalPlatform": false
                }
              },
              {
                "apiVersion": "cdi.kubevirt.io/v1alpha1",
                "kind": "CDI",
                "metadata": {
                  "name": "cdi"
                }
              },
              {
                "apiVersion": "kubevirt.io/v1alpha3",
                "kind": "KubeVirt",
                "metadata": {
                  "name": "kubevirt",
                  "namespace": "kubevirt"
                }
              },
              {
                "apiVersion": "networkaddonsoperator.network.kubevirt.io/v1alpha1",
                "kind": "NetworkAddonsConfig",
                "metadata": {
                  "name": "network-addons"
                }
              },
              {
                "apiVersion": "kubevirtcommontemplatesbundles.kubevirt.io/v1",
                "kind": "KubevirtCommonTemplatesBundle",
                "metadata": {
                  "name": "common-templates"
                }
              },
              {
                "apiVersion": "kubevirtnodelabellerbundles.kubevirt.io/v1",
                "kind": "KubevirtNodeLabellerBundle",
                "metadata": {
                  "name": "node-labeller"
                }
              },
              {
                "apiVersion": "kubevirttemplatevalidators.kubevirt.io/v1",
                "kind": "KubevirtTemplateValidator",
                "metadata": {
                  "name": "template-validator"
                }
              },
              {
                "apiVersion": "kubevirtmetricsaggregations.kubevirt.io/v1",
                "kind": "KubevirtMetricsAggregation",
                "metadata": {
                  "name": "metrics-aggregation"
                }
              },
              {
                "apiVersion": "kubevirt.io/v1alpha1",
                "kind": "NodeMaintenance",
                "metadata": {
                  "name": "node-maintenance"
                },
                "spec": {
                  "nodeName": "nodename",
                  "reason": "node maintenance reason"
                }
              },
              {
                "apiVersion": "kubevirt.io/v1alpha1",
                "kind": "V2VVmware",
                "metadata": {
                  "name": "v2vvmware"
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: OpenShift Optional
          certified: "false"
          containerImage: registry.redhat.io/container-native-virtualization/hyperconverged-cluster-operator:v2.2.0-12
          createdAt: "2020-01-26 16:24:03"
          description: Creates and maintains a CNV Deployment
          repository: https://github.com/kubevirt/hyperconverged-cluster-operator
          support: "false"
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents the deployment of a KubeVirt HyperConverged Cluster
              Operator
            displayName: KubeVirt HyperConverged Cluster Operator Deployment
            kind: HyperConverged
            name: hyperconvergeds.hco.kubevirt.io
            version: v1alpha1
          - description: V2V Vmware
            displayName: V2V Vmware
            kind: V2VVmware
            name: v2vvmwares.kubevirt.io
            version: v1alpha1
          - description: Cluster Network Addons
            displayName: Cluster Network Addons
            kind: NetworkAddonsConfig
            name: networkaddonsconfigs.networkaddonsoperator.network.kubevirt.io
            version: v1alpha1
          - description: Represents a KubeVirt deployment
            displayName: KubeVirt deployment
            kind: KubeVirt
            name: kubevirts.kubevirt.io
            version: v1alpha3
          - description: Represents a deployment of the predefined VM templates
            displayName: KubeVirt common templates
            kind: KubevirtCommonTemplatesBundle
            name: kubevirtcommontemplatesbundles.kubevirt.io
            version: v1
          - description: Provide aggregation rules for core kubevirt metrics
            displayName: KubeVirt Metric Aggregation
            kind: KubevirtMetricsAggregation
            name: kubevirtmetricsaggregations.kubevirt.io
            version: v1
          - description: Represents a deployment of Node labeller component
            displayName: KubeVirt Node labeller
            kind: KubevirtNodeLabellerBundle
            name: kubevirtnodelabellerbundles.kubevirt.io
            version: v1
          - description: Represents a deployment of admission control webhook to validate
              the KubeVirt templates
            displayName: KubeVirt Template Validator admission webhook
            kind: KubevirtTemplateValidator
            name: kubevirttemplatevalidators.kubevirt.io
            version: v1
          - description: Represents a CDI deployment
            displayName: CDI deployment
            kind: CDI
            name: cdis.cdi.kubevirt.io
            version: v1alpha1
          - description: Represents a deployment of node maintenance crd
            displayName: KubeVirt node maintenance
            kind: NodeMaintenance
            name: nodemaintenances.kubevirt.io
            version: v1alpha1
          - description: Represents a HostPathProvisioner deployment
            displayName: HostPathProvisioner deployment
            kind: HostPathProvisioner
            name: hostpathprovisioners.hostpathprovisioner.kubevirt.io
            version: v1alpha1
        description: |-
          **Container-native virtualization** extends Red Hat OpenShift Container Platform, allowing you to host and manage virtualized workloads on the same platform as container-based workloads. From the OpenShift Container Platform web console, you can import a VMware virtual machine from vSphere, create new or clone existing VMs, perform live migrations between nodes, and more. You can use container-native virtualization to manage both Linux and Windows VMs.

          The technology behind container-native virtualization is developed in the [KubeVirt](https://kubevirt.io) open source community. The KubeVirt project extends [Kubernetes](https://kubernetes.io) by adding additional virtualization resource types through [Custom Resource Definitions](https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/) (CRDs). Administrators can use Custom Resource Definitions to manage `VirtualMachine` resources alongside all other resources that Kubernetes provides.
        displayName: Container-native virtualization
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: KubeVirt project
        version: 2.2.0
      name: "2.2"
    defaultChannel: "2.2"
    packageName: kubevirt-hyperconverged
    provider:
      name: KubeVirt project
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: GoogleCloudPlatform
      provider-url: ""
    name: spark-gcp
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/spark-gcp
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: sparkoperator.v2.4.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "sparkoperator.k8s.io/v1beta1",
                "kind": "SparkApplication",
                "metadata": {
                  "name": "pyspark-pi"
                },
                "spec": {
                  "type": "Python",
                  "pythonVersion": "2",
                  "mode": "cluster",
                  "image": "gcr.io/spark-operator/spark-py:v2.4.0",
                  "imagePullPolicy": "Always",
                  "mainApplicationFile": "local:///opt/spark/examples/src/main/python/pi.py",
                  "sparkVersion": "2.4.0",
                  "restartPolicy": {
                    "type": "OnFailure",
                    "onFailureRetries": 3,
                    "onFailureRetryInterval": 10,
                    "onSubmissionFailureRetries": 5,
                    "onSubmissionFailureRetryInterval": 20
                  },
                  "driver": {
                    "cores": 0.1,
                    "coreLimit": "200m",
                    "memory": "512m",
                    "labels": {
                      "version": "2.4.0"
                    },
                    "serviceAccount": "spark"
                  },
                  "executor": {
                    "cores": 1,
                    "instances": 1,
                    "memory": "512m",
                    "labels": {
                      "version": "2.4.0"
                    }
                  }
                }
              },
              {
                "apiVersion": "sparkoperator.k8s.io/v1beta1",
                "kind": "ScheduledSparkApplication",
                "metadata": {
                  "name": "spark-pi-scheduled"
                },
                "spec": {
                  "schedule": "@every 5m",
                  "concurrencyPolicy": "Allow",
                  "template": {
                    "type": "Scala",
                    "mode": "cluster",
                    "image": "gcr.io/spark-operator/spark:v2.4.0",
                    "imagePullPolicy": "Always",
                    "mainClass": "org.apache.spark.examples.SparkPi",
                    "mainApplicationFile": "local:///opt/spark/examples/jars/spark-examples_2.11-2.4.0.jar",
                    "restartPolicy": {
                      "type": "Never"
                    },
                    "driver": {
                      "cores": 0.1,
                      "coreLimit": "200m",
                      "memory": "512m",
                      "labels": {
                        "version": "2.4.0"
                      },
                      "serviceAccount": "spark"
                    },
                    "executor": {
                      "cores": 1,
                      "instances": 1,
                      "memory": "512m",
                      "labels": {
                        "version": "2.4.0"
                      }
                    }
                  }
                }
              }
            ]
          capabilities: Deep Insights
          categories: Big Data
          certified: "False"
          containerImage: gcr.io/spark-operator/spark-operator:v2.4.0-v1beta1-latest
          description: Apache Spark is a unified analytics engine for large-scale
            data processing.
          repository: https://github.com/GoogleCloudPlatform/spark-on-k8s-operator
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A configuration file for a ScheduledSparkApplication custom
              resource.
            displayName: ScheduledSparkApplication
            kind: ScheduledSparkApplication
            name: scheduledsparkapplications.sparkoperator.k8s.io
            version: v1beta1
          - description: A configuration file for a SparkApplication custom resource.
            displayName: SparkApplication
            kind: SparkApplication
            name: sparkapplications.sparkoperator.k8s.io
            version: v1beta1
        description: "# Spark Operator\nApache Spark is a fast and general-purpose
          cluster computing system.\nIt provides high-level APIs in Java, Scala, Python
          and R, and an optimized engine that supports general execution graphs. \nIt
          also supports a rich set of higher-level tools including Spark SQL for SQL
          and structured data processing, MLlib for \nmachine learning, GraphX for
          graph processing, and Spark Streaming.\n\nThe [spark-on-k8s-operator](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator)
          allows Spark applications \nto be defined in a declarative manner and supports
          one-time Spark applications with `SparkApplication` \nand cron-scheduled
          applications with `ScheduledSparkApplication`.\n\nConsult the [user guide](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/user-guide.md)
          and \n[examples](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/tree/master/examples)
          to see how to \nwrite Spark applications for the operator.\n\nSee the [API
          spec](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/api.md)
          and \n[architecture doc](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/design.md#architecture)
          \nfor more details on how the operator functions.\n\n## Before You Start\nThe
          driver pod for Spark applications requires a ServiceAccount with sufficient
          privileges.\nSee the doc on [ServiceAccount for Driver Pods](https://github.com/GoogleCloudPlatform/spark-on-k8s-operator/blob/master/docs/quick-start-guide.md#about-the-service-account-for-driver-pods)
          for more details.\nA ServiceAccount named `spark` is already created in
          the namespace where the Operator is deployed.\nUse this ServiceAccount in
          the spec for your `SparkApplication` and `ScheduledSparkApplication`."
        displayName: Spark Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: GoogleCloudPlatform
        version: 2.4.0
      name: alpha
    defaultChannel: alpha
    packageName: spark-gcp
    provider:
      name: GoogleCloudPlatform
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: CNCF
      provider-url: ""
    name: jaeger
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/jaeger
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: jaeger-operator.v1.16.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "jaegertracing.io/v1",
                "kind": "Jaeger",
                "metadata": {
                  "name": "jaeger-all-in-one-inmemory"
                }
              },
              {
                "apiVersion": "jaegertracing.io/v1",
                "kind": "Jaeger",
                "metadata": {
                  "name": "jaeger-all-in-one-local-storage"
                },
                "spec": {
                  "storage": {
                    "options": {
                      "badger": {
                        "directory-key": "/badger/key",
                        "directory-value": "/badger/data",
                        "ephemeral": false
                      }
                    },
                    "type": "badger",
                    "volumeMounts": [
                      {
                        "mountPath": "/badger",
                        "name": "data"
                      }
                    ],
                    "volumes": [
                      {
                        "emptyDir": {},
                        "name": "data"
                      }
                    ]
                  }
                }
              },
              {
                "apiVersion": "jaegertracing.io/v1",
                "kind": "Jaeger",
                "metadata": {
                  "name": "jaeger-prod-elasticsearch"
                },
                "spec": {
                  "storage": {
                    "options": {
                      "es": {
                        "server-urls": "http://elasticsearch.default.svc:9200"
                      }
                    },
                    "type": "elasticsearch"
                  },
                  "strategy": "production"
                }
              }
            ]
          capabilities: Deep Insights
          categories: Logging & Tracing
          certified: "false"
          containerImage: docker.io/jaegertracing/jaeger-operator:1.16.0
          createdAt: "2019-09-04T13:28:40+00:00"
          description: Provides tracing, monitoring and troubleshooting for microservices-based
            distributed systems
          repository: https://github.com/jaegertracing/jaeger-operator
          support: Jaeger Community
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A configuration file for a Jaeger custom resource.
            displayName: Jaeger
            kind: Jaeger
            name: jaegers.jaegertracing.io
            version: v1
        description: |-
          Jaeger, inspired by [Dapper](https://research.google.com/pubs/pub36356.html) and [OpenZipkin](http://zipkin.io/), is a distributed tracing system released as open source by Uber Technologies. It is used for monitoring and troubleshooting microservices-based distributed systems.

          ### Core capabilities

          Jaeger is used for monitoring and troubleshooting microservices-based distributed systems, including:

          * Distributed context propagation
          * Distributed transaction monitoring
          * Root cause analysis
          * Service dependency analysis
          * Performance / latency optimization
          * OpenTracing compatible data model
          * Multiple storage backends: Badger, Cassandra, Elasticsearch, Memory.

          ### Operator features

          * **Multiple modes** - Supports `allInOne`, `production` and `streaming` [modes of deployment](https://www.jaegertracing.io/docs/latest/operator/#deployment-strategies).

          * **Configuration** - The Operator manages [configuration information](https://www.jaegertracing.io/docs/latest/operator/#configuring-the-custom-resource) when installing Jaeger instances.

          * **Storage** - [Configure storage](https://www.jaegertracing.io/docs/latest/operator/#storage-options) used by Jaeger. By default, `memory` is used. Other options include `badger`, `cassandra` or `elasticsearch`. On OpenShift, the operator can delegate creation of an Elasticsearch cluster to the Elasticsearch Operator if deployed.

          * **Agent** - can be deployed as [sidecar](https://www.jaegertracing.io/docs/latest/operator/#auto-injecting-jaeger-agent-sidecars) (default) and/or [daemonset](https://www.jaegertracing.io/docs/latest/operator/#installing-the-agent-as-daemonset).

          * **UI** - Optionally setup ingress (Kubernetes) or secure route (OpenShift) to provide [access to the Jaeger UI](https://www.jaegertracing.io/docs/latest/operator/#accessing-the-jaeger-console-ui).

          ### Before you start

          1. Ensure that the appropriate storage solution, that will be used by the Jaeger instance, is available and configured.
          2. If intending to deploy an Elasticsearch cluster via the Jaeger custom resource, then the Elasticsearch Operator must first be installed.

          ### Troubleshooting

          * https://www.jaegertracing.io/docs/latest/troubleshooting/
        displayName: Community Jaeger Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: CNCF
        version: 1.16.0
      name: stable
    defaultChannel: stable
    packageName: jaeger
    provider:
      name: CNCF
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: EnMasse
      provider-url: ""
    name: enmasse
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/enmasse
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: enmasse.0.30.2
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "admin.enmasse.io/v1beta1",
                "kind": "StandardInfraConfig",
                "metadata": {
                  "name": "default"
                },
                "spec": {
                  "broker": {
                    "resources": {
                      "memory": "1Gi",
                      "storage": "5Gi"
                    },
                    "addressFullPolicy": "FAIL"
                  },
                  "router": {
                    "linkCapacity": 50,
                    "resources": {
                      "memory": "512Mi"
                    }
                  }
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta1",
                "kind": "BrokeredInfraConfig",
                "metadata": {
                  "name": "default"
                },
                "spec": {
                  "broker": {
                    "resources": {
                      "memory": "4Gi"
                    }
                  }
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta2",
                "kind": "AddressPlan",
                "metadata": {
                  "name": "standard-small-queue"
                },
                "spec": {
                  "addressType": "queue",
                  "shortDescription": "Small Queue",
                  "resources": {
                    "router": 0.01,
                    "broker": 0.1
                  }
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta2",
                "kind": "AddressSpacePlan",
                "metadata": {
                  "name": "standard-small"
                },
                "spec": {
                  "addressSpaceType": "standard",
                  "infraConfigRef": "default",
                  "shortDescription": "Small Address Space Plan",
                  "resourceLimits": {
                    "router": 1.0,
                    "broker": 2.0,
                    "aggregate": 2.0
                  },
                  "addressPlans": [
                    "standard-small-queue"
                  ]
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta1",
                "kind": "AuthenticationService",
                "metadata": {
                  "name": "standard-authservice"
                },
                "spec": {
                  "type": "standard",
                  "standard": {
                    "storage": {
                      "claimName": "standard-authservice",
                      "deleteClaim": true,
                      "size": "1Gi",
                      "type": "persistent-claim"
                    }
                  }
                }
              },
              {
                "apiVersion": "enmasse.io/v1beta1",
                "kind": "AddressSpace",
                "metadata": {
                  "name": "myspace"
                },
                "spec": {
                  "plan": "standard-small",
                  "type": "standard"
                }
              },
              {
                "apiVersion": "enmasse.io/v1beta1",
                "kind": "Address",
                "metadata": {
                  "name": "myspace.myqueue"
                },
                "spec": {
                  "address": "myqueue",
                  "plan": "standard-small-queue",
                  "type": "queue"
                }
              },
              {
                "apiVersion": "user.enmasse.io/v1beta1",
                "kind": "MessagingUser",
                "metadata": {
                  "name": "myspace.user"
                },
                "spec": {
                  "authentication": {
                    "password": "ZW5tYXNzZQ==",
                    "type": "password"
                  },
                  "authorization": [
                    {
                      "addresses": [
                        "myqueue"
                      ],
                      "operations": [
                        "send",
                        "recv"
                      ]
                    }
                  ],
                  "username": "user"
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta1",
                "kind": "ConsoleService",
                "metadata": {
                  "name": "console"
                },
                "spec": {}
              },
              {
                "apiVersion": "iot.enmasse.io/v1alpha1",
                "kind": "IoTConfig",
                "metadata": {
                  "name": "default"
                },
                "spec": {
                  "adapters": {
                    "mqtt": {
                      "endpoint": {
                        "secretNameStrategy": {
                          "secretName": "iot-mqtt-adapter-tls"
                        }
                      }
                    }
                  },
                  "services": {
                    "deviceRegistry": {
                      "file": {
                        "numberOfDevicesPerTenant": 1000
                      }
                    }
                  }
                }
              },
              {
                "apiVersion": "enmasse.io/v1beta1",
                "kind": "AddressSpaceSchema",
                "metadata": {
                  "name": "undefined"
                },
                "spec": {}
              },
              {
                "apiVersion": "iot.enmasse.io/v1alpha1",
                "kind": "IoTProject",
                "metadata": {
                  "name": "iot"
                },
                "spec": {
                  "downstreamStrategy": {
                    "managedStrategy": {
                      "addressSpace": {
                        "name": "iot",
                        "plan": "standard-unlimited"
                      },
                      "addresses": {
                        "command": {
                          "plan": "standard-small-anycast"
                        },
                        "event": {
                          "plan": "standard-small-queue"
                        },
                        "telemetry": {
                          "plan": "standard-small-anycast"
                        }
                      }
                    }
                  }
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Streaming & Messaging
          certified: "false"
          containerImage: quay.io/enmasse/controller-manager:0.30.2
          createdAt: "2019-02-19T00:00:00Z"
          description: EnMasse provides messaging as a managed service on Kubernetes
          repository: https://github.com/EnMasseProject/enmasse
          support: EnMasse
        apiservicedefinitions:
          owned:
          - description: A group of messaging addresses that can be accessed via the
              same endpoint
            displayName: Address Space
            group: enmasse.io
            kind: AddressSpace
            name: addressspaces
            version: v1beta1
          - description: A messaging address that can be used to send/receive messages
              to/from
            displayName: Address
            group: enmasse.io
            kind: Address
            name: addresses
            version: v1beta1
          - description: A resource representing the available schema of plans and
              authentication services
            displayName: AddressSpaceSchema
            group: enmasse.io
            kind: AddressSpaceSchema
            name: addressspaceschemas
            version: v1beta1
          - description: A messaging user that can connect to an Address Space
            displayName: Messaging User
            group: user.enmasse.io
            kind: MessagingUser
            name: messagingusers
            version: v1beta1
        customresourcedefinitions:
          owned:
          - description: Infrastructure configuration template for the standard address
              space type
            displayName: Standard Infra Config
            kind: StandardInfraConfig
            name: standardinfraconfigs.admin.enmasse.io
            version: v1beta1
          - description: Infrastructure configuration template for the brokered address
              space type
            displayName: Brokered Infra Config
            kind: BrokeredInfraConfig
            name: brokeredinfraconfigs.admin.enmasse.io
            version: v1beta1
          - description: Plan describing the resource usage of a given address type
            displayName: Address Plan
            kind: AddressPlan
            name: addressplans.admin.enmasse.io
            version: v1beta2
          - description: Plan describing the capabilities and resource limits of a
              given address space type
            displayName: Address Space Plan
            kind: AddressSpacePlan
            name: addressspaceplans.admin.enmasse.io
            version: v1beta2
          - description: Authentication service configuration available to address
              spaces.
            displayName: Authentication Service
            kind: AuthenticationService
            name: authenticationservices.admin.enmasse.io
            version: v1beta1
          - description: Console Service Singleton for deploying global console.
            displayName: Console Service
            kind: ConsoleService
            name: consoleservices.admin.enmasse.io
            version: v1beta1
          - description: IoT Infrastructure Configuration Singleton
            displayName: IoT Config
            kind: IoTConfig
            name: iotconfigs.iot.enmasse.io
            version: v1alpha1
          - description: An IoT project instance
            displayName: IoT Project
            kind: IoTProject
            name: iotprojects.iot.enmasse.io
            version: v1alpha1
        description: "**EnMasse** provides messaging as a managed service on Kubernetes.\nWith
          EnMasse, administrators can configure a cloud-native,\nmulti-tenant messaging
          service either in the cloud or on premise.\nDevelopers can provision messaging
          using the EnMasse Console.\nMultiple development teams can provision the
          brokers and queues from the\nconsole, without requiring each team to install,
          configure, deploy,\nmaintain, or patch any software. \n\n**The core capabilities
          include:**\n\n  * **Built-in authentication and authorization** - Use the
          built-in authentication service or\n    plug in your own authentication
          service for authentication and\n    authorization of messaging clients.\n\n
          \ * **Self-service messaging for applications** - The service administrator
          deploys\n    and manages the messaging infrastructure, while applications
          can request\n    messaging resources regardless of the messaging infrastructure.\n\n
          \ * **Support for a wide variety of messaging patterns** - Choose between\n
          \   JMS-style messaging with strict guarantees, or messaging that supports\n
          \   a larger number of connections and higher throughput.\n\n## Post-installation
          tasks\n\nTo fully use EnMasse, you need to create a few\ninfrastructure
          components after the installation, including:\n\n  * An authentication service\n
          \ * Infrastructure configuration for the standard and brokered address space\n
          \ * Address and address space plans\n  * (Optional) Create RBAC roles to
          allow users to self-manage addresses and\n    address spaces.\n\nFor a complete
          overview of how to configure EnMasse, see\n[Configuring EnMasse](https://enmasse.io/documentation/).\n\n###
          Quickstart infrastructure configuration\n\nIf you simply want to get started
          quickly, you can import the following\nYAML by saving the content to a file
          and apply it by running the \n`oc apply -f <file>` command. You can also
          split the content (at the `---` marker)\nand import the single YAML document
          using the Web UI: \n\n~~~yaml\n---\napiVersion: admin.enmasse.io/v1beta1\nkind:
          StandardInfraConfig\nmetadata:\n  name: default\n---\napiVersion: admin.enmasse.io/v1beta2\nkind:
          AddressPlan\nmetadata:\n  name: standard-small-queue\nspec:\n  addressType:
          queue\n  resources:\n    router: 0.01\n    broker: 0.1\n---\napiVersion:
          admin.enmasse.io/v1beta2\nkind: AddressSpacePlan\nmetadata:\n  name: standard-small\nspec:\n
          \ addressSpaceType: standard\n  infraConfigRef: default\n  addressPlans:\n
          \ - standard-small-queue\n  resourceLimits:\n    router: 2.0\n    broker:
          3.0\n    aggregate: 4.0\n---\napiVersion: admin.enmasse.io/v1beta1\nkind:
          AuthenticationService\nmetadata:\n  name: standard-authservice\nspec:\n
          \ type: standard\n  standard:\n    storage:\n      claimName: standard-authservice\n
          \     deleteClaim: true\n      size: 1Gi\n      type: persistent-claim\n~~~\n\n###
          Creating infrastructure using the Web UI\n\nYou must create a new instance
          of each of the following custom resources. You can\nuse the example data
          directly, which is provided when using the\nWeb UI:\n\n  * *Authentication
          Service* â€“ Create an authentication service.\n  * *Brokered Infra Config*
          â€“ Create the broker infrastructure configuration.\n  * *Standard Infra Config*
          â€“ Create the standard infrastructure\n    configuration.\n\nYou must also
          create at least one address space plan and one address plan.\n\n*Note*:
          The name of the address space plan and address plan is required\nlater when
          creating new addresses. Some examples use specific plan\nnames, which might
          not be available in your environment. You can\ncreate those plans, or edit
          the examples to use different plan names.\n\n## Configuration for messaging
          tenants\n\nWhile service administrators perform the infrastructure configuration,
          the following\nresources are for the actual users of the system, the messaging
          tenants.\n\nYou need to create those resources to satisfy your particular
          use case.\n\n  * *Address space* â€“ A container for addresses\n  * *Address*
          â€“ A messaging address (address, topic, queue, and so on)\n  * *Messaging
          user* â€“ Manages access to an address\n\n## Enabling the IoT messaging layer
          (tech-preview)\n\nEnMasse contains an IoT messaging layer, which is not
          enabled\nby default. You may enable it by creating an `IoTConfig` resource,
          in the\nnamespace of the EnMasse installation. The name of the resource\nmust
          be `default`.\n\nYou can then create resources of the type `IoTProject`,
          which allow you to\nregister and connect devices via HTTP, MQTT, Sigfox
          and LoRaWAN.\n\n*Note:* The default `IoTConfig` example uses the \"file
          based\" device registry.\nIt allows you to quicky evaluate the IoT layer.
          However you must not use this\nin any productive environment.\n\n*Note:*
          The default `IoTConfig` examples configures the MQTT adapter with an\nexplicit
          TLS key/certificate stored in the secret `iot-mqtt-adapter-tls`. You\nmust
          provide this secret, under the configured name, in order for the\nMQTT protocol
          adapter to pick it up.\n\n*Note*: The `IoTProject` examples assume that
          you have deployed the example\nplans. If you did not deploy the example
          plans, or changed their names, you\nmust adapt the names in the IoT examples
          as well.\n\nFor more information on the IoT layer, see: [Getting Started
          with Internet of Things (IoT) on EnMasse](${EVAL_IOT_DOC_URL})\n"
        displayName: EnMasse
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: EnMasse
        version: 0.30.2
      name: alpha
    defaultChannel: alpha
    packageName: enmasse
    provider:
      name: EnMasse
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Dev4Devs.com
      provider-url: ""
    name: postgresql-operator-dev4devs-com
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/postgresql-operator-dev4devs-com
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: postgresql-operator.v0.1.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "postgresql.dev4devs.com/v1alpha1",
                "kind": "Database",
                "metadata": {
                  "name": "database"
                },
                "spec": {
                  "databaseCpu": "30m",
                  "databaseCpuLimit": "60m",
                  "databaseMemoryLimit": "512Mi",
                  "databaseMemoryRequest": "128Mi",
                  "databaseName": "example",
                  "databaseNameKeyEnvVar": "POSTGRESQL_DATABASE",
                  "databasePassword": "postgres",
                  "databasePasswordKeyEnvVar": "POSTGRESQL_PASSWORD",
                  "databaseStorageRequest": "1Gi",
                  "databaseUser": "postgres",
                  "databaseUserKeyEnvVar": "POSTGRESQL_USER",
                  "image": "centos/postgresql-96-centos7",
                  "size": 1
                }
              },
              {
                "apiVersion": "postgresql.dev4devs.com/v1alpha1",
                "kind": "Backup",
                "metadata": {
                  "name": "backup"
                },
                "spec": {
                  "awsAccessKeyId": "example-awsAccessKeyId",
                  "awsS3BucketName": "example-awsS3BucketName",
                  "awsSecretAccessKey": "example-awsSecretAccessKey",
                  "schedule": "0 0 * * *"
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Database
          certified: "false"
          containerImage: quay.io/dev4devs-com/postgresql-operator:0.1.1
          createdAt: "2019-09-08T08:00:00Z"
          description: Operator in Go developed using the Operator Framework to package,
            install, configure and manage a PostgreSQL database. This project includes
            backup feature.
          repository: https://github.com/dev4devs-com/postgresql-operator
          support: Dev4Devs, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Backup is the Schema for the backups API
            displayName: Database Backup
            kind: Backup
            name: backups.postgresql.dev4devs.com
            version: v1alpha1
          - description: Database is the Schema for the the Database Database API
            displayName: Database Database
            kind: Database
            name: databases.postgresql.dev4devs.com
            version: v1alpha1
        description: |-
          A very flexible and customizable Operator in Go developed using the Operator Framework to package, install, configure and manage a PostgreSQL database. Also, the usage of this operator offers:
          * Backup your data and sent it to a AWS Storage
          * Usage of encryption to send the data
          * Customization of the image and version of your PostgreSQL
          * Customization of the enviroment variables keys and values which should be used with
          * Allow you setup the operator to get the values required for its enviroment variables (user, password and database name) in an ConfigMap applied in the cluster already.
          *NOTE: It is very useful to centralize and share this information accross the cluster for your solutions. Also, you are able to configure each configMap keys that contains each EnvVar* required for the PostgreSQL database image.
          * To configure AWS Storage to sent the Backup data you are able to inform the data required or inform the secret which has this information and is applied in the cluster already
          * To configure encryption feature in the Backup you are able to inform the data required or inform the secret which has this information and is applied in the cluster already
          * Allow you customize the resources and their limmits (Memory, CPU and Storage)
        displayName: PostgreSQL Operator by Dev4Ddevs.com
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Dev4Devs.com
        version: 0.1.1
      name: alpha
    defaultChannel: alpha
    packageName: postgresql-operator-dev4devs-com
    provider:
      name: Dev4Devs.com
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: OpsMx
      provider-url: ""
    name: opsmx-spinnaker-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/opsmx-spinnaker-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: opsmx-spinnaker-operator.v1.17.4
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"OpsmxSpinnakerOperator","metadata":{"name":"op-spin"},"spec":{"halyard":{"spinnakerVersion":"1.17.4","image":{"repository":"opsmx11/operator-halyard","tag":"1.24.0"}},"dockerRegistries":[{"name":"dockerhub","address":"index.docker.io","repositories":["library/alpine","library/ubuntu","library/centos","library/nginx"]}],"spinnakerFeatureFlags":["artifacts"],"minio":{"enabled":true,"imageTag":"RELEASE.2018-06-09T02-18-09Z","serviceType":"ClusterIP","accessKey":"spinnakeradmin","secretKey":"spinnakeradmin","bucket":"spinnaker","nodeSelector":{},"persistence":{"enabled":false}},"rbac":{"create":false},"serviceAccount":{"create":false,"halyardName":"opsmx-spinnaker-operator","spinnakerName":null}}}]'
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: docker.io/opsmx11/opsmx-spinnaker-operator:v0.1
          createdAt: "2019-04-08 19:49:35"
          description: Spinnaker is an open source, multi-cloud continuous delivery
            platform to perform software releases with high velocity and confidence.
          support: OpsMx
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: OpsmxSpinnakerOperator
            displayName: opsmxspinnakeroperator
            kind: OpsmxSpinnakerOperator
            name: opsmxspinnakeroperators.charts.helm.k8s.io
            version: v1alpha1
        description: "# Spinnaker as an Operator\n\nThis Operator allows users to
          spin up Open Enterprise Spinnaker (OES) to manage deployments etc., using
          Openshift CRD&#39;s. With the help of Spinnaker Services Operator, users
          will have the convenience and confidence of simplified approach to execute
          CI/CD process with high velocity and quality deployments in all environments.
          \n\n## What is Spinnaker?\n\n* Spinnaker is an Open Source, multi-cloud
          Continuous delivery platform to perform software releases with high velocity
          and confidence.\n* Spinnaker helps user to create deployment pipelines that
          run integration and system tests, spin up and down server groups, and monitor
          your rollouts.\n* OES has enterprise value-adds as described at http://opsmx.com\n\n###
          List of Features\n\n* Multi-Cloud Deployment - Deploy your VM or Containers
          or functions across most public and private cloud including AWS EC2, ECS,
          EKS, Lambda, Kubernetes, Google Compute Engine, Google Kubernetes Engine,
          Google App Engine, Microsoft Azure, OpenStack, with Oracle Bare Metal and
          DC/OS.\n* Automated Releases with Pipelines - Create deployment pipelines
          that run integration and system tests, spin up and down server groups, and
          monitor your rollouts. Trigger pipelines via git events, Jenkins, Travis
          CI, Docker, CRON, or other Spinnaker pipelines\n* Pipeline-as-code - Manage
          the pipeline as code (JSON) or interact with pipeline using API or UI.\n*
          Safe Deployment Strategies - Deploy using Canary or Red/Black (Blue/Green)
          or Rolling update and enable automated Canary analysis to ensure safety
          of the new updates before full-rollout to production\n* 1-click Rollback
          - Rolling back new deployments is never been easier with a 1-click rollback
          of images/configurations.\n* See more spinnaker.io or docs.opsmx.com\n"
        displayName: Opsmx Spinnaker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: OpsMx
        version: 1.17.4
      name: alpha
    defaultChannel: alpha
    packageName: opsmx-spinnaker-operator
    provider:
      name: OpsMx
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: GigaSpaces
      provider-url: ""
    name: insightedge-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/insightedge-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: insightedge-operator.v0.0.2
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
               "apiVersion": "insightedge.example.com/v1alpha1",
               "kind": "Edge",
               "metadata": {
                  "name": "example-insightedge"
               },
               "spec": {
                  "image": {
                     "registry": "registry.access.redhat.com",
                     "repository": "gigaspaces/insightedge-enterprise-14-0",
                     "tag": "latest",
                     "pullPolicy": "IfNotPresent"
                  },
                  "service": {
                     "type": "ClusterIP",
                     "port": 3306
                  },
                  "serviceAccountName": "insightedge-operator",
                  "resources": {}
               }
            }]
          capabilities: Basic Install
          categories: Monitoring,OpenShift Optional
          certified: "true"
          containerImage: registry.connect.redhat.com/gigaspaces/insightedge:v0.0.2
          createdAt: "2019-04-25 12:59:59"
          description: Deploy the InsightEdge agent from a helm chart onto your Kubernetes
            or OpenShift cluster
          repository: https://github.com/RHC4TP/operators/tree/master/partners/gigaspaces-insightedge-operator
          support: Red Hat Connect
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: InsightEdge is an in-memory real-time analytics platform
              for instant insights to action. It's an always-on platform for mission-critical
              applications across cloud, on-premise or hybrid. InsightEdge operationalizes
              machine learning and transactional processing, at scale; analyzing data
              as it's born, enriching it with historical context, for instant insights.
            displayName: InsightEdge Operator
            kind: Edge
            name: edges.insightedge.example.com
            version: v1alpha1
        description: "This is an Operator for InsightEdge, developed using the InsightEdge
          helm chart available at (https://github.com/RHC4TP/operators/blob/master/partners/gigaspaces-insightedge-operator/helm-charts/edge)\n\nThe
          operator will deploy the InsightEdge agent (using the helm chart) on your
          Kubernetes or OpenShift cluster as a Daemonset.\n\n## Prerequisites\nThe
          InsightEdge Operator will (by default) deploy the latest version of InsightEdge
          Infrastructure from Red Hat Software Collections for RHEL 7.\nThe image
          will pull from the [Red Hat Container Catalog](https://access.redhat.com/containers),
          which will require the following:\n\n* RHN account (https://access.redhat.com)\n*
          RHCC Image pull secret in your project/namespace or [Red Hat Container Registry
          Authentication](https://access.redhat.com/RegistryAuthentication) setup
          on your OpenShift cluster\n\nTo create an `rhcc` secret using docker (requires
          root/sudo or you must be in the docker group):\n```\n# docker login -u <username>
          registry.connect.redhat.com\nPassword:\nLogin Succeeded\n# oc create secret
          generic rhcc --from-file=.dockerconfigjson=$HOME/.docker/config.json --type=kubernetes.io/dockerconfigjson\n```\n\nTo
          create an `rhcc` secret using podman:\n```\n$ podman login -u <username>
          registry.connect.redhat.com\nPassword:\nLogin Succeeded!\n$ oc create secret
          generic rhcc --from-file=.dockerconfigjson=$XDG_RUNTIME_DIR/containers/auth.json
          --type=kubernetes.io/dockerconfigjson\n```\n\nTo link the `rhcc` secret
          to the `default` service account to use as an image pull secret in the current
          project/namespace:\n```\n$ oc secrets link default rhcc --for=pull\n```\n\n##
          Required Parameters\nThere is only one required parameter, and that is the
          Agent Key available by logging into [InsightEdge](https://infrastructure.insightedge.com).\n\nNote
          that the Agent Key is kept in a base64-encoded K8s Secret, however the key
          is also visible as plain text within the Custom Resource spec.\n  \nValues
          listed below are relative to the `spec:` field in the Custom Resource.\n
          \n* **insightedge.agent.key** - Your InsightEdge Agent Key\n\n## Advanced
          Features\n\nThis is mostly a TODO section, however you can define your own
          values as required for the following fields:\n\nValues are relative to the
          `spec:` field in the Custom Resource.\n\n* **image.registry** - The dns
          hostname of the desired container image registry (defaults to `registry.access.redhat.com`)\n*
          **image.repository** - The repository path (appended to the registry URL)
          above where the container image resides (defaults to `rhscl/insightedge-102-rhel`)\n*
          **image.tag** - The container image tag (defaults to `:latest`)\n* **db.config**
          - The InsightEdge configuration populating `my.cnf` inside the container.
          Keep in mind that the `rhscl/insightedge-102-rhel` container is hard-coded
          to expose port 3306 (if you want to change this then you'll have to specify
          a custom MariaDB container image)\n* **resources** - Define your own Pod
          resource limits (`limits.cpu` and `limits.memory`)\n* **service.type** -
          The type of Service to configure for InsightEdge (defaults to `ClusterIP`)\n*
          **service.port** - The TCP port that the Service will listen on    \n* **serviceAccountName**
          - The name of the service account used to deploy InsightEdge (NOTE: This
          **must** match what is defined in the ClusterServiceVersion)\n"
        displayName: InsightEdge Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: GigaSpaces
        version: 0.0.2
      name: alpha
    defaultChannel: alpha
    packageName: insightedge-operator
    provider:
      name: GigaSpaces
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Sysdig
      provider-url: ""
    name: sysdig-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/sysdig-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: sysdig-operator.v1.4.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "sysdig.com/v1alpha1",
              "kind": "SysdigAgent",
              "metadata": {
                "name": "basic-agent-deployment"
              },
              "spec": {
                "sysdig": {
                  "accessKey": "XXX"
                }
              }
            },
            {
              "apiVersion": "sysdig.com/v1alpha1",
              "kind": "SysdigAgent",
              "metadata": {
                "name": "agent-with-ebpf-and-secure-enabled"
              },
              "spec": {
                "ebpf": {
                  "enabled": true
                },
                "secure": {
                  "enabled": true
                },
                "sysdig": {
                  "accessKey": "XXX"
                }
              }
            }]
          capabilities: Basic Install
          categories: Security, Monitoring
          certified: "false"
          containerImage: registry.connect.redhat.com/sysdig/sysdig-operator
          createdAt: "2019-03-08 18:30:00"
          description: Sysdig is a unified platform for container and microservices
            monitoring, troubleshooting, security and forensics. Sysdig platform has
            been built on top of Sysdig tool and Sysdig Inspect open-source technologies.
          repository: https://github.com/sysdiglabs/sysdig-operator/
          support: Sysdig, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Sysdig Agent running on each node of your cluster.
            displayName: Sysdig Agent daemonSet
            kind: SysdigAgent
            name: sysdigagents.sysdig.com
            version: v1alpha1
        description: |-
          [Sysdig](https://www.sysdig.com/) is a unified platform for container and
          microservices monitoring, troubleshooting, security and forensics. Sysdig
          platform has been built on top of
          [Sysdig tool](https://sysdig.com/opensource/sysdig/) and
          [Sysdig Inspect](https://sysdig.com/blog/sysdig-inspect/) open-source
          technologies.

          This operator installs the Sysdig Agent for
          [Sysdig Monitor](https://sysdig.com/product/monitor/) and
          [Sysdig Secure](https://sysdig.com/product/secure/) to all nodes in your
          cluster via a DaemonSet.

          ## Settings

          This operator, uses the same options than the
          [Helm Chart](https://hub.helm.sh/charts/stable/sysdig), please take a look
          to all the options in the following table:

          | Parameter                       | Description                                                            | Default                                     |
          | ---                             | ---                                                                    | ---                                         |
          | `image.registry`                | Sysdig agent image registry                                            | `docker.io`                                 |
          | `image.repository`              | The image repository to pull from                                      | `sysdig/agent`                              |
          | `image.tag`                     | The image tag to pull                                                  | `0.89.0`                                    |
          | `image.pullPolicy`              | The Image pull policy                                                  | `IfNotPresent`                              |
          | `image.pullSecrets`             | Image pull secrets                                                     | `nil`                                       |
          | `resources.requests.cpu`        | CPU requested for being run in a node                                  | `100m`                                      |
          | `resources.requests.memory`     | Memory requested for being run in a node                               | `512Mi`                                     |
          | `resources.limits.cpu`          | CPU limit                                                              | `200m`                                      |
          | `resources.limits.memory`       | Memory limit                                                           | `1024Mi`                                    |
          | `rbac.create`                   | If true, create & use RBAC resources                                   | `true`                                      |
          | `serviceAccount.create`         | Create serviceAccount                                                  | `true`                                      |
          | `serviceAccount.name`           | Use this value as serviceAccountName                                   | ` `                                         |
          | `daemonset.updateStrategy.type` | The updateStrategy for updating the daemonset                          | `RollingUpdate`                             |
          | `ebpf.enabled`                  | Enable eBPF support for Sysdig instead of `sysdig-probe` kernel module | `false`                                     |
          | `ebpf.settings.mountEtcVolume`  | Needed to detect which kernel version are running in Google COS        | `true`                                      |
          | `sysdig.accessKey`              | Your Sysdig Monitor Access Key                                         | `Nil` You must provide your own key         |
          | `sysdig.settings`               | Settings for agent's configuration file                                | `{}`                                        |
          | `secure.enabled`                | Enable Sysdig Secure                                                   | `false`                                     |
          | `customAppChecks`               | The custom app checks deployed with your agent                         | `{}`                                        |
          | `tolerations`                   | The tolerations for scheduling                                         | `node-role.kubernetes.io/master:NoSchedule` |

          For example, if you want to deploy a DaemonSet with eBPF and with Sysdig Secure
          enabled:

          ```yaml
          apiVersion: sysdig.com/v1alpha1
          kind: SysdigAgent
          metadata:
            name: agent-with-ebpf-and-secure
          spec:
            ebpf:
              enabled: true
            secure:
              enabled: true
            sysdig:
              accessKey: XXX
          ```

          Please, notice that `sysdig.accessKey` is **mandatory**. Once you have provided
          the accessKey, you can apply this file with `kubectl apply -f`

          ## Getting your Access Key

          To retrieve the key and use it in the agent:

          1. Log in to Sysdig Monitor or Sysdig Secure (maybe as administrator) and
             select **Settings**.

          2. Choose Agent Installation.

          3. Use the Copy button to copy the access key at the top of the page.

          If you need more help, you can read more about this process in the [Agent Installation: Overview and Key](
          https://sysdigdocs.atlassian.net/wiki/spaces/Platform/pages/213352719/Agent+Installation+Overview+and+Key)
          documentation page.

          ## Verify Metrics in Sysdig Monitor UI

          Once you have deployed the Sysdig Agent, it's time to verify that everything is
          working as expected. So, we are going to log in Sysdig Monitor to do the check.

          1. Access Sysdig Monitor:

             **SaaS**: https://app.sysdigcloud.com

             Log in with your Sysdig user name and password.

          2. Select the **Explore** tab to see if metrics are displayed.

          3. To verify that kube state metrics and cluster name are working correctly:

             Select the **Explore tab** and create a grouping by `kubernetes.cluster.name` and `kubernetes.pod.name`.

          4. Select an individual container or pod to see details.

          Don't rush about getting Kubernetes metadata. Pods, deployments ... appear a
          minute or two later than the nodes/containers themselves; if pod names do not
          appear immediately, wait and retry the Explore view.

          You can read more about verification in the [Verify Metrics in Sysdig Monitor UI section](https://sysdigdocs.atlassian.net/wiki/spaces/Platform/pages/256475257/GKE+Installation+Steps#GKEInstallationSteps-VerifyMetricsinSysdigMonitorUI)
          in the documentation pages.
        displayName: Sysdig Agent Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Sysdig
        version: 1.4.0
      name: stable
    defaultChannel: stable
    packageName: sysdig-certified
    provider:
      name: Sysdig
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: amq-online
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/amq-online
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: amqonline.1.3.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "admin.enmasse.io/v1beta1",
                "kind": "StandardInfraConfig",
                "metadata": {
                  "name": "default"
                },
                "spec": {
                  "broker": {
                    "resources": {
                      "memory": "1Gi",
                      "storage": "5Gi"
                    },
                    "addressFullPolicy": "FAIL"
                  },
                  "router": {
                    "linkCapacity": 50,
                    "resources": {
                      "memory": "512Mi"
                    }
                  }
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta1",
                "kind": "BrokeredInfraConfig",
                "metadata": {
                  "name": "default"
                },
                "spec": {
                  "broker": {
                    "resources": {
                      "memory": "4Gi"
                    }
                  }
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta2",
                "kind": "AddressPlan",
                "metadata": {
                  "name": "standard-small-queue"
                },
                "spec": {
                  "addressType": "queue",
                  "shortDescription": "Small Queue",
                  "resources": {
                    "router": 0.01,
                    "broker": 0.1
                  }
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta2",
                "kind": "AddressSpacePlan",
                "metadata": {
                  "name": "standard-small"
                },
                "spec": {
                  "addressSpaceType": "standard",
                  "infraConfigRef": "default",
                  "shortDescription": "Small Address Space Plan",
                  "resourceLimits": {
                    "router": 1.0,
                    "broker": 2.0,
                    "aggregate": 2.0
                  },
                  "addressPlans": [
                    "standard-small-queue"
                  ]
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta1",
                "kind": "AuthenticationService",
                "metadata": {
                  "name": "standard-authservice"
                },
                "spec": {
                  "type": "standard",
                  "standard": {
                    "storage": {
                      "claimName": "standard-authservice",
                      "deleteClaim": true,
                      "size": "1Gi",
                      "type": "persistent-claim"
                    }
                  }
                }
              },
              {
                "apiVersion": "enmasse.io/v1beta1",
                "kind": "AddressSpace",
                "metadata": {
                  "name": "myspace"
                },
                "spec": {
                  "plan": "standard-small",
                  "type": "standard"
                }
              },
              {
                "apiVersion": "enmasse.io/v1beta1",
                "kind": "Address",
                "metadata": {
                  "name": "myspace.myqueue"
                },
                "spec": {
                  "address": "myqueue",
                  "plan": "standard-small-queue",
                  "type": "queue"
                }
              },
              {
                "apiVersion": "user.enmasse.io/v1beta1",
                "kind": "MessagingUser",
                "metadata": {
                  "name": "myspace.user"
                },
                "spec": {
                  "authentication": {
                    "password": "ZW5tYXNzZQ==",
                    "type": "password"
                  },
                  "authorization": [
                    {
                      "addresses": [
                        "myqueue"
                      ],
                      "operations": [
                        "send",
                        "recv"
                      ]
                    }
                  ],
                  "username": "user"
                }
              },
              {
                "apiVersion": "admin.enmasse.io/v1beta1",
                "kind": "ConsoleService",
                "metadata": {
                  "name": "console"
                },
                "spec": {}
              },
              {
                "apiVersion": "iot.enmasse.io/v1alpha1",
                "kind": "IoTConfig",
                "metadata": {
                  "name": "default"
                },
                "spec": {
                  "adapters": {
                    "mqtt": {
                      "endpoint": {
                        "secretNameStrategy": {
                          "secretName": "iot-mqtt-adapter-tls"
                        }
                      }
                    }
                  },
                  "services": {
                    "deviceRegistry": {
                      "file": {
                        "numberOfDevicesPerTenant": 1000
                      }
                    }
                  }
                }
              },
              {
                "apiVersion": "enmasse.io/v1beta1",
                "kind": "AddressSpaceSchema",
                "metadata": {
                  "name": "undefined"
                },
                "spec": {}
              },
              {
                "apiVersion": "iot.enmasse.io/v1alpha1",
                "kind": "IoTProject",
                "metadata": {
                  "name": "iot"
                },
                "spec": {
                  "downstreamStrategy": {
                    "managedStrategy": {
                      "addressSpace": {
                        "name": "iot",
                        "plan": "standard-unlimited"
                      },
                      "addresses": {
                        "command": {
                          "plan": "standard-small-anycast"
                        },
                        "event": {
                          "plan": "standard-small-queue"
                        },
                        "telemetry": {
                          "plan": "standard-small-anycast"
                        }
                      }
                    }
                  }
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Streaming & Messaging
          certified: "false"
          containerImage: registry.redhat.io/amq7/amq-online-1-controller-manager:1.3
          createdAt: "2019-04-02"
          description: Red Hat AMQ Online provides messaging as a managed service
            on OpenShift
          repository: ""
          support: Red Hat, Inc.
        apiservicedefinitions:
          owned:
          - description: A group of messaging addresses that can be accessed via the
              same endpoint
            displayName: Address Space
            group: enmasse.io
            kind: AddressSpace
            name: addressspaces
            version: v1beta1
          - description: A messaging address that can be used to send/receive messages
              to/from
            displayName: Address
            group: enmasse.io
            kind: Address
            name: addresses
            version: v1beta1
          - description: A resource representing the available schema of plans and
              authentication services
            displayName: AddressSpaceSchema
            group: enmasse.io
            kind: AddressSpaceSchema
            name: addressspaceschemas
            version: v1beta1
          - description: A messaging user that can connect to an Address Space
            displayName: Messaging User
            group: user.enmasse.io
            kind: MessagingUser
            name: messagingusers
            version: v1beta1
        customresourcedefinitions:
          owned:
          - description: Infrastructure configuration template for the standard address
              space type
            displayName: Standard Infra Config
            kind: StandardInfraConfig
            name: standardinfraconfigs.admin.enmasse.io
            version: v1beta1
          - description: Infrastructure configuration template for the brokered address
              space type
            displayName: Brokered Infra Config
            kind: BrokeredInfraConfig
            name: brokeredinfraconfigs.admin.enmasse.io
            version: v1beta1
          - description: Plan describing the resource usage of a given address type
            displayName: Address Plan
            kind: AddressPlan
            name: addressplans.admin.enmasse.io
            version: v1beta2
          - description: Plan describing the capabilities and resource limits of a
              given address space type
            displayName: Address Space Plan
            kind: AddressSpacePlan
            name: addressspaceplans.admin.enmasse.io
            version: v1beta2
          - description: Authentication service configuration available to address
              spaces.
            displayName: Authentication Service
            kind: AuthenticationService
            name: authenticationservices.admin.enmasse.io
            version: v1beta1
          - description: Console Service Singleton for deploying global console.
            displayName: Console Service
            kind: ConsoleService
            name: consoleservices.admin.enmasse.io
            version: v1beta1
          - description: IoT Infrastructure Configuration Singleton
            displayName: IoT Config
            kind: IoTConfig
            name: iotconfigs.iot.enmasse.io
            version: v1alpha1
          - description: An IoT project instance
            displayName: IoT Project
            kind: IoTProject
            name: iotprojects.iot.enmasse.io
            version: v1alpha1
        description: "**Red Hat AMQ Online** provides messaging as a managed service
          on OpenShift.\nWith AMQ Online, administrators can configure a cloud-native,\nmulti-tenant
          messaging service either in the cloud or on premise.\nDevelopers can provision
          messaging using the Red Hat AMQ Online Console.\nMultiple development teams
          can provision the brokers and queues from the\nconsole, without requiring
          each team to install, configure, deploy,\nmaintain, or patch any software.
          \n\n**The core capabilities include:**\n\n  * **Built-in authentication
          and authorization** - Use the built-in authentication service or\n    plug
          in your own authentication service for authentication and\n    authorization
          of messaging clients.\n\n  * **Self-service messaging for applications**
          - The service administrator deploys\n    and manages the messaging infrastructure,
          while applications can request\n    messaging resources regardless of the
          messaging infrastructure.\n\n  * **Support for a wide variety of messaging
          patterns** - Choose between\n    JMS-style messaging with strict guarantees,
          or messaging that supports\n    a larger number of connections and higher
          throughput.\n\n## Post-installation tasks\n\nTo fully use AMQ Online, you
          need to create a few\ninfrastructure components after the installation,
          including:\n\n  * An authentication service\n  * Infrastructure configuration
          for the standard and brokered address space\n  * Address and address space
          plans\n  * (Optional) Create RBAC roles to allow users to self-manage addresses
          and\n    address spaces.\n\nFor a complete overview of how to configure
          AMQ Online, see\n[Configuring AMQ Online](https://access.redhat.com/documentation/en-us/red_hat_amq/7.5/html-single/installing_and_managing_amq_online_on_openshift_container_platform/index#configuring-messaging).\n\n###
          Quickstart infrastructure configuration\n\nIf you simply want to get started
          quickly, you can import the following\nYAML by saving the content to a file
          and apply it by running the \n`oc apply -f <file>` command. You can also
          split the content (at the `---` marker)\nand import the single YAML document
          using the Web UI: \n\n~~~yaml\n---\napiVersion: admin.enmasse.io/v1beta1\nkind:
          StandardInfraConfig\nmetadata:\n  name: default\n---\napiVersion: admin.enmasse.io/v1beta2\nkind:
          AddressPlan\nmetadata:\n  name: standard-small-queue\nspec:\n  addressType:
          queue\n  resources:\n    router: 0.01\n    broker: 0.1\n---\napiVersion:
          admin.enmasse.io/v1beta2\nkind: AddressSpacePlan\nmetadata:\n  name: standard-small\nspec:\n
          \ addressSpaceType: standard\n  infraConfigRef: default\n  addressPlans:\n
          \ - standard-small-queue\n  resourceLimits:\n    router: 2.0\n    broker:
          3.0\n    aggregate: 4.0\n---\napiVersion: admin.enmasse.io/v1beta1\nkind:
          AuthenticationService\nmetadata:\n  name: standard-authservice\nspec:\n
          \ type: standard\n  standard:\n    storage:\n      claimName: standard-authservice\n
          \     deleteClaim: true\n      size: 1Gi\n      type: persistent-claim\n~~~\n\n###
          Creating infrastructure using the Web UI\n\nYou must create a new instance
          of each of the following custom resources. You can\nuse the example data
          directly, which is provided when using the\nWeb UI:\n\n  * *Authentication
          Service* â€“ Create an authentication service.\n  * *Brokered Infra Config*
          â€“ Create the broker infrastructure configuration.\n  * *Standard Infra Config*
          â€“ Create the standard infrastructure\n    configuration.\n\nYou must also
          create at least one address space plan and one address plan.\n\n*Note*:
          The name of the address space plan and address plan is required\nlater when
          creating new addresses. Some examples use specific plan\nnames, which might
          not be available in your environment. You can\ncreate those plans, or edit
          the examples to use different plan names.\n\n## Configuration for messaging
          tenants\n\nWhile service administrators perform the infrastructure configuration,
          the following\nresources are for the actual users of the system, the messaging
          tenants.\n\nYou need to create those resources to satisfy your particular
          use case.\n\n  * *Address space* â€“ A container for addresses\n  * *Address*
          â€“ A messaging address (address, topic, queue, and so on)\n  * *Messaging
          user* â€“ Manages access to an address\n\n## Enabling the IoT messaging layer
          (tech-preview)\n\nAMQ Online contains an IoT messaging layer, which is not
          enabled\nby default. You may enable it by creating an `IoTConfig` resource,
          in the\nnamespace of the AMQ Online installation. The name of the resource\nmust
          be `default`.\n\nYou can then create resources of the type `IoTProject`,
          which allow you to\nregister and connect devices via HTTP, MQTT, Sigfox
          and LoRaWAN.\n\n*Note:* The default `IoTConfig` example uses the \"file
          based\" device registry.\nIt allows you to quicky evaluate the IoT layer.
          However you must not use this\nin any productive environment.\n\n*Note:*
          The default `IoTConfig` examples configures the MQTT adapter with an\nexplicit
          TLS key/certificate stored in the secret `iot-mqtt-adapter-tls`. You\nmust
          provide this secret, under the configured name, in order for the\nMQTT protocol
          adapter to pick it up.\n\n*Note*: The `IoTProject` examples assume that
          you have deployed the example\nplans. If you did not deploy the example
          plans, or changed their names, you\nmust adapt the names in the IoT examples
          as well.\n\nFor more information on the IoT layer, see: [Getting Started
          with Internet of Things (IoT) on AMQ Online](${EVAL_IOT_DOC_URL})\n"
        displayName: AMQ Online
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 1.3.0
      name: stable
    defaultChannel: stable
    packageName: amq-online
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: The Apache Software Foundation
      provider-url: ""
    name: camel-k
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/camel-k
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: camel-k-operator.v1.0.0-rc1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "camel.apache.org/v1",
              "kind": "IntegrationPlatform",
              "metadata": {
                "name": "example"
              },
              "spec": {}
            },
            {
              "apiVersion": "camel.apache.org/v1",
              "kind": "Integration",
              "metadata": {
                "name": "example"
              },
              "spec": {
                "sources": [
                  {
                    "content": "from('timer:groovy?period=1s')\n  .setBody().simple('Hello world from Camel K!')\n  .to('log:info?showAll=false')",
                    "name": "example.groovy"
                  }
                ]
              }
            },
            {
              "apiVersion": "camel.apache.org/v1",
              "kind": "IntegrationKit",
              "metadata": {
                "name": "example"
              },
              "spec": {}
            },
            {
              "apiVersion": "camel.apache.org/v1",
              "kind": "CamelCatalog",
              "metadata": {
                "name": "example"
              },
              "spec": {}
            },
            {
              "apiVersion": "camel.apache.org/v1",
              "kind": "Build",
              "metadata": {
                "name": "example"
              },
              "spec": {}
            }]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: docker.io/apache/camel-k:1.0.0-RC1
          createdAt: "2019-07-26T02:45:00Z"
          description: Apache Camel K is a lightweight integration platform, born
            on Kubernetes, with serverless superpowers.
          repository: https://github.com/apache/camel-k
          support: Camel
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Camel K build
            displayName: Build
            kind: Build
            name: builds.camel.apache.org
            version: v1
          - description: A Camel catalog
            displayName: Camel Catalog
            kind: CamelCatalog
            name: camelcatalogs.camel.apache.org
            version: v1
          - description: A Camel K integration
            displayName: Integration
            kind: Integration
            name: integrations.camel.apache.org
            version: v1
          - description: A Camel K integration kit
            displayName: Integration Kit
            kind: IntegrationKit
            name: integrationkits.camel.apache.org
            version: v1
          - description: A Camel K integration platform
            displayName: Integration Platform
            kind: IntegrationPlatform
            name: integrationplatforms.camel.apache.org
            version: v1
        description: |
          Apache Camel K
          ==============

          Apache Camel K is a lightweight integration platform, born on Kubernetes, with serverless superpowers.

          ## Installation

          To start using Camel K, install the operator and then create the following `IntegrationPlatform`:
          ```
          apiVersion: camel.apache.org/v1
          kind: IntegrationPlatform
          metadata:
            name: camel-k
            labels:
              app: "camel-k"
          ```

          ## Running an Integration

          After the initial setup, you can run a Camel integration on the cluster by creating an example `Integration`:
          ```
          apiVersion: camel.apache.org/v1
          kind: Integration
          metadata:
            name: example
          spec:
            sources:
            - content: |
                import org.apache.camel.builder.RouteBuilder;

                public class Example extends RouteBuilder {
                    @Override
                    public void configure() throws Exception {
                        from("timer:tick")
                            .setBody(constant("Hello World!"))
                        .to("log:info?skipBodyLineSeparator=false");
                    }
                }
            name: Example.java
          ```
        displayName: Camel K Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: The Apache Software Foundation
        version: 1.0.0-rc1
      name: alpha
    defaultChannel: alpha
    packageName: camel-k
    provider:
      name: The Apache Software Foundation
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: grafana-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/grafana-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: grafana-operator.v2.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
            {
              "apiVersion": "integreatly.org/v1alpha1",
              "kind": "Grafana",
              "metadata": {
                "name": "example-grafana"
              },
              "spec": {
                "ingress": {
                  "enabled": true
                },
                "config": {
                  "auth": {
                    "disable_signout_menu": true
                  },
                  "auth.anonymous": {
                    "enabled": true
                  },
                  "log": {
                    "level": "warn",
                    "mode": "console"
                  },
                  "security": {
                    "admin_password": "secret",
                    "admin_user": "root"
                  }
                },
                "dashboardLabelSelector": [
                {
                  "matchExpressions": [
                  {
                    "key": "app",
                    "operator": "In",
                    "values": [
                      "grafana"
                    ]
                  }
                  ]
                }
                ]
              }
            },
            {
              "apiVersion": "integreatly.org/v1alpha1",
              "kind": "GrafanaDashboard",
              "metadata": {
                "labels": {
                  "app": "grafana"
                },
                "name": "simple-dashboard"
              },
              "spec": {
                "json": "{\n  \"id\": null,\n  \"title\": \"Simple Dashboard\",\n  \"tags\": [],\n  \"style\": \"dark\",\n  \"timezone\": \"browser\",\n  \"editable\": true,\n  \"hideControls\": false,\n  \"graphTooltip\": 1,\n  \"panels\": [],\n  \"time\": {\n    \"from\": \"now-6h\",\n    \"to\": \"now\"\n  },\n  \"timepicker\": {\n    \"time_options\": [],\n    \"refresh_intervals\": []\n  },\n  \"templating\": {\n    \"list\": []\n  },\n  \"annotations\": {\n    \"list\": []\n  },\n  \"refresh\": \"5s\",\n  \"schemaVersion\": 17,\n  \"version\": 0,\n  \"links\": []\n}\n",
                "name": "simple-dashboard.json"
              }
            },
            {
              "apiVersion": "integreatly.org/v1alpha1",
              "kind": "GrafanaDataSource",
              "metadata": {
                "name": "example-grafanadatasource"
              },
              "spec": {
                "datasources": [
                {
                  "access": "proxy",
                  "editable": true,
                  "isDefault": true,
                  "jsonData": {
                    "timeInterval": "5s"
                  },
                  "name": "Prometheus",
                  "type": "prometheus",
                  "url": "http://prometheus-service:9090",
                  "version": 1
                }
                ],
                "name": "example-datasources.yaml"
              }
            }
            ]
          capabilities: Basic Install
          categories: Monitoring
          certified: "False"
          containerImage: quay.io/integreatly/grafana-operator:v2.0.0
          createdAt: "2019-07-23 00:00:00"
          description: An Operator for managing Grafana instances, dashboards and
            data sources
          repository: https://github.com/integr8ly/grafana-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Grafana Instance
            displayName: Grafana
            kind: Grafana
            name: grafanas.integreatly.org
            version: v1alpha1
          - description: Represents a Grafana Dashboard
            displayName: Grafana Dashboard
            kind: GrafanaDashboard
            name: grafanadashboards.integreatly.org
            version: v1alpha1
          - description: Represents a Grafana Data Source
            displayName: Grafana Data Source
            kind: GrafanaDataSource
            name: grafanadatasources.integreatly.org
            version: v1alpha1
        description: |
          A Kubernetes Operator based on the Operator SDK for creating and managing Grafana instances.

          Grafana is an open platform for beautiful analytics and monitoring. For more information please visit the [Grafana website](https://grafana.com)

          # Current status

          The Operator can deploy and manage a Grafana instance on Kubernetes and OpenShift. The following features are supported:

          * Install Grafana to a namespace
          * Configure Grafana through the custom resource
          * Import Grafana dashboards from the same or other namespaces
          * Import Grafana data sources from the same namespace
          * Install Plugins (panels)
        displayName: Grafana Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 2.0.0
      name: alpha
    defaultChannel: alpha
    packageName: grafana-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Containous
      provider-url: ""
    name: traefikee-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/traefikee-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: traefikee-certified.v0.4.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "containo.us/v1alpha1",
                "kind": "Traefikee",
                "metadata": {
                  "name": "example-traefikee",
                  "namespace": "traefikee"
                },
                "spec": {
                  "cluster": "traefikee",
                  "controllers": 1,
                  "image": "store/containous/traefikee:v2.0.0",
                  "proxies": 2
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "true"
          containerImage: containous/traefikee-operator:latest
          createdAt: "2019-12-12T09:01:00Z"
          description: Traefik Enterprise Edition
          repository: https://github.com/containous/traefikee-operator
          support: https://docs.containo.us
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a TraefikEE installation
            displayName: TraefikEE
            kind: Traefikee
            name: traefikees.containo.us
            version: v1alpha1
        description: |
          TraefikEE is a distributed, and highly available edge routing solution built on top of the open source Traefik and natively integrates with Openshift to:

          * Load balance any applications and easily scale out to meet production traffic needs
          * Secure services with end-to-end network and application encryption
          * Provide end-to-end monitoring and real-time tracing for better insight into application uptime and performance

          ## Before You Start

          To start using the operator you''ll need a license key

          Request your [30-days free trial](https://containo.us/traefikee)

          * Create a Secret with your License key
          ```
          kubectl create namespace traefikee
          kubectl create -n traefikee secret generic license --from-literal=license=${TRAEFIKEE_LICENSE_KEY}
          ```

          * Generate the client credentials
          ```
          export CLUSTER=test
          teectl setup --cluster="${CLUSTER}" --kubernetes --force
          kubectl create secret -n traefikee generic "${CLUSTER}-mtls" --from-file=bundle.zip=./bundle.zip
          kubectl label secret -n traefikee "${CLUSTER}-mtls" app=traefikee
          kubectl label secret -n traefikee "${CLUSTER}-mtls" release="${CLUSTER}"
          ```
        displayName: Traefikee Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Containous
        version: 0.4.1
      name: alpha
    defaultChannel: alpha
    packageName: traefikee-certified
    provider:
      name: Containous
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: metering-ocp
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/metering-ocp
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: metering-operator.4.2.18-202002031516
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "MeteringConfig",
                "metadata": {
                  "name": "operator-metering"
                },
                "spec": {
                  "storage": {
                    "hive": {
                      "s3": {
                        "bucket": "bucketname/path/",
                        "createBucket": true,
                        "region": "us-west-1",
                        "secretName": "my-aws-secret"
                      },
                      "type": "s3"
                    },
                    "type": "hive"
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "Report",
                "metadata": {
                  "name": "unready-deployment-replicas-hourly"
                },
                "spec": {
                  "query": "unready-deployment-replicas",
                  "schedule": {
                    "period": "hourly"
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "ReportQuery",
                "metadata": {
                  "name": "unready-deployment-replicas"
                },
                "spec": {
                  "columns": [
                    {
                      "name": "period_start",
                      "type": "timestamp"
                    },
                    {
                      "name": "period_end",
                      "type": "timestamp"
                    },
                    {
                      "name": "namespace",
                      "type": "varchar"
                    },
                    {
                      "name": "deployment",
                      "type": "varchar"
                    },
                    {
                      "name": "total_replica_unready_seconds",
                      "type": "double"
                    },
                    {
                      "name": "avg_replica_unready_seconds",
                      "type": "double"
                    }
                  ],
                  "inputs": [
                    {
                      "name": "ReportingStart",
                      "type": "time"
                    },
                    {
                      "name": "ReportingEnd",
                      "type": "time"
                    },
                    {
                      "default": "unready-deployment-replicas",
                      "name": "UnreadyDeploymentReplicasDataSourceName",
                      "type": "ReportDataSource"
                    }
                  ],
                  "query": "SELECT\n    timestamp '{| default .Report.ReportingStart .Report.Inputs.ReportingStart | prestoTimestamp |}' AS period_start,\n    timestamp '{| default .Report.ReportingEnd .Report.Inputs.ReportingEnd | prestoTimestamp |}' AS period_end,\n    labels['namespace'] AS namespace,\n    labels['deployment'] AS deployment,\n    sum(amount * \"timeprecision\") AS total_replica_unready_seconds,\n    avg(amount * \"timeprecision\") AS avg_replica_unready_seconds\nFROM {| dataSourceTableName .Report.Inputs.UnreadyDeploymentReplicasDataSourceName |}\nWHERE \"timestamp\" >= timestamp '{| default .Report.ReportingStart .Report.Inputs.ReportingStart | prestoTimestamp |}'\nAND \"timestamp\" < timestamp '{| default .Report.ReportingEnd .Report.Inputs.ReportingEnd | prestoTimestamp |}'\nGROUP BY labels['namespace'], labels['deployment']\nORDER BY total_replica_unready_seconds DESC, avg_replica_unready_seconds DESC, namespace ASC, deployment ASC\n"
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "ReportDataSource",
                "metadata": {
                  "name": "unready-deployment-replicas"
                },
                "spec": {
                  "prometheusMetricsImporter": {
                    "query": "sum(kube_deployment_status_replicas_unavailable) by (namespace, deployment)\n"
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "StorageLocation",
                "metadata": {
                  "name": "s3-storage-example"
                },
                "spec": {
                  "hive": {
                    "databaseName": "metering-s3",
                    "location": "s3a://bucketName/pathInBucket",
                    "unmanagedDatabase": true
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "PrestoTable",
                "metadata": {
                  "name": "example-baremetal-cost"
                },
                "spec": {
                  "catalog": "hive",
                  "columns": [
                    {
                      "name": "cost_per_gigabyte_hour",
                      "type": "double"
                    },
                    {
                      "name": "cost_per_cpu_hour",
                      "type": "double"
                    },
                    {
                      "name": "currency",
                      "type": "varchar"
                    }
                  ],
                  "createTableAs": true,
                  "query": "SELECT * FROM (\n  VALUES (10.00, 50.00, 'USD')\n) AS t (cost_per_gigabyte_hour, cost_per_cpu_hour, currency)\n",
                  "schema": "default",
                  "tableName": "example_baremetal_cost"
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "HiveTable",
                "metadata": {
                  "name": "apache-log",
                  "annotations": {
                    "reference": "based on the RegEx example from https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&SerDe"
                  }
                },
                "spec": {
                  "columns": [
                    {
                      "name": "host",
                      "type": "string"
                    },
                    {
                      "name": "identity",
                      "type": "string"
                    },
                    {
                      "name": "user",
                      "type": "string"
                    },
                    {
                      "name": "time",
                      "type": "string"
                    },
                    {
                      "name": "request",
                      "type": "string"
                    },
                    {
                      "name": "status",
                      "type": "string"
                    },
                    {
                      "name": "size",
                      "type": "string"
                    },
                    {
                      "name": "referer",
                      "type": "string"
                    },
                    {
                      "name": "agent",
                      "type": "string"
                    }
                  ],
                  "databaseName": "default",
                  "external": true,
                  "fileFormat": "TEXTFILE",
                  "location": "s3a://my-bucket/apache_logs",
                  "rowFormat": "SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\nWITH SERDEPROPERTIES (\n  \"input.regex\" = \"([^ ]*) ([^ ]*) ([^ ]*) (-|\\\\[[^\\\\]]*\\\\]) ([^ \\\"]*|\\\"[^\\\"]*\\\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\"]*|\\\"[^\\\"]*\\\") ([^ \\\"]*|\\\"[^\\\"]*\\\"))?\"\n)\n",
                  "tableName": "apache_log"
                }
              }
            ]
          capabilities: Basic Install
          categories: OpenShift Optional, Monitoring
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-metering-ansible-operator@sha256:d60c2eaa846b8f9de82cc7ab72d6c0b990ad688675da50ab96d6067fc8db8c22
          createdAt: "2019-01-01T11:59:59Z"
          description: Chargeback and reporting tool to provide accountability for
            how resources are used across a cluster
          repository: https://github.com/operator-framework/operator-metering
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An instance of Metering with high-level configuration
            displayName: Metering Configuration
            kind: MeteringConfig
            name: meteringconfigs.metering.openshift.io
            version: v1
          - description: A scheduled or on-off Metering Report summarizes data based
              on the query specified.
            displayName: Metering Report
            kind: Report
            name: reports.metering.openshift.io
            version: v1
          - description: A SQL query used by Metering to generate reports.
            displayName: Metering Report Query
            kind: ReportQuery
            name: reportqueries.metering.openshift.io
            version: v1
          - description: Used under-the-hood. A resource representing a database table
              in Presto. Used by ReportQueries to determine what tables exist, and
              by the HTTP API to determine how to query a table.
            displayName: Metering Data Source
            kind: ReportDataSource
            name: reportdatasources.metering.openshift.io
            version: v1
          - description: Represents a configurable storage location for Metering to
              store metering and report data.
            displayName: Metering Storage Location
            kind: StorageLocation
            name: storagelocations.metering.openshift.io
            version: v1
          - description: Used under-the-hood. A resource describing a source of data
              for usage by Report Queries.
            displayName: Metering Presto Table
            kind: PrestoTable
            name: prestotables.metering.openshift.io
            version: v1
          - description: Used under-the-hood. A resource representing a database table
              in Hive.
            displayName: Metering Hive Table
            kind: HiveTable
            name: hivetables.metering.openshift.io
            version: v1
        description: |
          Operator Metering is a chargeback and reporting tool to provide accountability for how resources are used across a cluster. Cluster admins can schedule reports based on historical usage data by Pod, Namespace, and Cluster wide. Operator Metering is part of the [Operator Framework](https://coreos.com/blog/introducing-operator-framework-metering).

          Read the user guide for more details on [running and viewing your first report](https://docs.openshift.com/container-platform/4.2/metering/metering-using-metering.html).

          ### Core capabilities

          * **Chargeback/Showback** - Break down the reserved and utlized resources requested by applications.

          * **Pod, Namespace & Cluster Reports** - Built in reports exist to break down CPU and RAM in any way you desire.

          * **Scheduled Reports** - Schedule reports to run on a standard interval, eg. daily or monthly

          * **Post-Processing** - Reports are generated in CSV format and stored in persistent storage for further post-processing. Use this to send reminder emails, integrate into your ERP system, or graph on a dashboard.

          * **HTTP API** - Reports can be queried from an in-cluster HTTP API in addition to reading from persistent storage.

          ### Before you start

          Metering runs a big data stack to crunch your data and requires at least 4GB of RAM and 4 CPU cores.
          At least one Node should have 2GB of RAM and 2 CPU cores.
          Memory and CPU consumption may often be lower, but will spike when running reports, or collecting data for larger clusters.

          Metering requires configuring storage, please review the [configuring persistent storage documentation](https://docs.openshift.com/container-platform/4.2/metering/configuring-metering/metering-configure-persistent-storage.html) before proceeding.

          ### Common Configurations

          * **Store data in object storage or in a PersistentVolume** - Store your report output [in an object storage bucket or in a PersistentVolume](https://docs.openshift.com/container-platform/4.2/metering/configuring-metering/metering-configure-persistent-storage.html).

          * **Configure AWS Billing Data Source** - Assign Pod $$ costs on using your [AWS billing reports stored in S3](https://docs.openshift.com/container-platform/4.2/metering/configuring-metering/metering-configure-aws-billing-correlation.html).

          * **Use Reports** - Customize what how you process data. [Specify what you want to report on, set the schedule, and reporting time period](https://docs.openshift.com/container-platform/4.2/metering/reports/metering-about-reports.html#metering-reports_metering-about-reports).
        displayName: Metering
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.2.18-202002031516
      name: "4.2"
    - currentCSV: metering-operator.4.3.2-202002112006
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "MeteringConfig",
                "metadata": {
                  "name": "operator-metering"
                },
                "spec": {
                  "storage": {
                    "hive": {
                      "s3": {
                        "bucket": "bucketname/path/",
                        "createBucket": true,
                        "region": "us-west-1",
                        "secretName": "my-aws-secret"
                      },
                      "type": "s3"
                    },
                    "type": "hive"
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "Report",
                "metadata": {
                  "name": "unready-deployment-replicas-hourly"
                },
                "spec": {
                  "query": "unready-deployment-replicas",
                  "schedule": {
                    "period": "hourly"
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "ReportQuery",
                "metadata": {
                  "name": "unready-deployment-replicas"
                },
                "spec": {
                  "columns": [
                    {
                      "name": "period_start",
                      "type": "timestamp"
                    },
                    {
                      "name": "period_end",
                      "type": "timestamp"
                    },
                    {
                      "name": "namespace",
                      "type": "varchar"
                    },
                    {
                      "name": "deployment",
                      "type": "varchar"
                    },
                    {
                      "name": "total_replica_unready_seconds",
                      "type": "double"
                    },
                    {
                      "name": "avg_replica_unready_seconds",
                      "type": "double"
                    }
                  ],
                  "inputs": [
                    {
                      "name": "ReportingStart",
                      "type": "time"
                    },
                    {
                      "name": "ReportingEnd",
                      "type": "time"
                    },
                    {
                      "default": "unready-deployment-replicas",
                      "name": "UnreadyDeploymentReplicasDataSourceName",
                      "type": "ReportDataSource"
                    }
                  ],
                  "query": "SELECT\n    timestamp '{| default .Report.ReportingStart .Report.Inputs.ReportingStart | prestoTimestamp |}' AS period_start,\n    timestamp '{| default .Report.ReportingEnd .Report.Inputs.ReportingEnd | prestoTimestamp |}' AS period_end,\n    labels['namespace'] AS namespace,\n    labels['deployment'] AS deployment,\n    sum(amount * \"timeprecision\") AS total_replica_unready_seconds,\n    avg(amount * \"timeprecision\") AS avg_replica_unready_seconds\nFROM {| dataSourceTableName .Report.Inputs.UnreadyDeploymentReplicasDataSourceName |}\nWHERE \"timestamp\" >= timestamp '{| default .Report.ReportingStart .Report.Inputs.ReportingStart | prestoTimestamp |}'\nAND \"timestamp\" < timestamp '{| default .Report.ReportingEnd .Report.Inputs.ReportingEnd | prestoTimestamp |}'\nGROUP BY labels['namespace'], labels['deployment']\nORDER BY total_replica_unready_seconds DESC, avg_replica_unready_seconds DESC, namespace ASC, deployment ASC\n"
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "ReportDataSource",
                "metadata": {
                  "name": "unready-deployment-replicas"
                },
                "spec": {
                  "prometheusMetricsImporter": {
                    "query": "sum(kube_deployment_status_replicas_unavailable) by (namespace, deployment)\n"
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "StorageLocation",
                "metadata": {
                  "name": "s3-storage-example"
                },
                "spec": {
                  "hive": {
                    "databaseName": "metering-s3",
                    "location": "s3a://bucketName/pathInBucket",
                    "unmanagedDatabase": true
                  }
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "PrestoTable",
                "metadata": {
                  "name": "example-baremetal-cost"
                },
                "spec": {
                  "catalog": "hive",
                  "columns": [
                    {
                      "name": "cost_per_gigabyte_hour",
                      "type": "double"
                    },
                    {
                      "name": "cost_per_cpu_hour",
                      "type": "double"
                    },
                    {
                      "name": "currency",
                      "type": "varchar"
                    }
                  ],
                  "createTableAs": true,
                  "query": "SELECT * FROM (\n  VALUES (10.00, 50.00, 'USD')\n) AS t (cost_per_gigabyte_hour, cost_per_cpu_hour, currency)\n",
                  "schema": "default",
                  "tableName": "example_baremetal_cost"
                }
              },
              {
                "apiVersion": "metering.openshift.io/v1",
                "kind": "HiveTable",
                "metadata": {
                  "name": "apache-log",
                  "annotations": {
                    "reference": "based on the RegEx example from https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL#LanguageManualDDL-RowFormats&SerDe"
                  }
                },
                "spec": {
                  "columns": [
                    {
                      "name": "host",
                      "type": "string"
                    },
                    {
                      "name": "identity",
                      "type": "string"
                    },
                    {
                      "name": "user",
                      "type": "string"
                    },
                    {
                      "name": "time",
                      "type": "string"
                    },
                    {
                      "name": "request",
                      "type": "string"
                    },
                    {
                      "name": "status",
                      "type": "string"
                    },
                    {
                      "name": "size",
                      "type": "string"
                    },
                    {
                      "name": "referer",
                      "type": "string"
                    },
                    {
                      "name": "agent",
                      "type": "string"
                    }
                  ],
                  "databaseName": "default",
                  "external": true,
                  "fileFormat": "TEXTFILE",
                  "location": "s3a://my-bucket/apache_logs",
                  "rowFormat": "SERDE 'org.apache.hadoop.hive.serde2.RegexSerDe'\nWITH SERDEPROPERTIES (\n  \"input.regex\" = \"([^ ]*) ([^ ]*) ([^ ]*) (-|\\\\[[^\\\\]]*\\\\]) ([^ \\\"]*|\\\"[^\\\"]*\\\") (-|[0-9]*) (-|[0-9]*)(?: ([^ \\\"]*|\\\"[^\\\"]*\\\") ([^ \\\"]*|\\\"[^\\\"]*\\\"))?\"\n)\n",
                  "tableName": "apache_log"
                }
              }
            ]
          capabilities: Basic Install
          categories: OpenShift Optional, Monitoring
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-metering-ansible-operator@sha256:f4dfe214ef3cde2d62f58589dfd27239c2c48e6540d2e02dd4eee666ad3b0254
          createdAt: "2019-01-01T11:59:59Z"
          description: Chargeback and reporting tool to provide accountability for
            how resources are used across a cluster
          repository: https://github.com/operator-framework/operator-metering
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An instance of Metering with high-level configuration
            displayName: Metering Configuration
            kind: MeteringConfig
            name: meteringconfigs.metering.openshift.io
            version: v1
          - description: A scheduled or on-off Metering Report summarizes data based
              on the query specified.
            displayName: Metering Report
            kind: Report
            name: reports.metering.openshift.io
            version: v1
          - description: A SQL query used by Metering to generate reports.
            displayName: Metering Report Query
            kind: ReportQuery
            name: reportqueries.metering.openshift.io
            version: v1
          - description: Used under-the-hood. A resource representing a database table
              in Presto. Used by ReportQueries to determine what tables exist, and
              by the HTTP API to determine how to query a table.
            displayName: Metering Data Source
            kind: ReportDataSource
            name: reportdatasources.metering.openshift.io
            version: v1
          - description: Represents a configurable storage location for Metering to
              store metering and report data.
            displayName: Metering Storage Location
            kind: StorageLocation
            name: storagelocations.metering.openshift.io
            version: v1
          - description: Used under-the-hood. A resource describing a source of data
              for usage by Report Queries.
            displayName: Metering Presto Table
            kind: PrestoTable
            name: prestotables.metering.openshift.io
            version: v1
          - description: Used under-the-hood. A resource representing a database table
              in Hive.
            displayName: Metering Hive Table
            kind: HiveTable
            name: hivetables.metering.openshift.io
            version: v1
        description: |
          Operator Metering is a chargeback and reporting tool to provide accountability for how resources are used across a cluster. Cluster admins can schedule reports based on historical usage data by Pod, Namespace, and Cluster wide. Operator Metering is part of the [Operator Framework](https://coreos.com/blog/introducing-operator-framework-metering).

          Read the user guide for more details on [running and viewing your first report](https://docs.openshift.com/container-platform/4.3/metering/metering-using-metering.html).

          ### Core capabilities

          * **Chargeback/Showback** - Break down the reserved and utlized resources requested by applications.

          * **Pod, Namespace & Cluster Reports** - Built in reports exist to break down CPU and RAM in any way you desire.

          * **Scheduled Reports** - Schedule reports to run on a standard interval, eg. daily or monthly

          * **Post-Processing** - Reports are generated in CSV format and stored in persistent storage for further post-processing. Use this to send reminder emails, integrate into your ERP system, or graph on a dashboard.

          * **HTTP API** - Reports can be queried from an in-cluster HTTP API in addition to reading from persistent storage.

          ### Before you start

          Metering runs a big data stack to crunch your data and requires at least 4GB of RAM and 4 CPU cores.
          At least one Node should have 2GB of RAM and 2 CPU cores.
          Memory and CPU consumption may often be lower, but will spike when running reports, or collecting data for larger clusters.

          Metering requires configuring storage, please review the [configuring persistent storage documentation](https://docs.openshift.com/container-platform/4.3/metering/configuring-metering/metering-configure-persistent-storage.html) before proceeding.

          ### Common Configurations

          * **Store data in object storage or in a PersistentVolume** - Store your report output [in an object storage bucket or in a PersistentVolume](https://docs.openshift.com/container-platform/4.3/metering/configuring-metering/metering-configure-persistent-storage.html).

          * **Configure AWS Billing Data Source** - Assign Pod $$ costs on using your [AWS billing reports stored in S3](https://docs.openshift.com/container-platform/4.3/metering/configuring-metering/metering-configure-aws-billing-correlation.html).

          * **Use Reports** - Customize what how you process data. [Specify what you want to report on, set the schedule, and reporting time period](https://docs.openshift.com/container-platform/4.3/metering/reports/metering-about-reports.html#metering-reports_metering-about-reports).
        displayName: Metering
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.3.2-202002112006
      name: "4.3"
    defaultChannel: "4.3"
    packageName: metering-ocp
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Disposable Zone
      provider-url: ""
    name: argocd-operator-helm
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/argocd-operator-helm
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: argocd-operator-helm.v0.0.3
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "argoproj.io/v1alpha1",
                "kind": "ArgoCD",
                "metadata": {
                  "name": "argocd",
                  "namespace": "argocd"
                },
                "spec": {
                  "nameOverride": "argocd",
                  "fullnameOverride": "",
                  "installCRDs": false,
                  "global": {
                    "image": {
                      "repository": "argoproj/argocd",
                      "tag": "v1.4.0",
                      "imagePullPolicy": "IfNotPresent"
                    },
                    "securityContext": {
                    }
                  },
                  "controller": {
                    "name": "application-controller",
                    "image": {
                      "repository": null,
                      "tag": null,
                      "imagePullPolicy": null
                    },
                    "args": {
                      "statusProcessors": "20",
                      "operationProcessors": "10"
                    },
                    "logLevel": "info",
                    "extraArgs": {
                    },
                    "env": [

                    ],
                    "podAnnotations": {
                    },
                    "podLabels": {
                    },
                    "containerSecurityContext": {
                    },
                    "containerPort": 8082,
                    "readinessProbe": {
                      "failureThreshold": 3,
                      "initialDelaySeconds": 10,
                      "periodSeconds": 10,
                      "successThreshold": 1,
                      "timeoutSeconds": 1
                    },
                    "livenessProbe": {
                      "failureThreshold": 3,
                      "initialDelaySeconds": 10,
                      "periodSeconds": 10,
                      "successThreshold": 1,
                      "timeoutSeconds": 1
                    },
                    "volumeMounts": [

                    ],
                    "volumes": [

                    ],
                    "service": {
                      "annotations": {
                      },
                      "labels": {
                      },
                      "port": 8082
                    },
                    "nodeSelector": {
                    },
                    "tolerations": [

                    ],
                    "affinity": {
                    },
                    "priorityClassName": "",
                    "resources": {
                    },
                    "serviceAccount": {
                      "create": true,
                      "name": "argocd-application-controller"
                    },
                    "metrics": {
                      "enabled": false,
                      "service": {
                        "annotations": {
                        },
                        "labels": {
                        },
                        "servicePort": 8082
                      },
                      "serviceMonitor": {
                        "enabled": false
                      },
                      "rules": {
                        "enabled": false,
                        "spec": [

                        ]
                      }
                    },
                    "clusterAdminAccess": {
                      "enabled": true
                    }
                  },
                  "dex": {
                    "enabled": true,
                    "name": "dex-server",
                    "image": {
                      "repository": "quay.io/dexidp/dex",
                      "tag": "latest",
                      "imagePullPolicy": "IfNotPresent"
                    },
                    "initImage": {
                      "repository": null,
                      "tag": null,
                      "imagePullPolicy": null
                    },
                    "env": [

                    ],
                    "serviceAccount": {
                      "create": true,
                      "name": "argocd-dex-server"
                    },
                    "volumeMounts": [
                      {
                        "name": "static-files",
                        "mountPath": "/shared"
                      }
                    ],
                    "volumes": [
                      {
                        "name": "static-files",
                        "emptyDir": {
                        }
                      }
                    ],
                    "containerPortHttp": 5556,
                    "servicePortHttp": 5556,
                    "containerPortGrpc": 5557,
                    "servicePortGrpc": 5557,
                    "nodeSelector": {
                    },
                    "tolerations": [

                    ],
                    "affinity": {
                    },
                    "priorityClassName": "",
                    "containerSecurityContext": {
                    },
                    "resources": {
                    }
                  },
                  "redis": {
                    "enabled": true,
                    "name": "redis",
                    "image": {
                      "repository": "redis",
                      "tag": "5.0.3",
                      "imagePullPolicy": "IfNotPresent"
                    },
                    "containerPort": 6379,
                    "servicePort": 6379,
                    "env": [

                    ],
                    "nodeSelector": {
                    },
                    "tolerations": [

                    ],
                    "affinity": {
                    },
                    "priorityClassName": "",
                    "containerSecurityContext": {
                    },
                    "resources": {
                    },
                    "volumeMounts": [

                    ],
                    "volumes": [

                    ]
                  },
                  "server": {
                    "name": "server",
                    "image": {
                      "repository": null,
                      "tag": null,
                      "imagePullPolicy": null
                    },
                    "extraArgs": {
                    },
                    "env": [

                    ],
                    "logLevel": "info",
                    "podAnnotations": {
                    },
                    "podLabels": {
                    },
                    "containerPort": 8080,
                    "readinessProbe": {
                      "failureThreshold": 3,
                      "initialDelaySeconds": 10,
                      "periodSeconds": 10,
                      "successThreshold": 1,
                      "timeoutSeconds": 1
                    },
                    "livenessProbe": {
                      "failureThreshold": 3,
                      "initialDelaySeconds": 10,
                      "periodSeconds": 10,
                      "successThreshold": 1,
                      "timeoutSeconds": 1
                    },
                    "volumeMounts": [

                    ],
                    "volumes": [

                    ],
                    "nodeSelector": {
                    },
                    "tolerations": [

                    ],
                    "affinity": {
                    },
                    "priorityClassName": "",
                    "containerSecurityContext": {
                    },
                    "resources": {
                    },
                    "certificate": {
                      "enabled": false,
                      "domain": "argocd.example.com",
                      "issuer": {
                      },
                      "additionalHosts": [

                      ]
                    },
                    "service": {
                      "annotations": {
                      },
                      "labels": {
                      },
                      "type": "ClusterIP",
                      "servicePortHttp": 80,
                      "servicePortHttps": 443
                    },
                    "metrics": {
                      "enabled": false,
                      "service": {
                        "annotations": {
                        },
                        "labels": {
                        },
                        "servicePort": 8083
                      },
                      "serviceMonitor": {
                        "enabled": false
                      }
                    },
                    "serviceAccount": {
                      "create": true,
                      "name": "argocd-server"
                    },
                    "ingress": {
                      "enabled": false,
                      "annotations": {
                      },
                      "labels": {
                      },
                      "hosts": [

                      ],
                      "paths": [
                        "/"
                      ],
                      "tls": [

                      ]
                    },
                    "config": {
                      "application.instanceLabelKey": "argocd.argoproj.io/instance"
                    },
                    "rbacConfig": {
                    },
                    "additionalApplications": [

                    ],
                    "additionalProjects": [

                    ]
                  },
                  "repoServer": {
                    "name": "repo-server",
                    "image": {
                      "repository": null,
                      "tag": null,
                      "imagePullPolicy": null
                    },
                    "extraArgs": {
                    },
                    "env": [

                    ],
                    "logLevel": "info",
                    "podAnnotations": {
                    },
                    "podLabels": {
                    },
                    "containerPort": 8081,
                    "readinessProbe": {
                      "failureThreshold": 3,
                      "initialDelaySeconds": 10,
                      "periodSeconds": 10,
                      "successThreshold": 1,
                      "timeoutSeconds": 1
                    },
                    "livenessProbe": {
                      "failureThreshold": 3,
                      "initialDelaySeconds": 10,
                      "periodSeconds": 10,
                      "successThreshold": 1,
                      "timeoutSeconds": 1
                    },
                    "volumeMounts": [

                    ],
                    "volumes": [

                    ],
                    "nodeSelector": {
                    },
                    "tolerations": [

                    ],
                    "affinity": {
                    },
                    "priorityClassName": "",
                    "containerSecurityContext": {
                    },
                    "resources": {
                    },
                    "service": {
                      "annotations": {
                      },
                      "labels": {
                      },
                      "port": 8081
                    },
                    "metrics": {
                      "enabled": false,
                      "service": {
                        "annotations": {
                        },
                        "labels": {
                        },
                        "servicePort": 8084
                      },
                      "serviceMonitor": {
                        "enabled": false
                      }
                    },
                    "serviceAccount": {
                      "create": false,
                      "annotations": {
                      }
                    }
                  },
                  "configs": {
                    "knownHosts": {
                      "data": {
                        "ssh_known_hosts": "bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAubiN81eDcafrgMeLzaFPsw2kNvEcqTKl/VqLat/MaB33pZy0y3rJZtnqwR2qOOvbwKZYKiEO1O6VqNEBxKvJJelCq0dTXWT5pbO2gDXC6h6QDXCaHo6pOHGPUy+YBaGQRGuSusMEASYiWunYN0vCAI8QaXnWMXNMdFP3jHAJH0eDsoiGnLPBlBp4TNm6rYI74nMzgz3B9IikW4WVK+dc8KZJZWYjAuORU3jc1c/NPskD2ASinf8v3xnfXeukU0sJ5N6m5E8VLjObPEO+mN2t/FZTMZLiFqPWc/ALSqnMnnhwrNi2rbfg/rd/IpL8Le3pSBne8+seeFVBoGqzHM9yXw==\ngithub.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==\ngitlab.com ecdsa-sha2-nistp256 AAAAE2VjZHNhLXNoYTItbmlzdHAyNTYAAAAIbmlzdHAyNTYAAABBBFSMqzJeV9rUzU4kWitGjeR4PWSa29SPqJ1fVkhtj3Hw9xjLVXVYrU9QlYWrOLXBpQ6KWjbjTDTdDkoohFzgbEY=\ngitlab.com ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIAfuCHKVTjquxvt6CM6tdG4SLp1Btn/nOeHHE5UOzRdf\ngitlab.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCsj2bNKTBSpIYDEGk9KxsGh3mySTRgMtXL583qmBpzeQ+jqCMRgBqB98u3z++J1sKlXHWfM9dyhSevkMwSbhoR8XIq/U0tCNyokEi/ueaBMCvbcTHhO7FcwzY92WK4Yt0aGROY5qX2UKSeOvuP4D6TPqKF1onrSzH9bx9XUf2lEdWT/ia1NEKjunUqu1xOB/StKDHMoX4/OKyIzuS0q/T1zOATthvasJFoPrAjkohTyaDUz2LN5JoH839hViyEG82yB+MjcFV5MU3N1l1QL3cVUCh93xSaua1N85qivl+siMkPGbO5xR/En4iEY6K2XPASUEMaieWVNTRCtJ4S8H+9\nssh.dev.azure.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\nvs-ssh.visualstudio.com ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC7Hr1oTWqNqOlzGJOfGJ4NakVyIzf1rXYd4d7wo6jBlkLvCA4odBlL0mDUyZ0/QUfTTqeu+tm22gOsv+VrVTMk6vwRU75gY/y9ut5Mb3bR5BV58dKXyq9A9UeB5Cakehn5Zgm6x1mKoVyf+FFn26iYqXJRgzIZZcZ5V6hrE0Qg39kZm4az48o0AUbf6Sp4SLdvnuMa2sVNwHBboS7EJkm57XQPVU3/QpyNLHbWDdzwtrlS+ez30S3AdYhLKEOxAG8weOnyrtLJAUen9mTkol8oII1edf7mWWbWVf0nBmly21+nZcmCTISQBtdcyPaEno7fFQMDD26/s0lfKob4Kw8H\n"
                      }
                    },
                    "tlsCerts": {
                    },
                    "repositoryCredentials": {
                    },
                    "secret": {
                      "createSecret": true,
                      "githubSecret": "",
                      "gitlabSecret": "",
                      "bitbucketServerSecret": "",
                      "bitbucketUUÃŒD": "",
                      "gogsSecret": "",
                      "extra": {
                      },
                      "argocdServerTlsConfig": {
                      }
                    }
                  },
                  "openshift": {
                    "enabled": true,
                    "route": {
                      "enabled": true,
                      "host": "",
                      "annotations": {
                      }
                    },
                    "oAuth": {
                      "enabled": true,
                      "config": {
                        "image": {
                          "repository": "quay.io/openshift/origin-cli",
                          "tag": "latest",
                          "imagePullPolicy": "IfNotPresent"
                        },
                        "base": {
                          "url": "openshiftOAuthConfigBaseUrl"
                        },
                        "redirect": {
                          "url": "openshiftOAuthConfigRedirectURI"
                        },
                        "dex": {
                          "dex.config": "connectors:\n  - type: openshift\n    # Required field for connector id.\n    id: openshift\n    # Required field for connector name.\n    name: OpenShift\n    config:\n      # Location of the OpenShift API Server. Get it with \"oc whoami --show-server\" or\n      # leave this default for automatic discovery (recommended)\n      issuer: openshiftOAuthConfigIssuer\n      # Name of the OpenShift OAuth Client with format: \"system:serviceaccount:<namespace>:<service_account_name>\" or\n      # leave this default for automatic discovery (recommended)\n      clientID: openshiftOAuthConfigClientID\n      # Value of the OpenShift OAuth client secret. Get it with \"oc serviceaccounts get-token <argocd_dex_server_service_account_name>\" or\n      # leave this default for automatic discovery (recommended)\n      clientSecret: openshiftOAuthConfigClientSecret\n      # ArgoCD redirect URI in format: \"https://<argocd_host>/api/dex/callback\" or\n      # leave this default for automatic discovery (recommended)\n      redirectURI: openshiftOAuthConfigRedirectURI\n      # Disable SSL verification\n      insecureCA: true\n      # Optional: The location of file containing SSL certificates to communicate to OpenShift\n      # rootCA: /etc/ssl/openshift.pem\n"
                        }
                      },
                      "rbac": {
                        "enabled": true,
                        "groups": {
                          "admins": {
                            "name": "argocd-admins",
                            "role": "role:admin"
                          },
                          "developers": {
                            "name": "argoc-developers",
                            "role": "role:readonly"
                          }
                        },
                        "users": {
                          "admins": [
                            "developer",
                            "kubedadmin",
                            "admin"
                          ],
                          "developers": [

                          ]
                        }
                      }
                    }
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: quay.io/disposab1e/argocd-operator-helm:v0.0.3
          createdAt: "2020-01-20 01:01:01"
          description: Declarative Continuous Delivery following Gitops.
          repository: https://github.com/disposab1e/argocd-operator-helm.git
          support: Community
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents the Argo CD installation.
            displayName: ArgoCD
            kind: ArgoCD
            name: argocds.argoproj.io
            version: v1alpha1
        description: "\n[Argo CD](https://argoproj.github.io/argo-cd/) is a declarative,
          GitOps continuous delivery tool for Kubernetes.\n\n### Overview\nThis **community
          maintained** [Argo CD Operator (Helm)](https://github.com/disposab1e/argocd-operator-helm)
          \nis based on the **community maintained** [Argo CD Helm Chart](https://github.com/argoproj/argo-helm/tree/master/charts/argo-cd)
          \nand currently installs the non-HA version of [Argo CD](https://argoproj.github.io/argo-cd/)
          in OpenShift Container Platform and Kubernetes. \n\n\n### Features:\n* Easy
          configuration and installation\n* OpenShift OAuth integration \n\n### Install\n\nThe
          operator shares all [configuration values](https://github.com/disposab1e/argocd-operator-helm/blob/release-0.0.3/helm-charts/argo-cd/README.md)
          \nfrom the Argo CD Helm Chart and manages a single namespace installation
          of Argo CD. \nTherefore you have to install the operator and Argo CD in
          the same namespace. \nFor simplicity we recommend creating a namespace **argocd**.
          \n\nTo install Argo CD create [a new ArgoCD resource](https://github.com/disposab1e/argocd-operator-helm/blob/release-0.0.3/guides/ocp4/examples/openshift-oauth.yaml)
          \nwith your own customizations or use the provided example from the Web
          UI.\n\nThe provided example with OpenShift OAuth integration enabled assumes
          you have an existing `developer`, `kubeadmin` or `admin` User \nalready
          available for the Argo CD administrator group. \nIf you have other existing
          groups or users feel free to change the example to fit your needs. You can
          also disable the \nOAuth integration with:\n```  \nopenshift.oAuth.enabled:
          false\n```\n\nFor Kubernetes you must disable the OpenShift integration
          with: \n```  \nopenshift.enabled: false\n```\n\nSee the documentation and
          examples at the [official github repository](https://github.com/disposab1e/argocd-operator-helm)
          for more information.\n"
        displayName: Argo CD Operator (Helm)
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Disposable Zone
        version: 0.0.3
      name: alpha
    defaultChannel: alpha
    packageName: argocd-operator-helm
    provider:
      name: Disposable Zone
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: knative-kafka-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/knative-kafka-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: knative-kafka-operator.v0.12.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "eventing.knative.dev/v1alpha1",
                "kind": "KnativeEventingKafka",
                "metadata": {
                  "name": "knative-eventing-kafka"
                },
                "spec": {
                  "bootstrapServers": "my-cluster-kafka-bootstrap.kafka:9092",
                  "setAsDefaultChannelProvisioner": true
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking,Integration & Delivery,Cloud Provider,Developer Tools
          certified: "false"
          containerImage: quay.io/openshift-knative/knative-kafka-operator:v0.12.1
          createdAt: "2020-02-10T14:00:00Z"
          description: Knative Eventing Kafka manages the Kafka source and channel
            provisioner for Knative Eventing
          repository: https://github.com/openshift-knative/knative-kafka-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents an installation of a Knative components for Apache
              Kafka
            displayName: Knative components for Apache Kafka
            kind: KnativeEventingKafka
            name: knativeeventingkafkas.eventing.knative.dev
            version: v1alpha1
        description: |
          Apache Kafka is a distributed streaming platform and this operator adds support for
          Knative by adding the following components:

          1. A Knative Eventing Source to consume messages from an Apache Kafka topic
          1. A Knative Eventing Channel that is backed by Apache Kafka topics

          ## Prerequisites

          ### Apache Kafka and Knative Eventing
          Knative Eventing and Apache Kafka is required since this operator will attempt to
          Knative components that work with an Apache Kafka cluster that is accessible the
          installation.

          For complete Knative Eventing documentation, see
          [Knative eventing](https://www.knative.dev/docs/eventing/) or
          [Knative docs](https://www.knative.dev/docs/) to learn about Knative.
        displayName: Knative Apache Kafka Operator
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.12.1
      name: alpha
    defaultChannel: alpha
    packageName: knative-kafka-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: openshiftansibleservicebroker
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/openshiftansibleservicebroker
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: openshiftansibleservicebroker.4.2.18-202002031246
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"osb.openshift.io/v1", "kind":"AutomationBroker",
            "metadata":{"name":"ansible-service-broker","namespace":"ansible-service-broker"},
            "spec":{"createBrokerNamespace":"false","waitForBroker":"false", "registries":
            [{"type": "rhcc", "name": "rhcc", "url": "https://registry.redhat.io",
            "white_list": [".*-apb$"], "auth_type": "secret", "auth_name": "asb-registry-auth"}]}}]'
          capabilities: Seamless Upgrades
          containerImage: registry.redhat.io/openshift4/ose-ansible-service-broker@sha256:42345695d0bd95a15e85d80cf59d54b75bb2cee7742e1e9cefbb7dbc5737a87d
          description: OpenShift Ansible Service Broker is an implementation of the
            [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          olm.skipRange: '>=4.1.0 <4.2.18-202002031246'
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Open Service Broker supporting management of application
              bundles
            displayName: Automation Broker
            kind: AutomationBroker
            name: automationbrokers.osb.openshift.io
            version: v1
          - description: An application bundle available for deployment via Automation
              Broker
            displayName: Automation Broker Bundle
            kind: Bundle
            name: bundles.automationbroker.io
            version: v1alpha1
          - description: An application bundle binding
            displayName: Automation Broker Bundle Binding
            kind: BundleBinding
            name: bundlebindings.automationbroker.io
            version: v1alpha1
          - description: An instance of an application bundle
            displayName: Automation Broker Bundle Instance
            kind: BundleInstance
            name: bundleinstances.automationbroker.io
            version: v1alpha1
        description: |
          OpenShift Ansible Service Broker is an implementation of the [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          that manages applications defined in [Ansible Playbook Bundles](https://github.com/ansibleplaybookbundle/ansible-playbook-bundle).
          Ansible Playbook Bundles (APB) are a method of defining applications via a collection of Ansible Playbooks built into a container
          with an Ansible runtime with the playbooks corresponding to a type of request specified in the
          [Open Service Broker API Specification](https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md#api-overview).

          Check out the [Keynote Demo from Red Hat Summit 2017](https://youtu.be/8MCbJmZQM9c?list=PLEGSLwUsxfEh4TE2GDU4oygCB-tmShkSn&t=4732)
        displayName: OpenShift Ansible Service Broker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.2.18-202002031246
      name: "4.2"
    - currentCSV: openshiftansibleservicebroker.4.2.18-202002031246-s390x
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"osb.openshift.io/v1", "kind":"AutomationBroker",
            "metadata":{"name":"ansible-service-broker","namespace":"ansible-service-broker"},
            "spec":{"createBrokerNamespace":"false","waitForBroker":"false", "registries":
            [{"type": "rhcc", "name": "rhcc", "url": "https://registry.redhat.io",
            "white_list": [".*-apb$"], "auth_type": "secret", "auth_name": "asb-registry-auth"}]}}]'
          capabilities: Seamless Upgrades
          containerImage: registry.redhat.io/openshift4/ose-ansible-service-broker@sha256:26292c4ee9caa49f49b57fd51bc2959b2583a6d8d93ee96ded39c06d540cc395
          description: OpenShift Ansible Service Broker is an implementation of the
            [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          olm.skipRange: '>=4.1.0 <4.2.18-202002031246'
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Open Service Broker supporting management of application
              bundles
            displayName: Automation Broker
            kind: AutomationBroker
            name: automationbrokers.osb.openshift.io
            version: v1
          - description: An application bundle available for deployment via Automation
              Broker
            displayName: Automation Broker Bundle
            kind: Bundle
            name: bundles.automationbroker.io
            version: v1alpha1
          - description: An application bundle binding
            displayName: Automation Broker Bundle Binding
            kind: BundleBinding
            name: bundlebindings.automationbroker.io
            version: v1alpha1
          - description: An instance of an application bundle
            displayName: Automation Broker Bundle Instance
            kind: BundleInstance
            name: bundleinstances.automationbroker.io
            version: v1alpha1
        description: |
          OpenShift Ansible Service Broker is an implementation of the [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          that manages applications defined in [Ansible Playbook Bundles](https://github.com/ansibleplaybookbundle/ansible-playbook-bundle).
          Ansible Playbook Bundles (APB) are a method of defining applications via a collection of Ansible Playbooks built into a container
          with an Ansible runtime with the playbooks corresponding to a type of request specified in the
          [Open Service Broker API Specification](https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md#api-overview).

          Check out the [Keynote Demo from Red Hat Summit 2017](https://youtu.be/8MCbJmZQM9c?list=PLEGSLwUsxfEh4TE2GDU4oygCB-tmShkSn&t=4732)
        displayName: OpenShift Ansible Service Broker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.2.18-202002031246
      name: 4.2-s390x
    - currentCSV: openshiftansibleservicebroker.4.3.2-202002112006
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"osb.openshift.io/v1", "kind":"AutomationBroker",
            "metadata":{"name":"ansible-service-broker","namespace":"ansible-service-broker"},
            "spec":{"createBrokerNamespace":"false","waitForBroker":"false", "registries":
            [{"type": "rhcc", "name": "rhcc", "url": "https://registry.redhat.io",
            "white_list": [".*-apb$"], "auth_type": "secret", "auth_name": "asb-registry-auth"}]}}]'
          capabilities: Seamless Upgrades
          containerImage: registry.redhat.io/openshift4/ose-ansible-service-broker@sha256:ad2b8cd12bf49b75427e43e889c47a4c944bf25203211f976daea1a10dc84071
          description: OpenShift Ansible Service Broker is an implementation of the
            [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          olm.skipRange: '>=4.2.0 <4.3.2-202002112006'
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Open Service Broker supporting management of application
              bundles
            displayName: Automation Broker
            kind: AutomationBroker
            name: automationbrokers.osb.openshift.io
            version: v1
          - description: An application bundle available for deployment via Automation
              Broker
            displayName: Automation Broker Bundle
            kind: Bundle
            name: bundles.automationbroker.io
            version: v1alpha1
          - description: An application bundle binding
            displayName: Automation Broker Bundle Binding
            kind: BundleBinding
            name: bundlebindings.automationbroker.io
            version: v1alpha1
          - description: An instance of an application bundle
            displayName: Automation Broker Bundle Instance
            kind: BundleInstance
            name: bundleinstances.automationbroker.io
            version: v1alpha1
        description: |
          OpenShift Ansible Service Broker is an implementation of the [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          that manages applications defined in [Ansible Playbook Bundles](https://github.com/ansibleplaybookbundle/ansible-playbook-bundle).
          Ansible Playbook Bundles (APB) are a method of defining applications via a collection of Ansible Playbooks built into a container
          with an Ansible runtime with the playbooks corresponding to a type of request specified in the
          [Open Service Broker API Specification](https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md#api-overview).

          Check out the [Keynote Demo from Red Hat Summit 2017](https://youtu.be/8MCbJmZQM9c?list=PLEGSLwUsxfEh4TE2GDU4oygCB-tmShkSn&t=4732)
        displayName: OpenShift Ansible Service Broker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.3.2-202002112006
      name: "4.3"
    - currentCSV: openshiftansibleservicebroker.4.1.34-202002061038
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"osb.openshift.io/v1", "kind":"AutomationBroker",
            "metadata":{"name":"ansible-service-broker","namespace":"ansible-service-broker"},
            "spec":{"createBrokerNamespace":"false","waitForBroker":"false", "registries":
            [{"type": "rhcc", "name": "rhcc", "url": "https://registry.redhat.io",
            "white_list": [".*-apb$"], "auth_type": "secret", "auth_name": "asb-registry-auth"}]}}]'
          capabilities: Seamless Upgrades
          containerImage: registry.redhat.io/openshift4/ose-ansible-service-broker@sha256:b5fdd12d71296fcd1d1756d1b83b4b2221af088251a3c7f5b7c1117e7a4cefce
          description: OpenShift Ansible Service Broker is an implementation of the
            [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          olm.skipRange: '>=4.1.0 <4.1.34-202002061038'
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Open Service Broker supporting management of application
              bundles
            displayName: Automation Broker
            kind: AutomationBroker
            name: automationbrokers.osb.openshift.io
            version: v1
          - description: An application bundle available for deployment via Automation
              Broker
            displayName: Automation Broker Bundle
            kind: Bundle
            name: bundles.automationbroker.io
            version: v1alpha1
          - description: An application bundle binding
            displayName: Automation Broker Bundle Binding
            kind: BundleBinding
            name: bundlebindings.automationbroker.io
            version: v1alpha1
          - description: An instance of an application bundle
            displayName: Automation Broker Bundle Instance
            kind: BundleInstance
            name: bundleinstances.automationbroker.io
            version: v1alpha1
        description: |
          OpenShift Ansible Service Broker is an implementation of the [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          that manages applications defined in [Ansible Playbook Bundles](https://github.com/ansibleplaybookbundle/ansible-playbook-bundle).
          Ansible Playbook Bundles (APB) are a method of defining applications via a collection of Ansible Playbooks built into a container
          with an Ansible runtime with the playbooks corresponding to a type of request specified in the
          [Open Service Broker API Specification](https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md#api-overview).

          Check out the [Keynote Demo from Red Hat Summit 2017](https://youtu.be/8MCbJmZQM9c?list=PLEGSLwUsxfEh4TE2GDU4oygCB-tmShkSn&t=4732)
        displayName: OpenShift Ansible Service Broker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.1.34-202002061038
      name: stable
    defaultChannel: "4.3"
    packageName: openshiftansibleservicebroker
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Citrix
      provider-url: ""
    name: citrix-adc-istio-ingress-gateway-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/citrix-adc-istio-ingress-gateway-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: citrix-adc-istio-ingress-gateway-operator.v1.0.1
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"Citrixadcistioingressgateway","metadata":{"name":"citrix-istio-ingress-gw"},"spec":{"citrixCPX":false,"ingressGateway":{"EULA":false,"httpNodePort":30180,"httpsNodePort":31443,"image":"quay.io/citrix/citrix-k8s-cpx-ingress","imagePullPolicy":"IfNotPresent","label":"citrix-ingressgateway","licenseServerIP":null,"licenseServerPort":27000,"lightWeightCPX":true,"mgmtHttpPort":10080,"mgmtHttpsPort":10443,"secretVolumes":null,"tag":"13.0-41.28","tcpPort":null},"istioAdaptor":{"ADMIP":null,"image":"quay.io/citrix/citrix-istio-adaptor","imagePullPolicy":"IfNotPresent","netProfile":null,"netscalerUrl":null,"proxyType":"router","secureConnect":true,"tag":"1.0.1","vserverIP":null},"istioPilot":{"SAN":"spiffe://cluster.local/ns/istio-system/sa/istio-pilot-service-account","insecureGrpcPort":15010,"name":"istio-pilot","namespace":"istio-system","secureGrpcPort":15011},"metricExporter":{"image":"quay.io/citrix/citrix-adc-metrics-exporter","imagePullPolicy":"IfNotPresent","logLevel":"ERROR","port":8888,"required":true,"secure":"YES","version":1.2},"nslogin":{"password":null,"username":null}}}]'
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.connect.redhat.com/citrix/istioingressgateway:1.0.2
          createdAt: "2019-12-03 14:16:17"
          description: An Istio ingress gateway acts as an entry point for the incoming
            traffic and secures and controls access to the service mesh from outside.
            It also performs routing and load balancing. Citrix ADC CPX, MPX, or VPX
            can be deployed as an ingress gateway to the Istio service mesh.
          repository: https://github.com/citrix/citrix-istio-adaptor
          support: citrix-adc-istio-ingress-gateway-operator
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: This is CRD for Citrix ADC Istio Ingress Gateway Operator
            displayName: Citrix ADC Istio Ingress Gateway Operator
            kind: Citrixadcistioingressgateway
            name: citrixadcistioingressgateways.charts.helm.k8s.io
            version: v1alpha1
        description: An Istio ingress gateway acts as an entry point for the incoming
          traffic and secures and controls access to the service mesh from outside.
          It also performs routing and load balancing. Citrix ADC CPX, MPX, or VPX
          can be deployed as an ingress gateway to the Istio service mesh.
        displayName: Citrix ADC Istio Ingress Gateway Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Citrix
        version: 1.0.1
      name: alpha
    defaultChannel: alpha
    packageName: citrix-adc-istio-ingress-gateway-operator
    provider:
      name: Citrix
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: PlanetScale
      provider-url: ""
    name: planetscale-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/planetscale-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: planetscale-operator.v0.1.8
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "planetscale.com/v1alpha1",
                "kind": "PsCluster",
                "metadata": {
                  "name": "example"
                },
                "spec": {
                  "monitored": true,
                  "lockserver": {
                    "endpoint": "<LockServer IP and Port>",
                    "root_path": "/vitess/example/global"
                  },
                  "cells": [
                    {
                      "name": "example1",
                      "useGlobalLockserver": true,
                      "gateway": {
                        "count": 2
                      },
                      "vtctld": {
                        "count": 1
                      },
                      "keyspaces": [
                        {
                          "name": "messagedb",
                          "shards": [
                            {
                              "range": "-80",
                              "replicas": [
                                {
                                  "type": "replica"
                                },
                                {
                                  "type": "replica"
                                },
                                {
                                  "type": "rdonly"
                                },
                                {
                                  "type": "replica"
                                }
                              ]
                            },
                            {
                              "range": "80-",
                              "replicas": [
                                {
                                  "type": "replica"
                                },
                                {
                                  "type": "replica"
                                },
                                {
                                  "type": "rdonly"
                                },
                                {
                                  "type": "replica"
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              }
            ]
          capabilities: Deep Insights
          categories: Database
          certified: "False"
          containerImage: registry.connect.redhat.com/planetscale/operator:0.1.8
          createdAt: "2019-03-06 21:40:00"
          description: PlanetScale's operator for Vitess deploys and manages instances
            of MySQL with Vitess, a database clustering system for horizontal scaling
            of MySQL.
          support: PlanetScale
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Instance of a PlanetScale Vitess Cluster
            displayName: PsCluster
            kind: PsCluster
            name: psclusters.planetscale.com
            version: v1alpha1
        description: |-
          The Vitess Operator deploys and manages instances of MySQL with Vitess, a database clustering system for horizontal scaling of MySQL
          through generalized sharding.


          By encapsulating shard-routing logic, Vitess allows application code and
          database queries to remain agnostic to the distribution of data onto
          multiple shards. With Vitess, you can even split and merge shards as your
          needs grow, with an atomic cutover step that takes only a few seconds.


          Vitess has been a core component of YouTube's database infrastructure since
          2011, and has grown to encompass tens of thousands of MySQL nodes. For more
          information, visit [https://planetscale.com](https://planetscale.com)

          ### New Features

          We have support for Orchestrator, backups, and restores from GCS and S3. We also have seamless upgrades for all containers.

          ### Supported Features

          * **Dashboard** - The Operator deploys a dashboard for monitoring and introspecting your cluster.

          * **Scale Gateways** - Scale out the Gateway, which is the component that responds to queries and returns consolidated results from the MySQL shards.

          * **Configure Keyspaces** - Configure how your data is sharded with in your cluster.

          ### Before You Start

          1. Create a RedHat registry image pull secret called
          `planetscale-operator-pull-secret`, which is required to pull the operator
          image.

          2. Create an etcd cluster, which is used as a lock server by PlanetScale Vitess clusters. Try out the [etcd Operator](https://www.operatorhub.io/?keyword=etcd).

          ### Permissions

          This operator only supports running in the same namespace as the PsCluster resources it is watching.


          This operator should be deployed in an isolated namespace since the Pods it creates use the `default` service account and require the `use` permission on the `anyuid` Security Context Contraint (SCC) to run correctly. Running this operator in a shared namespace is not recommended since unrelated pods will have access to use the `anyuid` SCC.
        displayName: PlanetScale Operator for Vitess
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: PlanetScale
        version: 0.1.8
      name: beta
    defaultChannel: beta
    packageName: planetscale-certified
    provider:
      name: PlanetScale
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: dv-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/dv-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: dv-operator.v7.5.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "teiid.io/v1alpha1",
                "kind": "VirtualDatabase",
                "metadata": {
                  "name": "rdbms-springboot"
                },
                "spec": {
                  "replicas": 1,
                  "env": [
                    {
                      "name": "SPRING_DATASOURCE_SAMPLEDB_USERNAME",
                      "value": "user"
                    },
                    {
                      "name": "SPRING_DATASOURCE_SAMPLEDB_PASSWORD",
                      "value": "mypassword"
                    },
                    {
                      "name": "SPRING_DATASOURCE_SAMPLEDB_DATABASENAME",
                      "value": "sampledb"
                    },
                    {
                      "name": "SPRING_DATASOURCE_SAMPLEDB_JDBCURL",
                      "value": "jdbc:postgresql://postgresql/$(SPRING_DATASOURCE_SAMPLEDB_DATABASENAME)"
                    }
                  ],
                  "build": {
                    "source": {
                      "ddl": "CREATE DATABASE customer OPTIONS (ANNOTATION 'Customer VDB');\nUSE DATABASE customer;\n\nCREATE FOREIGN DATA WRAPPER postgresql;\nCREATE SERVER sampledb TYPE 'NONE' FOREIGN DATA WRAPPER postgresql;\n\nCREATE SCHEMA accounts SERVER sampledb;\nCREATE VIRTUAL SCHEMA portfolio;\n\nSET SCHEMA accounts;\nIMPORT FOREIGN SCHEMA public FROM SERVER sampledb INTO accounts OPTIONS(\"importer.useFullSchemaName\" 'false');\n\nSET SCHEMA portfolio;\n\nCREATE VIEW CustomerZip(id bigint PRIMARY KEY, name string, ssn string, zip string) AS \n    SELECT c.ID as id, c.NAME as name, c.SSN as ssn, a.ZIP as zip \n    FROM accounts.CUSTOMER c LEFT OUTER JOIN accounts.ADDRESS a \n    ON c.ID = a.CUSTOMER_ID;\n"
                    }
                  }
                }
              }
            ]
          capabilities: Basic Install
          categories: Database, Integration & Delivery
          certified: "false"
          containerImage: fuse7-tech-preview/dv-rhel7-operator
          createdAt: "2019-06-24T12:48:22Z"
          description: Data Virtualization Cloud Operator for deployment and management
            of Virtual Database services.
          repository: https://github.com/teiid/teiid-operator
          support: Red Hat, Inc.
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A CRD for running a DV service.
            displayName: VirtualDatabase
            kind: VirtualDatabase
            name: virtualdatabases.teiid.io
            version: v1alpha1
        description: "Data Virtualization (DV) system allows applications to access
          data from multiple, heterogeneous data stores. Through its abstraction and
          federation layers, data is accessed and integrated in real-time across distributed
          data sources without copying or otherwise moving data from its system of
          record. \n\nFor example, you can access your all data in Oracle, Postgres,
          MongoDB and/or Rest API (many more) with a single request. Data Virtualization
          gives you all the tools required to build logical/abstraction layer that
          can exposed as Virtual Database by essentially making all sources underneath
          as a black box to the consuming user. Since this integration of data happens
          in real time there is no ETL process to run and data is always fresh. For
          the end user the Virtual Database exactly looks like a relational database
          like Postgres, that can be accessed using JDBC, ODBC, OData V4 and Postgres
          protocols, i.e. can be accessed from any language that you have your application
          written in.\n\nDV, out of box also exposes the OData v4 REST API without
          any further coding, with this you can expose you single or multiple databases
          as rest services without any coding. Alternatively if you want expose a
          Open API directly over your data that is also possible with DV.\n\n### Before
          you begin\nYou must configure authentication to Red Hat container registry
          before you can import and use the Red Hat Fuse OpenShift Image Streams.
          Follow instruction given below to configure the registration to container
          registry.\n\n1. Log in to the OpenShift Server as an administrator, as follow:\n
          \   ```\n    oc login -u system:admin\n    ```\n2. Log in to the OpenShift
          project where you will be installing the operator.\n    ```\n    oc project
          dv-project\n    ```\n3. Create a docker-registry secret using either Red
          Hat Customer Portal account or Red Hat Developer Program account credentials.\n
          \   ```\n    oc create secret docker-registry dv-pull-secret \\\n      --docker-server=registry.redhat.io
          \\\n      --docker-username=CUSTOMER_PORTAL_USERNAME \\\n      --docker-password=CUSTOMER_PORTAL_PASSWORD
          \\\n      --docker-email=EMAIL_ADDRESS\n    \n    oc secrets link builder
          dv-pull-secret\n    oc secrets link builder dv-pull-secret --for=pull\n
          \   ```\n    NOTE: You need to create a docker-registry secret in every
          new namespace where the image streams reside and which use registry.redhat.io.\n\n
          \   If you do not wish to use your Red Hat account username and password
          to create the secret, it is recommended to create an authentication token
          using a [registry service account](https://access.redhat.com/terms-based-registry/).\n\n###
          How to install\n- When the operator is installed (you have created a subscription
          and the operator is running in the selected namespace) and before you create
          a new CR of Kind VirtualDatabase, you have to link the secret created in
          the previous section to the operator service account.\n```\noc secrets link
          dv-operator dv-pull-secret --for=pull\n```\n\n- Create a new CR of Kind
          VirtualDatabase (click the Create New button). The CR spec contains all
          defaults (see below)."
        displayName: Data Virtualization Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 7.5.0
      name: alpha
    defaultChannel: alpha
    packageName: dv-operator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: nfd
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/nfd
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: nfd.4.2.18-202002031246
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "nfd.openshift.io/v1alpha1",
                "kind": "NodeFeatureDiscovery",
                "metadata": {
                  "name": "nfd-master-server"
                },
                "spec": {
                  "namespace": "openshift-nfd"
                }
              }
            ]
          capabilities: Basic Install
          categories: Database
          certified: "false"
          containerImage: ""
          createdAt: "2019-05-30T00:00:00Z"
          description: This software enables node feature discovery for Kubernetes.
            It detects hardware features available on each node in a Kubernetes cluster,
            and advertises those features using node labels.
          provider: Red Hat
          repository: https://github.com/openshift/cluster-nfd-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: "The NFD operator creates and maintains the Node Feature
              Discovery (NFD) on Kubernetes. \nIt detects hardware features available
              on each node in a Kubernetes cluster, and advertises those features
              using node labels.\n"
            displayName: Node Feature Discovery
            kind: NodeFeatureDiscovery
            name: nodefeaturediscoveries.nfd.openshift.io
            version: v1alpha1
        description: "The NFD operator creates and maintiains the Node Feature Discovery
          (NFD) on Kubernetes. \nIt detects hardware features available on each node
          in a Kubernetes cluster, and advertises those features using node labels.\n"
        displayName: Node Feature Discovery
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.2.0
      name: "4.2"
    - currentCSV: nfd.4.2.18-202002031246-s390x
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "nfd.openshift.io/v1alpha1",
                "kind": "NodeFeatureDiscovery",
                "metadata": {
                  "name": "nfd-master-server"
                },
                "spec": {
                  "namespace": "openshift-nfd"
                }
              }
            ]
          capabilities: Basic Install
          categories: Database
          certified: "false"
          containerImage: ""
          createdAt: "2019-05-30T00:00:00Z"
          description: This software enables node feature discovery for Kubernetes.
            It detects hardware features available on each node in a Kubernetes cluster,
            and advertises those features using node labels.
          provider: Red Hat
          repository: https://github.com/openshift/cluster-nfd-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: "The NFD operator creates and maintains the Node Feature
              Discovery (NFD) on Kubernetes. \nIt detects hardware features available
              on each node in a Kubernetes cluster, and advertises those features
              using node labels.\n"
            displayName: Node Feature Discovery
            kind: NodeFeatureDiscovery
            name: nodefeaturediscoveries.nfd.openshift.io
            version: v1alpha1
        description: "The NFD operator creates and maintiains the Node Feature Discovery
          (NFD) on Kubernetes. \nIt detects hardware features available on each node
          in a Kubernetes cluster, and advertises those features using node labels.\n"
        displayName: Node Feature Discovery
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.2.0
      name: 4.2-s390x
    - currentCSV: nfd.4.3.2-202002112006
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "nfd.openshift.io/v1alpha1",
                "kind": "NodeFeatureDiscovery",
                "metadata": {
                  "name": "nfd-master-server"
                },
                "spec": {
                  "namespace": "openshift-nfd"
                }
              }
            ]
          capabilities: Basic Install
          categories: Database
          certified: "false"
          containerImage: ""
          createdAt: "2019-05-30T00:00:00Z"
          description: This software enables node feature discovery for Kubernetes.
            It detects hardware features available on each node in a Kubernetes cluster,
            and advertises those features using node labels.
          provider: Red Hat
          repository: https://github.com/openshift/cluster-nfd-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: "The NFD operator creates and maintains the Node Feature
              Discovery (NFD) on Kubernetes. \nIt detects hardware features available
              on each node in a Kubernetes cluster, and advertises those features
              using node labels.\n"
            displayName: Node Feature Discovery
            kind: NodeFeatureDiscovery
            name: nodefeaturediscoveries.nfd.openshift.io
            version: v1alpha1
        description: "The NFD operator creates and maintiains the Node Feature Discovery
          (NFD) on Kubernetes. \nIt detects hardware features available on each node
          in a Kubernetes cluster, and advertises those features using node labels.\n"
        displayName: Node Feature Discovery
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.3.0
      name: "4.3"
    defaultChannel: "4.3"
    packageName: nfd
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: 3scale-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/3scale-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: 3scale-operator.v0.3.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager"},"spec":{"wildcardDomain":"example.com"}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager-ha"},"spec":{"highAvailability":{"enabled":true},"wildcardDomain":"example.com"}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager-s3"},"spec":{"system":{"fileStorage":{"amazonSimpleStorageService":{"awsBucket":"\u003cbucket-name\u003e","awsCredentialsSecret":{"name":"\u003ccredentials-secret-name\u003e"},"awsRegion":"\u003cregion\u003e"}}},"wildcardDomain":"\u003cdesired-domain\u003e"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"API","metadata":{"labels":{"environment":"testing"},"name":"example-api"},"spec":{"description":"api01","integrationMethod":{"apicastHosted":{"apiTestGetRequest":"/","authenticationSettings":{"credentials":{"apiKey":{"authParameterName":"user-key","credentialsLocation":"headers"}},"errors":{"authenticationFailed":{"contentType":"text/plain;
            charset=us-ascii","responseBody":"Authentication failed","responseCode":403},"authenticationMissing":{"contentType":"text/plain;
            charset=us-ascii","responseBody":"Authentication Missing","responseCode":403}},"hostHeader":"","secretToken":"MySecretTokenBetweenApicastAndMyBackend_1237120312"},"mappingRulesSelector":{"matchLabels":{"api":"api01"}},"privateBaseURL":"https://echo-api.3scale.net:443"}}}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Binding","metadata":{"name":"example-binding"},"spec":{"APISelector":{"matchLabels":{"environment":"testing"}},"credentialsRef":{"name":"ecorp-tenant-secret"}}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Limit","metadata":{"labels":{"api":"api01"},"name":"plan01-metric01-day-10"},"spec":{"description":"Limit
            for metric01 in plan01","maxValue":10,"metricRef":{"name":"metric01"},"period":"day"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"MappingRule","metadata":{"labels":{"api":"api01"},"name":"metric01-get-path01"},"spec":{"increment":1,"method":"GET","metricRef":{"name":"metric01"},"path":"/path01"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Metric","metadata":{"labels":{"api":"api01"},"name":"metric01"},"spec":{"description":"metric01","incrementHits":false,"unit":"hit"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Plan","metadata":{"labels":{"api":"api01"},"name":"example-plan"},"spec":{"approvalRequired":false,"costs":{"costMonth":0,"setupFee":0},"default":true,"limitSelector":{"matchLabels":{"plan":"plan01"}},"trialPeriod":0}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Tenant","metadata":{"name":"example-tenant"},"spec":{"email":"admin@example.com","masterCredentialsRef":{"name":"system-seed"},"organizationName":"Example.com","passwordCredentialsRef":{"name":"ecorp-admin-secret"},"systemMasterUrl":"https://master.example.com","tenantSecretRef":{"name":"ecorp-tenant-secret","namespace":"operator-test"},"username":"admin"}}]'
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: registry.redhat.io/3scale-amp26/3scale-operator
          createdAt: "2019-05-30 22:40:00"
          description: 3scale Operator to provision 3scale and publish/manage API
          repository: https://github.com/3scale/3scale-operator
          support: Red Hat, Inc.
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: API Manager
            displayName: API Manager
            kind: APIManager
            name: apimanagers.apps.3scale.net
            version: v1alpha1
          - description: API
            displayName: API
            kind: API
            name: apis.capabilities.3scale.net
            version: v1alpha1
          - description: Binding
            displayName: Binding
            kind: Binding
            name: bindings.capabilities.3scale.net
            version: v1alpha1
          - description: Limit
            displayName: Limit
            kind: Limit
            name: limits.capabilities.3scale.net
            version: v1alpha1
          - description: MappingRule
            displayName: MappingRule
            kind: MappingRule
            name: mappingrules.capabilities.3scale.net
            version: v1alpha1
          - description: Metric
            displayName: Metric
            kind: Metric
            name: metrics.capabilities.3scale.net
            version: v1alpha1
          - description: Plan
            displayName: Plan
            kind: Plan
            name: plans.capabilities.3scale.net
            version: v1alpha1
          - description: Tenant
            displayName: Tenant
            kind: Tenant
            name: tenants.capabilities.3scale.net
            version: v1alpha1
        description: |
          The 3scale Operator creates and maintains the Red Hat 3scale API Management on [OpenShift](https://www.openshift.com/) in various deployment configurations.

          [3scale API Management](https://www.redhat.com/en/technologies/jboss-middleware/3scale) makes it easy to manage your APIs.
          Share, secure, distribute, control, and monetize your APIs on an infrastructure platform built for performance, customer control, and future growth.

          ### Supported Features
          * **Installer** A way to install a 3scale API Management solution, providing configurability options at the time of installation
          * **Capabilities** Ability to define 3scale API definitions and set them into a 3scale API Management solution

          ### Upgrading your installation
          Currently upgrading feature is not available.

          ### Documentation
          Documentation can be found on our [website](https://github.com/3scale/3scale-operator/blob/v0.3.0/doc/user-guide.md).

          ### Getting help
          If you encounter any issues while using 3scale operator, you can create an issue on our [website](https://github.com/3scale/3scale-operator) for bugs, enhancements, or other requests.

          ### Contributing
          You can contribute by:

          * Raising any issues you find using 3scale Operator
          * Fixing issues by opening [Pull Requests](https://github.com/3scale/3scale-operator/pulls)
          * Improving [documentation](https://github.com/3scale/3scale-operator/blob/v0.3.0/doc/user-guide.md)
          * Talking about 3scale Operator

          All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/3scale-operator/issues).

          ### License
          3scale Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/3scale-operator/blob/master/LICENSE)
        displayName: 3scale Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.3.0
      name: threescale-2.6
    - currentCSV: 3scale-operator.v0.4.2
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager"},"spec":{"wildcardDomain":"example.com"}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager-ha"},"spec":{"highAvailability":{"enabled":true},"wildcardDomain":"example.com"}},
            {"apiVersion":"apps.3scale.net/v1alpha1","kind":"APIManager","metadata":{"name":"example-apimanager-s3"},"spec":{"system":{"fileStorage":{"amazonSimpleStorageService":{"awsBucket":"\u003cbucket-name\u003e","awsCredentialsSecret":{"name":"\u003ccredentials-secret-name\u003e"},"awsRegion":"\u003cregion\u003e"}}},"wildcardDomain":"\u003cdesired-domain\u003e"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"API","metadata":{"labels":{"environment":"testing"},"name":"example-api"},"spec":{"description":"api01","integrationMethod":{"apicastHosted":{"apiTestGetRequest":"/","authenticationSettings":{"credentials":{"apiKey":{"authParameterName":"user-key","credentialsLocation":"headers"}},"errors":{"authenticationFailed":{"contentType":"text/plain;
            charset=us-ascii","responseBody":"Authentication failed","responseCode":403},"authenticationMissing":{"contentType":"text/plain;
            charset=us-ascii","responseBody":"Authentication Missing","responseCode":403}},"hostHeader":"","secretToken":"MySecretTokenBetweenApicastAndMyBackend_1237120312"},"mappingRulesSelector":{"matchLabels":{"api":"api01"}},"privateBaseURL":"https://echo-api.3scale.net:443"}}}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Binding","metadata":{"name":"example-binding"},"spec":{"APISelector":{"matchLabels":{"environment":"testing"}},"credentialsRef":{"name":"ecorp-tenant-secret"}}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Limit","metadata":{"labels":{"api":"api01"},"name":"plan01-metric01-day-10"},"spec":{"description":"Limit
            for metric01 in plan01","maxValue":10,"metricRef":{"name":"metric01"},"period":"day"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"MappingRule","metadata":{"labels":{"api":"api01"},"name":"metric01-get-path01"},"spec":{"increment":1,"method":"GET","metricRef":{"name":"metric01"},"path":"/path01"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Metric","metadata":{"labels":{"api":"api01"},"name":"metric01"},"spec":{"description":"metric01","incrementHits":false,"unit":"hit"}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Plan","metadata":{"labels":{"api":"api01"},"name":"example-plan"},"spec":{"approvalRequired":false,"costs":{"costMonth":0,"setupFee":0},"default":true,"limitSelector":{"matchLabels":{"plan":"plan01"}},"trialPeriod":0}},
            {"apiVersion":"capabilities.3scale.net/v1alpha1","kind":"Tenant","metadata":{"name":"example-tenant"},"spec":{"email":"admin@example.com","masterCredentialsRef":{"name":"system-seed"},"organizationName":"Example.com","passwordCredentialsRef":{"name":"ecorp-admin-secret"},"systemMasterUrl":"https://master.example.com","tenantSecretRef":{"name":"ecorp-tenant-secret","namespace":"operator-test"},"username":"admin"}}]'
          capabilities: Full Lifecycle
          categories: Integration & Delivery
          certified: "false"
          containerImage: registry.redhat.io/3scale-amp2/3scale-rhel7-operator:1.10-6
          createdAt: "2019-05-30 22:40:00"
          description: 3scale Operator to provision 3scale and publish/manage API
          repository: https://github.com/3scale/3scale-operator
          support: Red Hat, Inc.
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: API Manager
            displayName: API Manager
            kind: APIManager
            name: apimanagers.apps.3scale.net
            version: v1alpha1
          - description: API
            displayName: API
            kind: API
            name: apis.capabilities.3scale.net
            version: v1alpha1
          - description: Binding
            displayName: Binding
            kind: Binding
            name: bindings.capabilities.3scale.net
            version: v1alpha1
          - description: Limit
            displayName: Limit
            kind: Limit
            name: limits.capabilities.3scale.net
            version: v1alpha1
          - description: MappingRule
            displayName: MappingRule
            kind: MappingRule
            name: mappingrules.capabilities.3scale.net
            version: v1alpha1
          - description: Metric
            displayName: Metric
            kind: Metric
            name: metrics.capabilities.3scale.net
            version: v1alpha1
          - description: Plan
            displayName: Plan
            kind: Plan
            name: plans.capabilities.3scale.net
            version: v1alpha1
          - description: Tenant
            displayName: Tenant
            kind: Tenant
            name: tenants.capabilities.3scale.net
            version: v1alpha1
        description: |
          The 3scale Operator creates and maintains the Red Hat 3scale API Management on [OpenShift](https://www.openshift.com/) in various deployment configurations.

          [3scale API Management](https://www.redhat.com/en/technologies/jboss-middleware/3scale) makes it easy to manage your APIs.
          Share, secure, distribute, control, and monetize your APIs on an infrastructure platform built for performance, customer control, and future growth.

          ### Supported Features
          * **Installer** A way to install a 3scale API Management solution, providing configurability options at the time of installation
          * **Upgrade** Upgrade from previously installed 3scale API Management solution
          * **Reconcilliation** Tunable CRD parameters after 3scale API Management solution is installed
          * **Capabilities** Ability to define 3scale API definitions and set them into a 3scale API Management solution

          ### Documentation
          [3scale api management](https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management)
          [Deploying 3scale using the operator](https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.7/html/installing_3scale/install-threescale-on-openshift-guide#deploying-threescale-using-the-operator)

          ### Getting help
          If you encounter any issues while using 3scale operator, you can create an issue on our [Github repo](https://github.com/3scale/3scale-operator) for bugs, enhancements, or other requests.

          ### Contributing
          You can contribute by:

          * Raising any issues you find using 3scale Operator
          * Fixing issues by opening [Pull Requests](https://github.com/3scale/3scale-operator/pulls)
          * Improving [documentation](https://github.com/3scale/3scale-operator/blob/v0.4.0/doc/user-guide.md)
          * Talking about 3scale Operator

          All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/3scale/3scale-operator/issues).

          ### License
          3scale Operator is licensed under the [Apache 2.0 license](https://github.com/3scale/3scale-operator/blob/master/LICENSE)
        displayName: 3scale
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.4.1
      name: threescale-2.7
    defaultChannel: threescale-2.7
    packageName: 3scale-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: NeuVector
      provider-url: ""
    name: neuvector-community-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/neuvector-community-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: neuvector-operator.v0.9.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "apm.neuvector.com/v1alpha1",
                "kind": "Neuvector",
                "metadata": {
                  "name": "example-neuvector"
                },
                "spec": {
                  "admissionwebhook": {
                    "type": "ClusterIP"
                  },
                  "containerd": {
                    "enabled": false,
                    "path": "/var/run/containerd/containerd.sock"
                  },
                  "controller": {
                    "apisvc": {
                      "type": null
                    },
                    "azureFileShare": {
                      "enabled": false,
                      "secretName": null,
                      "shareName": null
                    },
                    "configmap": {
                      "data": null,
                      "enabled": false
                    },
                    "enabled": true,
                    "federation": {
                      "managedsvc": {
                        "type": null
                      },
                      "mastersvc": {
                        "type": null
                      }
                    },
                    "image": {
                      "repository": "neuvector/controller"
                    },
                    "ingress": {
                      "annotations": {
                        "ingress.kubernetes.io/protocol": "https"
                      },
                      "enabled": false,
                      "host": null,
                      "path": "/",
                      "secretName": null,
                      "tls": false
                    },
                    "pvc": {
                      "accessModes": [
                        "ReadWriteMany"
                      ],
                      "enabled": false,
                      "storageClass": null
                    },
                    "replicas": 3,
                    "strategy": {
                      "rollingUpdate": {
                        "maxSurge": 1,
                        "maxUnavailable": 0
                      },
                      "type": "RollingUpdate"
                    }
                  },
                  "crdwebhook": {
                    "type": "ClusterIP"
                  },
                  "crio": {
                    "enabled": true,
                    "path": "/var/run/crio/crio.sock"
                  },
                  "cve": {
                    "updater": {
                      "enabled": false,
                      "image": {
                        "repository": "neuvector/updater",
                        "tag": "latest"
                      },
                      "schedule": "0 0 * * *"
                    }
                  },
                  "docker": {
                    "enabled": false,
                    "path": "/var/run/docker.sock"
                  },
                  "enforcer": {
                    "enabled": true,
                    "image": {
                      "repository": "neuvector/enforcer"
                    },
                    "tolerations": [
                      {
                        "effect": "NoSchedule",
                        "key": "node-role.kubernetes.io/master"
                      }
                    ]
                  },
                  "exporter": {
                    "CTRL_PASSWORD": "admin",
                    "CTRL_USERNAME": "admin",
                    "enabled": false,
                    "image": {
                      "repository": "neuvector/prometheus-exporter",
                      "tag": "0.9.0"
                    },
                    "scrapping": true
                  },
                  "imagePullSecrets": "regsecret",
                  "manager": {
                    "enabled": true,
                    "env": {
                      "ssl": true
                    },
                    "image": {
                      "repository": "neuvector/manager"
                    },
                    "ingress": {
                      "annotations": {},
                      "enabled": false,
                      "host": null,
                      "path": "/",
                      "secretName": null,
                      "tls": false
                    },
                    "svc": {
                      "type": "NodePort"
                    }
                  },
                  "openshift": true,
                  "registry": "docker.io",
                  "resources": {},
                  "tag": "latest"
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Monitoring, Networking, Security
          certified: "false"
          containerImage: neuvector/neuvector-operator:v0.0.1
          createdAt: "2019-11-18T02:09:59Z"
          description: NeuVector delivers the only cloud-native Kubernetes security
            platform with uncompromising end-to-end protection from DevOps vulnerability
            protection to automated run-time security, and featuring a true Layer
            7 container firewall.
          repository: https://github.com/neuvector/neuvector-operator
          support: support@neuvector.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Full LifeCycle Container Security Platform
            displayName: Neuvector
            kind: Neuvector
            name: neuvectors.apm.neuvector.com
            version: v1alpha1
        description: "NeuVector delivers the only cloud-native Kubernetes security
          platform with uncompromising end-to-end protection from DevOps vulnerability
          protection to automated run-time security, and featuring a true Layer 7
          container firewall.\n\nThe NeuVector Operator runs  in the openshift container
          platform to deploy and manage the NeuVector Security cluster components.
          The NeuVector operator contains all necessary information to deploy NeuVector
          using helm charts. You simply need to install the NeuVector operator from
          the OpenShift embeded operator hub and create NeuVector instance. You can
          modify the NeuVector installation configuration by modifying yaml while
          creating the NeuVector instance such as imagePullSecrets, tag version, etc.
          Please refer to https://github.com/neuvector/neuvector-helm for the values
          that can be modifed during installation. To upgrade to a newer version of
          NeuVector, just reapply the NeuVector instance with desired tag , which
          in turn pulls the specified NeuVector image tags and upgrades as per upgrade
          plan configured on the helm chart. \n\n**Complete below steps to create
          secret for accessing Docker or similar registry and Grant Service Account
          Access to the Privileged SCC before installation.**\n\nCreate the NeuVector
          namespace\n\n         oc new-project  neuvector\nConfigure OpenShift to
          pull images from the private NeuVector registry on Docker Hub\n\n         oc
          create secret docker-registry regsecret -n neuvector --docker-server=https://index.docker.io/v1/
          --docker-username=your-name --docker-password=your-pword --docker-email=your-email\n\t\t\t\t
          \n\nWhere â€™your-nameâ€™ is your Docker username, â€™your-pwordâ€™ is your Docker
          password, â€™your-emailâ€™ is your Docker email.\n\nLogin as system:admin account\n\n
          \        oc login -u system:admin\n\nGrant Service Account Access to the
          Privileged SCC\n\n         oc -n neuvector adm policy add-scc-to-user privileged
          -z default\n\nThe following info will be added in the Privileged SCC users:\n\n
          \        - system:serviceaccount:neuvector:default\n\n\n**Add Neuvector
          license from NeuVector WebUI->setting**"
        displayName: Neuvector Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: NeuVector
        version: 0.9.0
      name: beta
    defaultChannel: beta
    packageName: neuvector-community-operator
    provider:
      name: NeuVector
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: PerceptiLabs
      provider-url: ""
    name: perceptilabs-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/perceptilabs-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: perceptilabs-operator-trial.v1.0.8
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"perceptilabs.com/v1","kind":"PerceptiLabs","metadata":{"name":"example-perceptilabs"},"spec":{"namespace":"your-namespace","corePvc":"perceptilabs-pvc","coreGpus":0,"license_name":"demo","license_value":"demo"}}]'
          capabilities: Basic Install
          categories: AI/Machine Learning
          certified: "false"
          containerImage: registry.connect.redhat.com/perceptilabs/modeling-operator-trial-1:1.0.8
          createdAt: "2019-10-02T12:00:00Z"
          description: AI platform which lets you Build, Train and Analyze
          support: support@perceptilabs.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: The PerceptiLabs Modeling tool
            displayName: PerceptiLabs
            kind: PerceptiLabs
            name: perceptilabs.perceptilabs.com
            version: v1
        description: "The PerceptiLabs operator creates and maintains PerceptiLabs,
          a visual tool modeling for machine learning at warp speed.\n\nPerceptiLabs
          visual modeling tool provides a GUI for building,\ntraining, and assessing
          your models, while also enabling deeper\ndevelopment with code. You get
          faster iterations and better\nexplainability of your results.\n\nFor more
          information visit [http://perceptilabs.com](http://perceptilabs.com).\n\n#
          Features\n\n**Fast modeling**  \nMake changes, debug, and tune your model
          through the GUI of custom code\neditor where every component/layer is reprogrammable.
          Choose from\nmultiple neural network models as well as classical AI methods.\n\n\n**Transparency
          of Model Performance and Results**  \nGet instant feedback about your model's
          performance through the\nvisualization of the architecture, to better review
          and understand the\nresults. See real-time analytics in every operation
          and variable, and\ngranular previews of output from each model component.\n\n\n**Flexibility**
          \ \nCustomize your environment and statistics dashboard. Use high-level\nabstractions
          or low-level code. Execute any custom Python code or export\na fully trained
          TensorFlow model to perform inference in your projects.\n\n\n\n# Installation
          Instructions  \nFor your convenience, we've included an example quickstart
          for running PerceptiLabs in demo mode.\n\n## Prepare your namespace  \nChoose
          or create the namespace into which you'd like install PerceptiLabs. For
          example:  \n```\noc create namespace REPLACE_NAMESPACE\n```\n\n## Prepare
          storage for your data\n\nYou'll need to have a place on your cluster for
          storing training data and models.\n\nHere's an example configuration for
          creating storage on a cluster hosted on AWS that you can tailor to your
          needs:\n\n```\nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n
          \ name: perceptilabs-example-sc\n  annotations:\n    description: Example
          Storage for PerceptiLabs\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n
          \ fsType: ext4\n  type: gp2\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\n---\nkind:
          PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: REPLACE_PVC_NAME\n
          \ namespace: REPLACE_NAMESPACE\nspec:\n  storageClassName: perceptilabs-example-sc\n
          \ volumeMode: Filesystem\n  accessModes:\n    - ReadWriteOnce\n  resources:\n
          \   requests:\n      storage: 50Gi\n```\n\n## Create the service account\n\nChoose
          or create a service account to run PerceptiLabs. If you want a new service
          account, create it like so:\n\n```\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n
          \ name: REPLACE_SERVICEACCOUNT_NAME\n  namespace: REPLACE_NAMESPACE\n```\n\n##
          Subscribe to the PerceptiLabs operator in your namespace\n\nIf you're using
          the OpenShift console webpage, just click the Install button on this operator.
          If not, you can customize and apply this configuration:\n\n```\napiVersion:
          operators.coreos.com/v1\nkind: OperatorGroup\nmetadata:\n  name: REPLACE_NAMESPACE-operatorgroup\n
          \ namespace: REPLACE_NAMESPACE\nspec:\n  targetNamespaces:\n  - REPLACE_NAMESPACE\n---\napiVersion:
          operators.coreos.com/v1alpha1\nkind: Subscription\nmetadata:\n  name: perceptilabs-operator-trial\n
          \ namespace: REPLACE_NAMESPACE\nspec:\n  channel: stable\n  name: perceptilabs-operator-trial\n
          \ source: perceptilabs-operators\n  sourceNamespace: openshift-marketplace\n
          \ namespace: REPLACE_NAMESPACE\n```\n\nAfter this, you should see a `perceptilabs-operator-trial`
          pod start up in your namespace. In that pod, the log for the `operator`
          container should eventually say \"starting to serve\".\n\n## Start a copy
          of PerceptiLabs\n\nThis is where you connect your storage and service account
          to a PerceptiLabs instance and run it. You can customize and apply the following
          configuration:\n\n```\napiVersion: perceptilabs.com/v1\nkind: PerceptiLabs\nmetadata:\n
          \ name: example-perceptilabs\n  namespace: REPLACE_NAMESPACE\nspec:\n  serviceAccountName:
          default  namespace: REPLACE_NAMESPACE\n  corePvc: REPLACE_PVC_NAME\n```\n\nAt
          this point two pods named 'perceptilabs-core-...' and 'perceptilabs-frontend-...`
          will start up in your namespace.\n\n## Copy data files to your cluster\n\nIf
          you've used the persistent storage configuration from above, then you have
          a read-write volume mounted in the pod at `/mnt/plabs`. Copy your files
          there:\n\n```\noc cp REPLACE_FILENAME --namespace=REPLACE_NAMESPACE REPLACE_CORE_POD_NAME:/mnt/plabs
          --container=core\n```\n\n## Get the URL of your PerceptiLabs\n\nOnce everything
          is up and running, you'll have two new routes in your namespace. Go to the
          routes for your namespace and follow the link named `perceptilabs-frontend`.
          Your browser will be connected to your instance of PerceptiLabs! Alternatively,
          you can get the URL from the command line:\n\n```\noc get routes --namespace
          REPLACE_NAMESPACE perceptilabs-frontend\n```"
        displayName: PerceptiLabs Operator Trial
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: PerceptiLabs
        version: 1.0.8
      name: stable
    defaultChannel: stable
    packageName: perceptilabs-operator
    provider:
      name: PerceptiLabs
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc
      provider-url: ""
    name: elasticsearch-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/elasticsearch-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: elasticsearch-operator.4.2.18-202002031246
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
                {
                    "apiVersion": "logging.openshift.io/v1",
                    "kind": "Elasticsearch",
                    "metadata": {
                      "name": "elasticsearch"
                    },
                    "spec": {
                      "managementState": "Managed",
                      "nodeSpec": {
                        "image": "registry.redhat.io/openshift4/ose-logging-elasticsearch5@sha256:1e59909fd95232d950087bf44eb96ea4fce769d86479ad303cd552f152cbe960",
                        "resources": {
                          "limits": {
                            "memory": "1Gi"
                          },
                          "requests": {
                            "memory": "512Mi"
                          }
                        }
                      },
                      "redundancyPolicy": "SingleRedundancy",
                      "nodes": [
                        {
                            "nodeCount": 1,
                            "roles": ["client","data","master"]
                        }
                      ]
                    }
                }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-elasticsearch-operator@sha256:6635be4bf7a218d2aead6039f99bbe68695223c57522c2eb74b6d5fcddfb9401
          createdAt: "2019-02-20T08:00:00Z"
          description: |-
            The Elasticsearch Operator for OKD provides a means for configuring and managing an Elasticsearch cluster for tracing and cluster logging.
            ## Prerequisites and Requirements
            ### Elasticsearch Operator Namespace
            The Elasticsearch Operator must be deployed to the global operator group namespace
            ### Memory Considerations
            Elasticsearch is a memory intensive application.  The initial
            set of OKD nodes may not be large enough to support the Elasticsearch cluster.  Additional OKD nodes must be added
            to the OKD cluster if you desire to run with the recommended (or better) memory. Each ES node can operate with a
            lower memory setting though this is not recommended for production deployments.
          olm.skipRange: '>=4.1.0 <4.2.18-202002031246'
          support: AOS Cluster Logging, Jaeger
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Elasticsearch cluster instance
            displayName: Elasticsearch
            kind: Elasticsearch
            name: elasticsearches.logging.openshift.io
            version: v1
        description: |
          The Elasticsearch Operator for OKD provides a means for configuring and managing an Elasticsearch cluster for use in tracing and cluster logging.
          This operator only supports OKD Cluster Logging and Jaeger.  It is tightly coupled to each and is not currently capable of
          being used as a general purpose manager of Elasticsearch clusters running on OKD.

          It is recommended this operator be deployed to the **openshift-operators** namespace to properly support the Cluster Logging and Jaeger use cases.

          Once installed, the operator provides the following features:
          * **Create/Destroy**: Deploy an Elasticsearch cluster to the same namespace in which the Elasticsearch custom resource is created.
        displayName: Elasticsearch Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc
        version: 4.2.18-202002031246
      name: "4.2"
    - currentCSV: elasticsearch-operator.4.2.18-202002031246-s390x
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
                {
                    "apiVersion": "logging.openshift.io/v1",
                    "kind": "Elasticsearch",
                    "metadata": {
                      "name": "elasticsearch"
                    },
                    "spec": {
                      "managementState": "Managed",
                      "nodeSpec": {
                        "image": "registry.redhat.io/openshift4/ose-logging-elasticsearch5@sha256:2da34e2311c20506cdbe0056bae8f68d4ca84e0ce4d5eea577a5fa4d27040575",
                        "resources": {
                          "limits": {
                            "memory": "1Gi"
                          },
                          "requests": {
                            "memory": "512Mi"
                          }
                        }
                      },
                      "redundancyPolicy": "SingleRedundancy",
                      "nodes": [
                        {
                            "nodeCount": 1,
                            "roles": ["client","data","master"]
                        }
                      ]
                    }
                }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-elasticsearch-operator@sha256:54ab9266814e65b35eb01b7c7364c5e70ab86962e5610664c908a4311b4b0ca3
          createdAt: "2019-02-20T08:00:00Z"
          description: |-
            The Elasticsearch Operator for OKD provides a means for configuring and managing an Elasticsearch cluster for tracing and cluster logging.
            ## Prerequisites and Requirements
            ### Elasticsearch Operator Namespace
            The Elasticsearch Operator must be deployed to the global operator group namespace
            ### Memory Considerations
            Elasticsearch is a memory intensive application.  The initial
            set of OKD nodes may not be large enough to support the Elasticsearch cluster.  Additional OKD nodes must be added
            to the OKD cluster if you desire to run with the recommended (or better) memory. Each ES node can operate with a
            lower memory setting though this is not recommended for production deployments.
          olm.skipRange: '>=4.1.0 <4.2.18-202002031246'
          support: AOS Cluster Logging, Jaeger
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Elasticsearch cluster instance
            displayName: Elasticsearch
            kind: Elasticsearch
            name: elasticsearches.logging.openshift.io
            version: v1
        description: |
          The Elasticsearch Operator for OKD provides a means for configuring and managing an Elasticsearch cluster for use in tracing and cluster logging.
          This operator only supports OKD Cluster Logging and Jaeger.  It is tightly coupled to each and is not currently capable of
          being used as a general purpose manager of Elasticsearch clusters running on OKD.

          It is recommended this operator be deployed to the **openshift-operators** namespace to properly support the Cluster Logging and Jaeger use cases.

          Once installed, the operator provides the following features:
          * **Create/Destroy**: Deploy an Elasticsearch cluster to the same namespace in which the Elasticsearch custom resource is created.
        displayName: Elasticsearch Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc
        version: 4.2.18-202002031246
      name: 4.2-s390x
    - currentCSV: elasticsearch-operator.4.3.1-202002032140
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
                {
                    "apiVersion": "logging.openshift.io/v1",
                    "kind": "Elasticsearch",
                    "metadata": {
                      "name": "elasticsearch"
                    },
                    "spec": {
                      "managementState": "Managed",
                      "nodeSpec": {
                        "image": "registry.redhat.io/openshift4/ose-logging-elasticsearch5@sha256:c360ab6acbac3d10989a4e8b0054e277e2584c737f8371c48a054d314fd1e94b",
                        "resources": {
                          "limits": {
                            "memory": "1Gi"
                          },
                          "requests": {
                            "memory": "512Mi"
                          }
                        }
                      },
                      "redundancyPolicy": "SingleRedundancy",
                      "nodes": [
                        {
                            "nodeCount": 1,
                            "roles": ["client","data","master"]
                        }
                      ]
                    }
                }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-elasticsearch-operator@sha256:b604641f95c9762ff9b1c9d550cec908d9caab3cc333120e7cf60a55539b8149
          createdAt: "2019-02-20T08:00:00Z"
          description: |-
            The Elasticsearch Operator for OKD provides a means for configuring and managing an Elasticsearch cluster for tracing and cluster logging.
            ## Prerequisites and Requirements
            ### Elasticsearch Operator Namespace
            The Elasticsearch Operator must be deployed to the global operator group namespace
            ### Memory Considerations
            Elasticsearch is a memory intensive application.  The initial
            set of OKD nodes may not be large enough to support the Elasticsearch cluster.  Additional OKD nodes must be added
            to the OKD cluster if you desire to run with the recommended (or better) memory. Each ES node can operate with a
            lower memory setting though this is not recommended for production deployments.
          olm.skipRange: '>=4.2.0 <4.3.0'
          support: AOS Cluster Logging, Jaeger
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Elasticsearch cluster instance
            displayName: Elasticsearch
            kind: Elasticsearch
            name: elasticsearches.logging.openshift.io
            version: v1
        description: |
          The Elasticsearch Operator for OKD provides a means for configuring and managing an Elasticsearch cluster for use in tracing and cluster logging.
          This operator only supports OKD Cluster Logging and Jaeger.  It is tightly coupled to each and is not currently capable of
          being used as a general purpose manager of Elasticsearch clusters running on OKD.

          It is recommended this operator be deployed to the **openshift-operators** namespace to properly support the Cluster Logging and Jaeger use cases.

          Once installed, the operator provides the following features:
          * **Create/Destroy**: Deploy an Elasticsearch cluster to the same namespace in which the Elasticsearch custom resource is created.
        displayName: Elasticsearch Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc
        version: 4.3.1-202002032140
      name: "4.3"
    - currentCSV: elasticsearch-operator.4.1.31-202001140447
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
                {
                    "apiVersion": "logging.openshift.io/v1",
                    "kind": "Elasticsearch",
                    "metadata": {
                      "name": "elasticsearch"
                    },
                    "spec": {
                      "managementState": "Managed",
                      "nodeSpec": {
                        "image": "registry.redhat.io/openshift4/ose-logging-elasticsearch5@sha256:b6fb90ecfb602e33015e430aa481c174b0b1d7ecda01737fe16ec6269e0c4845",
                        "resources": {
                          "limits": {
                            "memory": "1Gi"
                          },
                          "requests": {
                            "memory": "512Mi"
                          }
                        }
                      },
                      "nodes": [
                        {
                            "nodeCount": 1,
                            "roles": ["client","data","master"],
                            "redundancyPolicy": "SingleRedundancy"
                        }
                      ]
                    }
                }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional, Logging & Tracing
          certified: "false"
          containerImage: registry.redhat.io/openshift4/ose-elasticsearch-operator@sha256:ada9eb2c427698b497164d65ff41af254cf99b8d0effd37c0d6c519292c556d2
          createdAt: "2019-02-20T08:00:00Z"
          description: |-
            The Elasticsearch Operator for OKD provides a means for configuring and managing an Elasticsearch cluster for tracing and cluster logging.
            ## Prerequisites and Requirements
            ### Elasticsearch Operator Namespace
            The Elasticsearch Operator must be deployed to the global operator group namespace
            ### Memory Considerations
            Elasticsearch is a memory intensive application.  The initial
            set of OKD nodes may not be large enough to support the Elasticsearch cluster.  Additional OKD nodes must be added
            to the OKD cluster if you desire to run with the recommended(or better) memory. Each ES node can operate with a
            lower memory setting though this is not recommended for production deployments.
          olm.skipRange: '>=4.1.0 <4.1.31-202001140447'
          support: AOS Cluster Logging, Jaeger
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Elasticsearch cluster instance
            displayName: Elasticsearch
            kind: Elasticsearch
            name: elasticsearches.logging.openshift.io
            version: v1
        description: |
          The Elasticsearch Operator for OKD provides a means for configuring and managing an Elasticsearch cluster for use in tracing and cluster logging.
          This operator only supports OKD Cluster Logging and Jaeger.  It is tightly coupled to each and is not currently capable of
          being used as a general purpose manager of Elasticsearch clusters running on OKD.

          It is recommended this operator be deployed to the **openshift-operators** namespace to properly support the Cluster Logging and Jaeger use cases.

          Once installed, the operator provides the following features:
          * **Create/Destroy**: Deploy an Elasticsearch cluster to the same namespace in which the Elasticsearch custom resource is created.
        displayName: Elasticsearch Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat, Inc
        version: 4.1.31-202001140447
      name: preview
    defaultChannel: "4.3"
    packageName: elasticsearch-operator
    provider:
      name: Red Hat, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Lightbend, Inc.
      provider-url: ""
    name: akka-cluster-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/akka-cluster-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: akka-cluster-operator.v0.2.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "app.lightbend.com/v1alpha1",
                "kind": "AkkaCluster",
                "metadata": {
                  "name": "akka-cluster-demo"
                },
                "spec": {
                  "replicas": 3,
                  "template": {
                    "spec": {
                      "containers": [
                        {
                          "name": "main",
                          "image": "lightbend-docker-registry.bintray.io/lightbend/akka-cluster-demo:1.0.2",
                          "readinessProbe": {
                            "httpGet": {
                              "path": "/ready",
                              "port": "management"
                            },
                            "periodSeconds": 10,
                            "failureThreshold": 10,
                            "initialDelaySeconds": 20
                          },
                          "livenessProbe": {
                            "httpGet": {
                              "path": "/alive",
                              "port": "management"
                            },
                            "periodSeconds": 10,
                            "failureThreshold": 10,
                            "initialDelaySeconds": 20
                          },
                          "ports": [
                            {
                              "name": "http",
                              "containerPort": 8080
                            },
                            {
                              "name": "remoting",
                              "containerPort": 2552
                            },
                            {
                              "name": "management",
                              "containerPort": 8558
                            }
                          ]
                        }
                      ]
                    }
                  }
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Application Runtime
          certified: "false"
          containerImage: lightbend-docker-registry.bintray.io/lightbend/akkacluster-operator:v0.2.0
          createdAt: "2019-06-28T15:23:00Z"
          description: Run Akka Cluster applications on Kubernetes.
          repository: https://github.com/lightbend/akka-cluster-operator
          support: Lightbend, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An example Akka Cluster app that provides cluster visualization.
            displayName: Akka Cluster
            kind: AkkaCluster
            name: akkaclusters.app.lightbend.com
            version: v1alpha1
        description: |
          The Akka Cluster Operator allows you to manage applications designed for
          [Akka Cluster](https://doc.akka.io/docs/akka/current/common/cluster.html).
          Clustering with [Akka](https://doc.akka.io/docs/akka/current/guide/introduction.html) provides a
          fault-tolerant, decentralized, peer-to-peer based cluster
          for building stateful, distributed applications with no single point of failure.
          Developers should use Akka Management v1.x or newer, with both Bootstrap and HTTP modules enabled.
          When deploying using the Akka Cluster Operator, only the `management port` needs to be defined.
          Defaults are provided by the Operator for all other required configuration.
          The Akka Cluster Operator provides scalability control and membership status information
          for deployed applications using Akka Cluster. As part of supervising membership of running clusters,
          this Operator creates a pod-listing ServiceAccount, Role, and RoleBinding suitable for
          each application. See the project [Readme](https://github.com/lightbend/akka-cluster-operator/blob/master/README.md)
          for more information and details.
          This is an incubating project in alpha version.
        displayName: Akka Cluster Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Lightbend, Inc.
        version: 0.2.0
      name: alpha
    defaultChannel: alpha
    packageName: akka-cluster-operator
    provider:
      name: Lightbend, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Seldon Technologies
      provider-url: ""
    name: seldon-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/seldon-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: seldonoperator.v0.1.5
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion": "machinelearning.seldon.io/v1alpha2","kind":
            "SeldonDeployment","metadata": {"labels": {"app": "seldon"},"name": "seldon-model"},"spec":
            {"name": "test-deployment","oauth_key": "oauth-key","oauth_secret": "oauth-secret","predictors":
            [{"componentSpecs": [{"spec": {"containers": [{"image": "seldonio/mock_classifier:1.0","imagePullPolicy":
            "IfNotPresent","name": "classifier","resources": {"requests": {"memory":
            "1Mi"}}}],"terminationGracePeriodSeconds": 1}}],"graph": {"children":
            [],"name": "classifier","endpoint": {"type" : "REST"},"type": "MODEL"},"name":
            "example","replicas": 1,"labels": {"version" : "v1"}}]}}]'
          capabilities: Seamless Upgrades
          categories: Logging & Tracing
          certified: "false"
          containerImage: seldonio/seldon-core-operator:0.4.0
          createdAt: "2019-05-21 15:00:00"
          description: The Seldon operator for management, monitoring and operations
            of machine learning systems through the Seldon Engine. Once installed,
            the Seldon Operator provides multiple functions which facilitate the productisation,
            monitoring and maintenance of machine learning systems at scale.
          repository: https://github.com/SeldonIO/seldon-operator
          support: Clive Cox
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A seldon engine deployment
            displayName: Seldon Delpoyment
            kind: SeldonDeployment
            name: seldondeployments.machinelearning.seldon.io
            version: v1alpha2
        description: "The Seldon operator enables for native operation of production
          machine learning workloads, including monitoring and operations of language-agnostic
          models with the benefits of real-time metrics and log analysis.\n   \n##
          Overview\nSeldon Core is an open source platform for deploying machine learning
          models on a Kubernetes cluster.\n\n* Deploy machine learning models in the
          cloud or on-premise.\n* Get metrics and ensure proper governance and compliance
          for your running machine learning models.\n* Create powerful inference graphs
          made up of multiple components.\n* Provide a consistent serving layer for
          models built using heterogeneous ML toolkits.\n\nYou can get started by
          following the guides in our documentation at https://docs.seldon.io/projects/seldon-core/en/latest/workflow/README.html\n"
        displayName: Seldon Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Seldon Technologies
        version: 0.1.5
      name: alpha
    defaultChannel: alpha
    packageName: seldon-operator
    provider:
      name: Seldon Technologies
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: H2O.ai
      provider-url: ""
    name: driverlessai-deployment-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/driverlessai-deployment-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: driverless-ai-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"ai.h2o.dai/v1alpha1","kind":"DriverlessAiDeployment","metadata":{"name":"test-dai-dep"},"spec":{"cpus":"4","group":"dai-demo","image":"registry.connect.redhat.com/h2oai/driverlessai-rhelubi7:latest","memory":"16Gi","port":12345,"pvcsize":"25Gi","config":"",
            "licensekey":""}}]'
          capabilities: Basic Install
          categories: AI/Machine Learning
          certified: "false"
          containerImage: registry.connect.redhat.com/h2oai/driverless-ai-operator
          createdAt: "2019-09-19"
          description: Operator that allows users to spin up a Driverless AI server
            in OpenShift environment
          repository: https://github.com/h2oai
          support: Driverless Ai
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: operator for deploying H2O.ai Driverless Ai
            displayName: Driverless Ai Deployment
            kind: DriverlessAiDeployment
            name: driverlessaideployments.ai.h2o.dai
            version: v1alpha1
        description: operator for deploying H2O.ai Driverless Ai
        displayName: Driverless Ai Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: H2O.ai
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: driverlessai-deployment-operator-certified
    provider:
      name: H2O.ai
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Red Hat Partner Connect
      provider-url: ""
    name: newrelic-infrastructure
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/newrelic-infrastructure
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: newrelic-operator.v0.0.3
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
               "apiVersion": "apm.newrelic.com/v1alpha1",
               "kind": "NewRelic",
               "metadata": {
                  "name": "example-newrelic"
               },
               "spec": {
                  "image": {
                     "registry": "registry.access.redhat.com",
                     "repository": "newrelic-openshift/newrelic-infrastructure-k8s-1",
                     "tag": "latest",
                     "pullPolicy": "IfNotPresent"
                  },
                  "service": {
                     "type": "ClusterIP",
                     "port": 3306
                  },
                  "serviceAccountName": "newrelic-operator",
                  "resources": {}
               }
            }]
          capabilities: Basic Install
          categories: Monitoring,OpenShift Optional
          certified: "true"
          containerImage: registry.connect.redhat.com/newrelic-openshift/newrelic-infra-agent
          createdAt: "2019-04-18 12:59:59"
          description: Deploy the NewRelic agent from a helm chart onto your Kubernetes
            or OpenShift cluster
          repository: https://github.com/RHC4TP/operators/tree/master/partners/newrelic-operator
          support: Red Hat Connect
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: NewRelic Operator by Red Hat Partner Connect
            displayName: NewRelic Operator
            kind: NewRelic
            name: newrelics.apm.newrelic.com
            version: v1alpha1
        description: "This is an Operator for NewRelic, developed using the NewRelic
          helm chart available at [Newrelic's GitHub](https://github.com/helm/charts/tree/master/stable/newrelic-infrastructure).\n\nThe
          operator will deploy the NewRelic agent (using the helm chart) on your Kubernetes
          or OpenShift cluster as a Daemonset.\n\n## Prerequisites\nThe NewRelic Operator
          will (by default) deploy the latest version of NewRelic Infrastructure from
          Red Hat Software Collections for RHEL 7.\nThe image will pull from the [Red
          Hat Container Catalog](https://access.redhat.com/containers), which will
          require the following:\n\n* RHN account (https://access.redhat.com)\n* RHCC
          Image pull secret in your project/namespace or [Red Hat Container Registry
          Authentication](https://access.redhat.com/RegistryAuthentication) setup
          on your OpenShift cluster\n\nTo create an `rhcc` secret using docker (requires
          root/sudo or you must be in the docker group):\n```\n# docker login -u <username>
          registry.connect.redhat.com\nPassword:\nLogin Succeeded\n# oc create secret
          generic rhcc --from-file=.dockerconfigjson=$HOME/.docker/config.json --type=kubernetes.io/dockerconfigjson\n```\n\nTo
          create an `rhcc` secret using podman:\n```\n$ podman login -u <username>
          registry.connect.redhat.com\nPassword:\nLogin Succeeded!\n$ oc create secret
          generic rhcc --from-file=.dockerconfigjson=$XDG_RUNTIME_DIR/containers/auth.json
          --type=kubernetes.io/dockerconfigjson\n```\n\nTo link the `rhcc` secret
          to the `default` service account to use as an image pull secret in the current
          project/namespace:\n```\n$ oc secrets link default rhcc --for=pull\n```\n\n##
          Required Parameters\nThere is only one required parameter, and that is the
          Agent Key available by logging into [NewRelic](https://infrastructure.newrelic.com).\n\nNote
          that the Agent Key is kept in a base64-encoded K8s Secret, however the key
          is also visible as plain text within the Custom Resource spec.\n  \nValues
          listed below are relative to the `spec:` field in the Custom Resource.\n
          \n* **newrelic.agent.key** - Your NewRelic Agent Key\n\n## Advanced Features\n\nThis
          is mostly a TODO section, however you can define your own values as required
          for the following fields:\n\nValues are relative to the `spec:` field in
          the Custom Resource.\n\n* **image.registry** - The dns hostname of the desired
          container image registry (defaults to `registry.access.redhat.com`)\n* **image.repository**
          - The repository path (appended to the registry URL) above where the container
          image resides (defaults to `rhscl/newrelic-102-rhel`)\n* **image.tag** -
          The container image tag (defaults to `:latest`)\n* **db.config** - The NewRelic
          configuration populating `my.cnf` inside the container. Keep in mind that
          the `rhscl/newrelic-102-rhel` container is hard-coded to expose port 3306
          (if you want to change this then you'll have to specify a custom MariaDB
          container image)\n* **resources** - Define your own Pod resource limits
          (`limits.cpu` and `limits.memory`)\n* **service.type** - The type of Service
          to configure for NewRelic (defaults to `ClusterIP`)\n* **service.port**
          - The TCP port that the Service will listen on    \n* **serviceAccountName**
          - The name of the service account used to deploy NewRelic (NOTE: This **must**
          match what is defined in the ClusterServiceVersion)\n"
        displayName: NewRelic Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat Partner Connect
        version: 0.0.3
      name: alpha
    defaultChannel: alpha
    packageName: newrelic-infrastructure
    provider:
      name: Red Hat Partner Connect
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Appranix, Inc
      provider-url: ""
    name: appranix-cps
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/appranix-cps
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ax-cps-operator.v1.0.0
      currentCSVDesc:
        annotations:
          capabilities: Seamless Upgrades
          categories: Monitoring
          certified: "true"
          containerImage: registry.connect.redhat.com/appranix/apx-operator:2.2.0
          createdAt: "2020-01-30T10:30:20+05:30"
          description: The Appranix CPS operator enables you to back up and restore
            your Kubernetes/OpenShift cluster resources and persistent volumes.
          repository: https://github.com/operator-framework/community-operators/tree/master/upstream-community-operators/appranix
          support: Appranix
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Backup respresents the capture of K8s cluster state at a
              point in time (API objects and associated volume state).
            displayName: Backup
            kind: Backup
            name: backups.aps.appranix.com
            version: v1
          - description: Schedule is an Appranix resource that represents a pre-scheduled
              or periodic Backup that should be run.
            displayName: Schedule
            kind: Schedule
            name: schedules.aps.appranix.com
            version: v1
          - description: Restore represents the application of resources from an Appranix
              backup to a target K8s cluster.
            displayName: Restore
            kind: Restore
            name: restores.aps.appranix.com
            version: v1
          - description: DownloadRequest is a request to download an artifact from
              backup object storage, such as a backup log file.
            displayName: DownloadRequest
            kind: DownloadRequest
            name: downloadrequests.aps.appranix.com
            version: v1
          - description: DeleteBackupRequest is a request to delete one or more backups.
            displayName: DeleteBackupRequest
            kind: DeleteBackupRequest
            name: deletebackuprequests.aps.appranix.com
            version: v1
          - description: PodVolumeBackup respresents the capture of the Pod's volume
              at a point in time.
            displayName: PodVolumeBackup
            kind: PodVolumeBackup
            name: podvolumebackups.aps.appranix.com
            version: v1
          - description: PodVolumeRestore represents the restoration of a PodVolume
              from a volume snapshot.
            displayName: PodVolumeRestore
            kind: PodVolumeRestore
            name: podvolumerestores.aps.appranix.com
            version: v1
          - description: ResticRepository represents the repository from where the
              restic tool is to be downloaded.
            displayName: ResticRepository
            kind: ResticRepository
            name: resticrepositories.aps.appranix.com
            version: v1
          - description: BackupStorageLocation represents a storage bucket in a supported
              cloud provider.
            displayName: BackupStorageLocation
            kind: BackupStorageLocation
            name: backupstoragelocations.aps.appranix.com
            version: v1
          - description: VolumeSnapshotLocation is a location where Appranix stores
              volume snapshots.
            displayName: VolumeSnapshotLocation
            kind: VolumeSnapshotLocation
            name: volumesnapshotlocations.aps.appranix.com
            version: v1
          - description: ServerStatusRequest is a request to access current status
              information about the Appranix server.
            displayName: ServerStatusRequest
            kind: ServerStatusRequest
            name: serverstatusrequests.aps.appranix.com
            version: v1
        description: |-
          ## About this Operator

          Appranix helps you to securely backup and restore, perform a disaster
          recovery and migrate Kubernetes/OpenShift cluster resources and persistent
          volumes.

          The operator works with Appranix to:

          - Install and initiate Appranix controller on your cluster

          - Take backups of your cluster and restore it in case of loss.

          - Migrate cluster resources to other clusters.

          - Replicate your production cluster to development and testing clusters.

          ## Prerequisites

          - A storage bucket in GCP or AWS & credentials to access it
          - Appranix token generated for your cluster from the Appranix console
          - A Configmap and Secret in your cluster for the bucket config and credentials

          #### Step 1: Creating a storage bucket

          Note: Currently only Google Storage bucket is supported, support for AWS Elastic Bucket Store will be available in a future release.

          - You can create a bucket through the GCP console or by running the following command in [Google Cloud Shell](https://cloud.google.com/shell/)

          ```shell
          AX_CPS_BUCKET_NAME=<bucket_name>
          gsutil mb -b on gs://$AX_CPS_BUCKET_NAME/
          gcloud iam service-accounts create appranix --display-name "Appranix service account"
          AX_SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list --filter="displayName:Appranix service account" --format 'value(email)')
          gsutil iam ch serviceAccount:$AX_SERVICE_ACCOUNT_EMAIL:objectAdmin gs://${AX_CPS_BUCKET_NAME}
          gcloud iam service-accounts keys create ax_service_account.json --iam-account $AX_SERVICE_ACCOUNT_EMAIL
          ```


          #### Step 2: Obtaining the Appranix token for the cluster

          - Log in to your Appranix console and create a cluster. After the cluster creation copy the token generated for that cluster.

          #### Step 3: Creating the Configmap and Secret in the cluster

          A Configmap and Secret which contains the credetianls and bucket configuration has to be created before instaling the operator. It can be done by running the following command against your cluster.

          Note: Replace **token**, **bucket_name**, **bucket_provider** accordingly.


          ```shell
          AX_TOKEN=<token>
          AX_CPS_BUCKET_NAME=<bucket_name>
          AX_CPS_BUCKET_PROVIDER=<bucket_provider>
          AX_SERVICE_ACCOUNT=$(cat ax_service_account.json | base64 -w 0)
          # Run only for a Kubernetes cluster
          curl -Ls -H "Content-Type: application/json" https://us-central1-appranix-dev-07.cloudfunctions.net/gen-setup  -d '{"token": "'$AX_TOKEN'", "bucket": "'$AX_CPS_BUCKET_NAME'", "cloud": "'$AX_CPS_BUCKET_PROVIDER'", "secret": "'$AX_SERVICE_ACCOUNT'"}' | kubectl apply -f -
          # Run only for an OpenShift cluster
          curl -Ls -H "Content-Type: application/json" https://us-central1-appranix-dev-07.cloudfunctions.net/gen-setup  -d '{"token": "'$AX_TOKEN'", "bucket": "'$AX_CPS_BUCKET_NAME'", "cloud": "'$AX_CPS_BUCKET_PROVIDER'", "secret": "'$AX_SERVICE_ACCOUNT'"}' | oc apply -f -
          ```

          **After performing the above prerequisites you can proceed to install the Appranix Operator from OperatorHub**
        displayName: Appranix CPS Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Appranix, Inc
        version: 1.0.0
      name: deprecated
    - currentCSV: ax-cps-operator.v2.3.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "aps.appranix.com/v1",
                "kind": "BackupStorageLocation",
                "metadata": {
                  "name": "default"
                },
                "spec": {
                  "provider": "gcp",
                  "objectStorage": {
                    "bucket": "aps-bucket-store"
                  }
                }
              },
              {
                "apiVersion": "aps.appranix.com/v1",
                "kind": "VolumeSnapshotLocation",
                "metadata": {
                  "name": "gcp-default"
                },
                "spec": {
                  "provider": "gcp"
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Monitoring
          certified: "true"
          containerImage: registry.connect.redhat.com/appranix/apx-operator:2.3.1
          createdAt: "2020-02-18T16:17:57+05:30"
          description: The Appranix operator enables you to back up and restore your
            Kubernetes/OpenShift cluster resources and persistent volumes.
          repository: https://github.com/operator-framework/community-operators/tree/master/upstream-community-operators/appranix
          support: Appranix
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: BackupStorageLocation represents a storage bucket in a supported
              cloud provider.
            displayName: BackupStorageLocation
            kind: BackupStorageLocation
            name: backupstoragelocations.aps.appranix.com
            version: v1
          - description: VolumeSnapshotLocation is a location where Appranix stores
              volume snapshots.
            displayName: VolumeSnapshotLocation
            kind: VolumeSnapshotLocation
            name: volumesnapshotlocations.aps.appranix.com
            version: v1
        description: |-
          ## About this Operator

          Appranix helps you to securely backup and restore, perform a disaster
          recovery and migrate Kubernetes/OpenShift cluster resources and persistent
          volumes.

          The operator works with Appranix to:

          - Install and initiate Appranix controller on your cluster

          - Take backups of your cluster and restore it in case of loss.

          - Migrate cluster resources to other clusters.

          - Replicate your production cluster to development and testing clusters.

          ## Prerequisites

          - A storage bucket in GCP or AWS & credentials to access it
          - Appranix token generated for your cluster from the Appranix console
          - A Configmap and Secret in your cluster for the bucket config and credentials

          #### Step 1: Creating a storage bucket

          Note: Currently only Google Storage bucket is supported, support for AWS Elastic Bucket Store will be available in a future release.

          - You can create a bucket through the GCP console or by running the following command in [Google Cloud Shell](https://cloud.google.com/shell/)

          ```shell
          AX_CPS_BUCKET_NAME=<bucket_name>
          gsutil mb -b on gs://$AX_CPS_BUCKET_NAME/
          gcloud iam service-accounts create appranix --display-name "Appranix service account"
          AX_SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list --filter="displayName:Appranix service account" --format 'value(email)')
          gsutil iam ch serviceAccount:$AX_SERVICE_ACCOUNT_EMAIL:objectAdmin gs://${AX_CPS_BUCKET_NAME}
          gcloud iam service-accounts keys create ax_service_account.json --iam-account $AX_SERVICE_ACCOUNT_EMAIL
          ```


          #### Step 2: Obtaining the Appranix token for the cluster

          - Log in to your Appranix console and create a cluster. After the cluster creation copy the token generated for that cluster.

          #### Step 3: Creating the Configmap and Secret in the cluster

          A Configmap and Secret which contains the credetianls and bucket configuration has to be created before instaling the operator. It can be done by running the following command against your cluster.

          Note: Replace **token**, **bucket_name**, **bucket_provider** accordingly.


          ```shell
          AX_TOKEN=<token>
          AX_CPS_BUCKET_NAME=<bucket_name>
          AX_CPS_BUCKET_PROVIDER=<bucket_provider>
          AX_SERVICE_ACCOUNT=$(cat ax_service_account.json | base64 -w 0)
          # Run only for a Kubernetes cluster
          curl -Ls -H "Content-Type: application/json" https://us-central1-appranix-dev-07.cloudfunctions.net/gen-setup  -d '{"token": "'$AX_TOKEN'", "bucket": "'$AX_CPS_BUCKET_NAME'", "cloud": "'$AX_CPS_BUCKET_PROVIDER'", "secret": "'$AX_SERVICE_ACCOUNT'"}' | kubectl apply -f -
          # Run only for an OpenShift cluster
          curl -Ls -H "Content-Type: application/json" https://us-central1-appranix-dev-07.cloudfunctions.net/gen-setup  -d '{"token": "'$AX_TOKEN'", "bucket": "'$AX_CPS_BUCKET_NAME'", "cloud": "'$AX_CPS_BUCKET_PROVIDER'", "secret": "'$AX_SERVICE_ACCOUNT'"}' | oc apply -f -
          ```

          **After performing the above prerequisites you can proceed to install the Appranix Operator from OperatorHub**
        displayName: Appranix APS Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Appranix, Inc
        version: 2.3.0
      name: stable
    defaultChannel: stable
    packageName: appranix-cps
    provider:
      name: Appranix, Inc
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: AtlasMap
      provider-url: ""
    name: atlasmap-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/atlasmap-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: atlasmap-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion":"atlasmap.io/v1alpha1",
              "kind":"AtlasMap",
              "metadata": {
                "name":"example-atlasmap"
              },
              "spec": {
                "replicas":1
              }
            }]
          capabilities: Seamless Upgrades
          categories: Integration & Delivery
          certified: "false"
          containerImage: docker.io/atlasmap/atlasmap-operator
          createdAt: "2019-06-10 06:42:43"
          description: AtlasMap is a data mapping solution with an interactive web
            based user interface, that simplifies configuring integrations between
            Java, XML, and JSON data sources
          repository: https://github.com/atlasmap/atlasmap-operator
          support: AtlasMap
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A running AtlasMap instance
            displayName: AtlasMap
            kind: AtlasMap
            name: atlasmaps.atlasmap.io
            version: v1alpha1
        description: |
          AtlasMap is a data mapping solution with an interactive web based user interface, that simplifies configuring integrations between Java, XML, and JSON data sources.

          The AtlasMap operator simplifies the deployment of AtlasMap on OpenShift / Kubernetes clusters.

          For further information about AtlasMap and the operator, visit the documentation.

          * [AtlasMap documentation](https://www.atlasmap.io/)
          * [AtlasMap operator documentation](https://github.com/atlasmap/atlasmap-operator)
        displayName: AtlasMap Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: AtlasMap
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: atlasmap-operator
    provider:
      name: AtlasMap
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: https://teiid.io
      provider-url: ""
    name: teiid
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/teiid
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: teiid.0.1.1
      currentCSVDesc:
        annotations:
          alm-examples: "[\n  {\n    \"apiVersion\": \"teiid.io/v1alpha1\",\n    \"kind\":
            \"VirtualDatabase\",\n    \"metadata\": {\n      \"name\": \"customer-vdb\"\n
            \   },\n    \"spec\": {\n      \"replicas\": 1,\n      \"env\": [\n        {\n
            \         \"name\": \"SPRING_DATASOURCE_SAMPLEDB_USERNAME\",\n          \"value\":
            \"user\"\n        },\n        {\n          \"name\": \"SPRING_DATASOURCE_SAMPLEDB_PASSWORD\",\n
            \         \"value\": \"mypassword\"\n        },\n        {\n          \"name\":
            \"SPRING_DATASOURCE_SAMPLEDB_DATABASENAME\",\n          \"value\": \"sampledb\"\n
            \       },\n        {\n          \"name\": \"SPRING_DATASOURCE_SAMPLEDB_JDBCURL\",\n
            \         \"value\": \"jdbc:postgresql://postgresql/$(SPRING_DATASOURCE_SAMPLEDB_DATABASENAME)\"\n
            \       }\n      ],\n      \"resources\": {\n        \"memory\": \"512Mi\",\n
            \       \"cpu\": 1\n      },            \n      \"build\": {\n        \"source\":
            {\n          \"ddl\": \"CREATE DATABASE customer OPTIONS (ANNOTATION 'Customer
            VDB');\\nUSE DATABASE customer;\\n\\nCREATE SERVER sampledb TYPE 'NONE'
            FOREIGN DATA WRAPPER postgresql;\\n\\nCREATE SCHEMA accounts SERVER sampledb;\\nCREATE
            VIRTUAL SCHEMA portfolio;\\n\\nSET SCHEMA accounts;\\nIMPORT FOREIGN SCHEMA
            public FROM SERVER sampledb INTO accounts OPTIONS(\\\"importer.useFullSchemaName\\\"
            'false');\\n\\nSET SCHEMA portfolio;\\n\\nCREATE VIEW CustomerZip(id bigint
            PRIMARY KEY, name string, ssn string, zip string) AS \\n    SELECT c.ID
            as id, c.NAME as name, c.SSN as ssn, a.ZIP as zip \\n    FROM accounts.CUSTOMER
            c LEFT OUTER JOIN accounts.ADDRESS a \\n    ON c.ID = a.CUSTOMER_ID;\\n\"\n
            \       }\n      }\n    }\n  }\n]"
          capabilities: Seamless Upgrades
          categories: Database, Integration & Delivery
          certified: "false"
          containerImage: quay.io/teiid/teiid-operator:0.1.0
          createdAt: "2019-06-24T12:48:22Z"
          description: Teiid Operator for deployment and management of Data Virtualization
            services in Kubernetes/OpenShift cloud.
          repository: https://github.com/teiid/teiid-operator
          support: https://teiid.io
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An example built using PostgreSQL database.
            displayName: VirtualDatabase
            kind: VirtualDatabase
            name: virtualdatabases.teiid.io
            version: v1alpha1
        description: "Teiid is a Data Virtualization system that allows applications
          to federate data from multiple, heterogeneous data stores. Through its abstraction
          and federation, data is accessed and integrated in real-time across distributed
          data sources without copying or otherwise moving data from its system of
          record. \n\nFor example, you can access your all data in Oracle, Postgres,
          MongoDB and/or Rest API (many more) with a single request. Teiid gives you
          all the tools required to build logical/abstraction layer that can exposed
          as Virtual Database by essentially making all sources underneath as a black
          box to the consuming user. Since this integration of data happens in real
          time there is no ETL process to run and data is always fresh. For the end
          user the Virtual Database exactly looks like a relational database like
          Postgres, that can be accessed using JDBC, ODBC, OData V4 and Postgres protocols,
          i.e. can be accessed from any language that you have your application written
          in.\n\nTeiid, out of box also exposes the OData v4 REST API without any
          further coding, with this you can expose you single or multiple databases
          as rest services without any coding. Alternatively if you want expose a
          Open API directly over your data that is also possible with Teiid.\n\nIn
          Monolith to Microservices migration scenarios, this tool is very valuable
          in terms of providing boundaries around the monolith data access, schema
          versioning, domain model support using its logical/abstraction layers. \n\n**Features
          and capabilities**\n\n* Highly optimized and performant database engine
          with abstraction layer, to decouple your databases from your application
          layer.\n* Federated access layer for two or more data sources.\n* Ability
          to define logical/domain models without affecting the physical models/schema.
          \n* Secure data access to all data sources from a single location.\n* Data
          access over multiple protocols and programing languages such as JDBC, ODBC,
          OData V4 (Rest API), SQLAlchemy, ADO.NET.\n* Locally cached data for faster
          response time.\n* Other features include SQL/MED for view definition, transaction
          support, source hints, query plans, virtual procedures, temp-tables, user-defined
          functions, extensible translator layer to name a few."
        displayName: Teiid
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: https://teiid.io
        version: 0.1.1
      name: beta
    defaultChannel: beta
    packageName: teiid
    provider:
      name: https://teiid.io
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: xridge.io
      provider-url: ""
    name: kubestone
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kubestone
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kubestone.v0.4.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"perf.kubestone.xridge.io/v1alpha1","kind":"Drill","metadata":{"name":"drill-sample"},"spec":{"benchmarkFile":"benchmark.yml","benchmarksVolume":{"benchmark.yml":"---\nthreads:
            1\nbase: ''https://kubernetes.io''\niterations: 2\nrampup: 2\n\nplan:\n  -
            name: Include other file\n    include: other.yml\n\n  - name: Fetch kubernetes.io\n    request:\n      url:
            /\n","other.yml":"---\n- name: Fetch docs\n  request:\n    url: /docs\n"},"image":{"name":"xridge/drill:0.5.0"},"options":"--stats"}},{"apiVersion":"perf.kubestone.xridge.io/v1alpha1","kind":"Fio","metadata":{"name":"fio-sample"},"spec":{"cmdLineArgs":"--name=randwrite
            --iodepth=1 --rw=randwrite --bs=4m --size=256M","image":{"name":"xridge/fio:3.13"},"volume":{"volumeSource":{"emptyDir":{}}}}},{"apiVersion":"perf.kubestone.xridge.io/v1alpha1","kind":"Ioping","metadata":{"name":"ioping-sample"},"spec":{"args":"-w
            10","image":{"name":"xridge/ioping:1.1"},"volume":{"volumeSource":{"emptyDir":{}}}}},{"apiVersion":"perf.kubestone.xridge.io/v1alpha1","kind":"Iperf3","metadata":{"name":"iperf3-sample"},"spec":{"clientConfiguration":{"podScheduling":{"affinity":{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"iperf-mode","operator":"In","values":["server"]}]},"topologyKey":"kubernetes.io/hostname"}]}}}},"image":{"name":"xridge/iperf3:3.7.0","pullPolicy":"IfNotPresent"},"serverConfiguration":{"cmdLineArgs":"--verbose","hostNetwork":false,"podLabels":{"iperf-mode":"server"},"podScheduling":{"affinity":{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["store"]}]},"topologyKey":"kubernetes.io/hostname"}]}}}},"udp":false}},{"apiVersion":"perf.kubestone.xridge.io/v1alpha1","kind":"Pgbench","metadata":{"name":"pgbench-sample"},"spec":{"image":{"name":"xridge/pgbench"},"initArgs":"-s
            5","postgres":{"database":"benchdb","host":"postgres","password":"admin","port":5432,"user":"admin"}}},{"apiVersion":"perf.kubestone.xridge.io/v1alpha1","kind":"Qperf","metadata":{"name":"qperf-sample"},"spec":{"clientConfiguration":{"podLabels":{"qperf-mode":"client"},"podScheduling":{"affinity":{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"qperf-mode","operator":"In","values":["server"]}]},"topologyKey":"kubernetes.io/hostname"}]}}}},"image":{"name":"xridge/qperf:0.4.11-r0","pullPolicy":"IfNotPresent"},"options":"--verbose
            --time 10","serverConfiguration":{"hostNetwork":false,"podLabels":{"qperf-mode":"server"},"podScheduling":{"affinity":{"podAntiAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":[{"labelSelector":{"matchExpressions":[{"key":"app","operator":"In","values":["server"]}]},"topologyKey":"kubernetes.io/hostname"}]}}}},"tests":["tcp_bw","tcp_lat"]}},{"apiVersion":"perf.kubestone.xridge.io/v1alpha1","kind":"Sysbench","metadata":{"name":"sysbench-sample"},"spec":{"command":"run","image":{"name":"xridge/sysbench:1.0.17-1"},"options":"--threads=1
            --time=10","testName":"cpu"}}]'
          capabilities: Full Lifecycle
          categories: Developer Tools
          certified: "false"
          containerImage: xridge/kubestone:v0.4.0
          createdAt: "2019-10-18T12:32:00Z"
          description: Kubestone is a benchmarking Operator that can evaluate the
            performance of Kubernetes installations and external services.
          repository: https://github.com/xridge/kubestone
          support: info@xridge.io
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: 'With the drill load generator, you can create a load test
              plan and execute it against any Web Service inside or outside of your
              Kubernetes installation: ''Drill is a HTTP load testing application
              written in Rust. The main goal for this project is to build a really
              lightweight tool as alternative to other that require JVM and other
              stuff. You can write brenchmark files, in YAML format, describing all
              the stuff you want to test. It was inspired by Ansible syntax because
              it is really easy to use and extend.'''
            displayName: drill
            kind: Drill
            name: drills.perf.kubestone.xridge.io
            version: v1alpha1
          - description: 'With the fio benchmark you can measure the I/O performance
              of the disks used in your Kubernetes cluster: ''fio is a tool that will
              spawn a number of threads or processes doing a particular type of I/O
              action as specified by the user. The typical use of fio is to write
              a job file matching the I/O load one wants to simulate.'''
            displayName: fio
            kind: Fio
            name: fios.perf.kubestone.xridge.io
            version: v1alpha1
          - description: 'With ioping benchmark you can measure the latency of the
              storage I/O subsystem in your Kubernetes cluster: ''A tool to monitor
              I/O latency in real time. It shows disk latency in the same way as ping
              shows network latency.'''
            displayName: ioping
            kind: Ioping
            name: iopings.perf.kubestone.xridge.io
            version: v1alpha1
          - description: 'With the iperf3 benchmark, you can measure the I/O performance
              of the network hardware and stack used in your Kubernetes cluster: ''iPerf3
              is a tool for active measurements of the maximum achievable bandwidth
              on IP networks. It supports tuning of various parameters related to
              timing, buffers and protocols (TCP, UDP, SCTP with IPv4 and IPv6).'''
            displayName: iperf3
            kind: Iperf3
            name: iperf3s.perf.kubestone.xridge.io
            version: v1alpha1
          - description: With the qperf benchmark, you can measure the I/O performance
              of the network hardware and stack used in your Kubernetes cluster. 'Qperf
              measures bandwidth and latency between two nodes. It can work over TCP/IP
              as well as the RDMA transports. On one of the nodes, qperf is typically
              run with no arguments designating it the server node. One may then run
              qperf on a client node to obtain measurements such as bandwidth, latency
              and cpu utilization.'
            displayName: qperf
            kind: Qperf
            name: qperves.perf.kubestone.xridge.io
            version: v1alpha1
          - description: 'With the sysbench benchmark you can measure the CPU, Memory,
              Database and Filesystem characteritics of your Kubernetes cluster: ''sysbench
              is a scriptable multi-threaded benchmark tool based on LuaJIT. It is
              most frequently used for database benchmarks, but can also be used to
              create arbitrarily complex workloads that do not involve a database
              server.'''
            displayName: sysbench
            kind: Sysbench
            name: sysbenches.perf.kubestone.xridge.io
            version: v1alpha1
          - description: 'With the pgbench benchmark, you can benchmark your PostgreSQL
              database, which can both run in the same Kubernetes cluster as kubestone,
              or anywhere else, as long as it''s reachable: ''pgbench is a simple
              program for running benchmark tests on PostgreSQL. It runs the same
              sequence of SQL commands over and over, possibly in multiple concurrent
              database sessions, and then calculates the average transaction rate
              (transactions per second). By default, pgbench tests a scenario that
              is loosely based on TPC-B, involving five SELECT, UPDATE, and INSERT
              commands per transaction. However, it is easy to test other cases by
              writing your own transaction script files.'''
            displayName: pgbench
            kind: Pgbench
            name: pgbenches.perf.kubestone.xridge.io
            version: v1alpha1
        description: |
          Kubestone is a benchmarking Operator that can evaluate the performance of Kubernetes installations and external services.

          Features:
          - Supports common set of benchmarks to measure: CPU, Disk, Network and Application performance
          - Fine-grained control over Kubernetes Scheduling primitives: Affinity, Anti-Affinity, Tolerations, Storage Classes and Node Selection
          - Cloud Native: Benchmarks runs are defined as Custom Resources and executed in the cluster using Kubernetes resources: Pods, Jobs, Deployments and Services.
          - Extensible: New benchmarks can easily be added by implementing a new controller.

          Fore more information please visit [https://kubestone.io/](https://kubestone.io)
        displayName: Kubestone
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: xridge.io
        version: 0.4.0
      name: alpha
    defaultChannel: alpha
    packageName: kubestone
    provider:
      name: xridge.io
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: CognitiveScale
      provider-url: ""
    name: cortex-certifai-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cortex-certifai-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: cortex-certifai-operator.v1.2.10-rc.4
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "cortex.cognitivescale.com/v1",
                "kind": "Certifai",
                "metadata": {
                  "name": "default-certifai"
                },
                "spec": {
                  "console": {
                    "azure": {
                      "account-key": "your account key",
                      "account-name": "your account name",
                      "sas-token": "your SAS token"
                    },
                    "replicas": 1,
                    "route-type": "none",
                    "s3": {
                      "access-key": "your access key",
                      "endpoint": "https://s3.amazonaws.com",
                      "secret-key": "your secret key",
                      "verify-cert": false
                    },
                    "scan-dir": "s3://bucket or abfs://container"
                  },
                  "reference-model": {
                    "enabled": true
                  }
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: AI/Machine Learning
          containerImage: registry.connect.redhat.com/c12e/cortex-certifai-operator
          createdAt: "2020-1-23T12:03:45Z"
          description: Cortex Certifai empowers enterprises to identify and mitigate
            risk and vulnerabilities within AI and deploy trusted AI systems into
            production.
          support: CognitiveScale
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Certifai is the Schema for the certifais API
            displayName: Cortex Certifai Operator
            kind: Certifai
            name: certifais.cortex.cognitivescale.com
            version: v1
        description: |
          Cortex Certifai empowers enterprises to identify and mitigate risk and vulnerabilities within AI and deploy trusted AI systems into production. It evaluates AI models and allows users to compare different models or model versions for the following qualities.
          * **Robustness:** measures how well models retain an outcome given changes to the data feature values. The more robust a model is, the greater the changes required to alter the outcome.
          * **Fairness by group:** measures the difference required to change the outcome for different groups implicit in a feature given the same model and dataset. For example, implicit groups male, female, and nonbinary belong to the feature, â€œgenderâ€. A fair model shows that all 3 groups require a similar amount of change to alter the results.
          * **Explainability:** measures the average simplicity of counterfactual explanations provided for each model. An explanation that requires a single changed feature will score 100%. Explanations that require more changed features will score lower.
          * **Explanations:** display the prediction provided through the generation of counterfactuals for the change that must occur in a dataset with given restrictions to obtain a different outcome. Users can explore the entire dataset one observation at a time to understand what features changed and by how much to obtain a different result.
          See our [documentation](https://cognitivescale.github.io/cortex-certifai/) for information about how to deploy the Certifai Operator and how to get started using our toolkit.
          ## License Copyright 2020 Cognitive Scale, Inc. All Rights Reserved.
          By downloading you agree to the terms of CognitiveScale's [End User License Agreement](https://www.cognitivescale.com/legal-information/).
        displayName: Cortex Certifai Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: CognitiveScale
        version: 1.2.10-rc.4
      name: beta
    - currentCSV: cortex-certifai-operator.v1.2.9
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "cortex.cognitivescale.com/v1",
                "kind": "Certifai",
                "metadata": {
                  "name": "default-certifai"
                },
                "spec": {
                  "console": {
                    "replicas": 1,
                    "route-type": "none",
                    "s3": {
                      "access-key": "your access key",
                      "bucket-name": "your bucket name",
                      "endpoint": "s3.amazonaws.com",
                      "secret-key": "your secret key",
                      "verify-cert": false
                    }
                  },
                  "reference-model": {
                    "enabled": true
                  }
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: AI/Machine Learning
          containerImage: registry.connect.redhat.com/c12e/cortex-certifai-operator
          createdAt: "2020-01-23 12:03:45"
          description: Cortex Certifai empowers enterprises to identify and mitigate
            risk and vulnerabilities within AI and deploy trusted AI systems into
            production.
          support: CognitiveScale
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Certifai is the Schema for the certifais API
            displayName: Cortex Certifai Operator
            kind: Certifai
            name: certifais.cortex.cognitivescale.com
            version: v1
        description: |
          Cortex Certifai empowers enterprises to identify and mitigate risk and vulnerabilities within AI and deploy trusted AI systems into production. It evaluates AI models and allows users to compare different models or model versions for the following qualities.
          * **Robustness:** measures how well models retain an outcome given changes to the data feature values. The more robust a model is, the greater the changes required to alter the outcome.
          * **Fairness by group:** measures the difference required to change the outcome for different groups implicit in a feature given the same model and dataset. For example, implicit groups male, female, and nonbinary belong to the feature, â€œgenderâ€. A fair model shows that all 3 groups require a similar amount of change to alter the results.
          * **Explainability:** measures the average simplicity of counterfactual explanations provided for each model. An explanation that requires a single changed feature will score 100%. Explanations that require more changed features will score lower.
          * **Explanations:** display the prediction provided through the generation of counterfactuals for the change that must occur in a dataset with given restrictions to obtain a different outcome. Users can explore the entire dataset one observation at a time to understand what features changed and by how much to obtain a different result.
          See our [documentation](https://cognitivescale.github.io/cortex-certifai/) for information about how to deploy the Certifai Operator and how to get started using our toolkit.
          ## License Copyright 2020 Cognitive Scale, Inc. All Rights Reserved.
          By downloading you agree to the terms of CognitiveScale's [End User License Agreement](https://www.cognitivescale.com/legal-information/).
        displayName: Cortex Certifai Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: CognitiveScale
        version: 1.2.9
      name: stable
    defaultChannel: stable
    packageName: cortex-certifai-operator
    provider:
      name: CognitiveScale
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: federation
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/federation
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: federation.v0.0.10
      currentCSVDesc:
        annotations:
          capabilities: Basic Install
          categories: OpenShift Optional, Integration & Delivery
          certified: "false"
          containerImage: quay.io/kubernetes-multicluster/federation-v2:v0.0.10
          createdAt: "2019-01-01T00:00:00Z"
          description: Gain Hybrid Cloud capabilities between your clusters with Kubernetes
            Federation.
          repository: https://github.com/openshift/federation-v2-operator
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents endpoint information about a remote cluster
            displayName: Cluster Registry Application
            kind: Cluster
            name: clusters.clusterregistry.k8s.io
            version: v1alpha1
          - description: Represents a Domain name and nameserver for use with multicluster
              DNS
            displayName: Domain
            kind: Domain
            name: domains.multiclusterdns.federation.k8s.io
            version: v1alpha1
          - description: Represents a Cluster that Federation can spread resources
              to
            displayName: FederatedCluster Resource
            kind: FederatedCluster
            name: federatedclusters.core.federation.k8s.io
            version: v1alpha1
          - description: Programs Federation to be aware of a new API type
            displayName: Federation Type Configuration
            kind: FederatedTypeConfig
            name: federatedtypeconfigs.core.federation.k8s.io
            version: v1alpha1
          - description: Represents the status of a FederatedService
            displayName: Federated service status
            kind: FederatedServiceStatus
            name: federatedservicestatuses.core.federation.k8s.io
            version: v1alpha1
          - description: Represents a configuration of a Federation controller
            displayName: FederationConfig
            kind: FederationConfig
            name: federationconfigs.core.federation.k8s.io
            version: v1alpha1
          - description: Represents information about the version of a federated resource
            displayName: PropagatedVersion Resource
            kind: PropagatedVersion
            name: propagatedversions.core.federation.k8s.io
            version: v1alpha1
          - description: Represents an instance of a DNSEndpoint resource
            displayName: DNSEndpoint Resource
            kind: DNSEndpoint
            name: dnsendpoints.multiclusterdns.federation.k8s.io
            version: v1alpha1
          - description: Represents an instance of a MultiClusterIngressDNSRecord
              resource
            displayName: MultiClusterIngressDNSRecord Resource
            kind: IngressDNSRecord
            name: ingressdnsrecords.multiclusterdns.federation.k8s.io
            version: v1alpha1
          - description: Represents an instance of a MultiClusterServiceDNSRecord
              resource
            displayName: MultiClusterServiceDNSRecord Resource
            kind: ServiceDNSRecord
            name: servicednsrecords.multiclusterdns.federation.k8s.io
            version: v1alpha1
          - description: Represents an instance of a ReplicaSchedulingPreference resource
            displayName: ReplicaSchedulingPreference Resource
            kind: ReplicaSchedulingPreference
            name: replicaschedulingpreferences.scheduling.federation.k8s.io
            version: v1alpha1
        description: "Kubernetes Federation is a tool to sync (aka \"federate\") a
          set of Kubernetes\nobjects from a \"source\" into a set of other clusters.
          Common use-cases\ninclude federating Namespaces across all of your clusters
          or rolling out an\napplication across several geographically distributed
          clusters. The\nKubernetes Federation Operator runs all of the components
          under the hood to\nquickly get up and running with this powerful concept.
          Federation is a key\npart of any Hybrid Cloud capability.\n\nKubernetes
          Federation is currently in preview. This operator will be updated\nas we
          release new versions of the [upstream repository](https://github.com/kubernetes-sigs/federation-v2).\n\n**Important
          Note**: Currently, while in preview, this operator does not\nsupport automatic
          upgrades. You must remove the old version of the operator\nmanually before
          installing a new version.\n\n## Using Federation\n\nThis deploys Federation
          in a namespace-scoped configuration which handles\nthe resources only in
          the namespace where federation is deployed. For\nexample, if you subscribe
          to this package in namespace `my-namespace`,\nfederation will be deployed
          to manage objects _only_ in `my-namespace` in\nthe host cluster and target
          clusters.\n\nAfter installing Federation instance, you will use the `kubefedctl`
          command\nline tool to do two types of configuration:\n  \n- Configure federation
          to connect to your target clusters with `kubefedctl join`\n- Create new
          federation APIs for the API types you want to federate with `kubefedctl
          enable`\n\n\n**Important Note**: To add federation capability to your namespace,
          you are\nonly required to install federation in the hosting cluster; there
          is no need\nto install federation in the target clusters (and no effect
          gained from\ndoing so).\n\n\n**Important Note**: The instructions in this
          guide refer to the namespace\nwhere federation is installed as `<namespace>`
          in command examples. The\nupstream user guide uses `federation-system` as
          the name of this namespace.\n\n### Get the kubefedctl CLI tool\n\nBefore
          you use federation, you should ensure that you're using the binary\nfor
          this version. You can get it from the releases on GitHub.\n\n    curl -Ls
          https://github.com/kubernetes-sigs/federation-v2/releases/download/v0.0.10/kubefedctl.tgz
          | tar xz\n\n### Joining Clusters\n\nUse the `kubefedctl join` command to
          connect clusters you want federation to\nspread resources to. `kubefedctl
          join` reads information about how to connect\nto the joining cluster and
          the cluster hosting federation from your\nKUBECONFIG.\n\n    kubefedctl
          join cluster-name \\\n      --cluster-context mycluster \\            #
          name of a KUBECONFIG context for the cluster to join\n      --host-cluster-context
          mycluster \\       # name of a KUBECONFIG context for the hosting cluster\n
          \     --add-to-registry \\                      # add clusters to the cluster-registry\n
          \     --federation-namespace=<namespace> \\     # namespace where federation
          is deployed\n      --registry-namespace=<namespace> \\       # namespace
          where federation is deployed\n      --limited-scope=true \\                   #
          ensure that created serviceaccount has permissions only scoped to the namespace\n
          \     -v 2\n\n`kubefedctl join` creates `FederatedCluster` and `Cluster`
          resources named\n`cluster-name` to represent the joined cluster.\n\n`Cluster`
          resources hold endpoint information about clusters federation connects to.\n\n
          \   kubectl get clusters -n <namespace>\n    NAME           AGE\n    cluster-name
          \  7s\n\n    kubectl get clusters cluster-name -o yaml\n    apiVersion:
          clusterregistry.k8s.io/v1alpha1\n    kind: Cluster\n    metadata:\n      creationTimestamp:
          2019-03-12T22:51:47Z\n      generation: 1\n      name: cluster-name\n      namespace:
          <namespace>\n      resourceVersion: \"42099\"\n      selfLink: /apis/clusterregistry.k8s.io/v1alpha1/namespaces/<namespace>/clusters/cluster-name\n
          \     uid: 6aa9f784-4519-11e9-baaf-02de7fb9d5b2\n    spec:\n      authInfo:
          {}\n      kubernetesApiEndpoints:\n        serverEndpoints:\n        - clientCIDR:
          0.0.0.0/0\n          serverAddress: https://cluster-name.example.com:6443\n
          \   status: {}\n\n`FederatedCluster` resources pair `Cluster` resources
          to secrets holding\n`KUBECONFIG` information that federation uses to connect
          to `Clusters`.\n\n    kubectl get federatedclusters -n <namespace>\n    NAME
          \          AGE\n    cluster-name   7s\n    \n    kubectl get federatedclusters
          cluster-name -o yaml                                                                                                                                                                     \n
          \   apiVersion: core.federation.k8s.io/v1alpha1\n    kind: FederatedCluster\n
          \   metadata:\n      creationTimestamp: 2019-03-12T22:51:48Z\n      generation:
          1\n      name: cluster-name\n      namespace: <namespace>\n      resourceVersion:
          \"145794\"\n      selfLink: /apis/core.federation.k8s.io/v1alpha1/namespaces/<namespace>/federatedclusters/cluster-name
          \                                                                                                        \n
          \     uid: 6ad4738c-4519-11e9-baaf-02de7fb9d5b2\n    spec:\n      clusterRef:\n
          \       name: cluster-name\n      secretRef:\n        name: cluster-name-nfs7m\n
          \   status:\n      conditions:\n      - lastProbeTime: 2019-03-13T01:38:02Z\n
          \       lastTransitionTime: 2019-03-12T22:57:52Z\n        message: /healthz
          responded with ok\n        reason: ClusterReady\n        status: \"True\"\n
          \       type: Ready\n      region: us-east-1\n      zone: us-east-1a\n\nSee
          also the upstream [user guide](https://github.com/kubernetes-sigs/federation-v2/blob/v0.0.10/docs/userguide.md#join-clusters)
          for more information.\n\n### Unjoining Clusters\n\nUse the `kubefedctl unjoin`
          command to remove a previously joined cluster.\n\n    kubefedctl unjoin
          mycluster \\\n      --host-cluster-context mycluster \\       # name of
          a KUBECONFIG context for the hosting cluster\n      --registry-namespace=<namespace>
          \\       # namespace where federation is deployed\n      -v 2\n\nThis command
          deletes the `FederatedCluster` and `Cluster` for the unjoining\ncluster.\n\n
          \   kubectl get federatedclusters -n <namespace>\n    No resources found.\n\n
          \   kubectl get clusters -n <namespace>\n    No resources found.\n\n###
          Enabling federation for new API types\n\nFederation allows you to spread
          any API type to target clusters. Use\n`kubefedctl enable <API type>` for
          the API types you want to add federation\ncapability for. Enabling federation
          for an API type does two things:\n\n1. Creates a new API type to represent
          the federation API surface for that type\n\n\n2. Creates a `FederatedTypeConfig`
          resource that tells the federation\n  controller to handle the new federation
          API\n\n\n    kubefedctl enable deployments.extensions \\\n      --federation-namespace=<namespace>\n\nRunning
          this command produces a new CRD in the hosting cluster to be the\nfederation
          API surface for the deployments resource.\n\n    kubectl get crd/federateddeployments.types.federation.k8s.io\n
          \   No resources found.\n\n`FederatedTypeConfig` resources configure federation
          to watch the new\nfederation API surface created by `kubefedctl enable`.\n\n
          \   kubectl get federatedtypeconfigs -n <namespace>\n\n\n**Important Note**:
          Federation of a CRD requires that the CRD be installed\non all member clusters.
          If the CRD is not installed on a member cluster,\npropagation to that cluster
          will fail.\n\n\n**Important Note**: You must enable Federation support for
          Namespaces in\nthis configuration before other types will work. Use `kubefedctl
          enable\nnamespaces`.\n\n\n**Important Note**: Federation is currently installed
          **without** any\n`FederatedTypeConfig` resources created. You must run `kubefedctl
          enable`\noperation for each API type you want to add federation capability
          for.\n\nSee also the upstream [user guide](https://github.com/kubernetes-sigs/federation-v2/blob/v0.0.10/docs/userguide.md#enabling-federation-of-an-api-type)
          for more information.\n\n### Example: federating deployments\n\nLet's do
          an example using the `deployments` resource.\n\n    kubectl create -f -
          <<END\n    apiVersion: types.federation.k8s.io/v1alpha1\n    kind: FederatedDeployment\n
          \   metadata:\n      name: hello\n      namespace: <namespace>\n    spec:\n
          \     placement:\n        clusterNames:\n        - cluster-name\n      template:\n
          \       apiVersion: extensions/v1beta1\n        kind: Deployment\n        metadata:\n
          \         name: hello\n        spec:\n          replicas: 1\n          template:\n
          \           metadata:\n              labels:\n                app: hello\n
          \           spec:\n              containers:\n              - name: hello\n
          \               image: openshift/hello-openshift:latest\n                ports:\n
          \               - containerPort: 80\n    END\n\nYou should be able to see
          the deployment in the target cluster!\n\n    kubectl get deployments\n    NAME
          \   READY   UP-TO-DATE   AVAILABLE   AGE\n    hello   1/1     1            1
          \          33s\n\n### Disabling federation for an API type\n\nUse the `kubefedctl
          disable` command to disable federation for an API type.\n\n    kubefedctl
          disable deployments.extensions --federation-namespace=<namespace>\n\nThis
          will disable the propagation of federated deployments, but leave the\nAPI
          surface and `FederatedTypeConfig` intact.\n\nTo fully remove the API surface,
          use the `--delete-from-api` flag:\n\n    kubefedctl disable deployments.extensions
          --federation-namespace=<namespace> --delete-from-api\n\nSee also the upstream
          [user guide](https://github.com/kubernetes-sigs/federation-v2/blob/v0.0.10/docs/userguide.md#disabling-federation-of-an-api-type)
          for more information.\n"
        displayName: Federation
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.0.10
      name: alpha
    defaultChannel: alpha
    packageName: federation
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Kiali
      provider-url: ""
    name: kiali
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kiali
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kiali-operator.v1.13.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "kiali.io/v1alpha1",
                "kind": "Kiali",
                "metadata": {
                  "name": "kiali"
                },
                "spec": {
                  "installation_tag": "My Kiali",
                  "istio_namespace": "istio-system",
                  "deployment": {
                    "namespace": "istio-system",
                    "verbose_mode": "4",
                    "view_only_mode": false
                  },
                  "external_services": {
                    "grafana": {
                      "url": ""
                    },
                    "prometheus": {
                      "url": ""
                    },
                    "tracing": {
                      "url": ""
                    }
                  },
                  "server": {
                    "web_root": "/mykiali"
                  }
                }
              },
              {
                "apiVersion": "monitoring.kiali.io/v1alpha1",
                "kind": "MonitoringDashboard",
                "metadata": {
                  "name": "myappdashboard"
                },
                "spec": {
                  "title": "My App Dashboard",
                  "items": [
                    {
                      "chart": {
                        "name": "My App Processing Duration",
                        "unit": "seconds",
                        "spans": 6,
                        "metricName": "my_app_duration_seconds",
                        "dataType": "histogram",
                        "aggregations": [
                          {
                            "label": "id",
                            "displayName": "ID"
                          }
                        ]
                      }
                    }
                  ]
                }
              }
            ]
          capabilities: Deep Insights
          categories: Monitoring,Logging & Tracing
          certified: "false"
          containerImage: quay.io/kiali/kiali-operator:v1.13.0
          createdAt: "2020-01-21T12:39:22Z"
          description: 'Kiali project provides answers to the questions: What microservices
            are part of my Istio service mesh and how are they connected?'
          repository: https://github.com/kiali/kiali
          support: Kiali
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A configuration file for a Kiali installation.
            displayName: Kiali
            kind: Kiali
            name: kialis.kiali.io
            version: v1alpha1
          - description: A configuration file for defining an individual metric dashboard.
            displayName: Monitoring Dashboard
            kind: MonitoringDashboard
            name: monitoringdashboards.monitoring.kiali.io
            version: v1alpha1
        description: |-
          ## About the managed application

          A Microservice Architecture breaks up the monolith into many smaller pieces
          that are composed together. Patterns to secure the communication between
          services like fault tolerance (via timeout, retry, circuit breaking, etc.)
          have come up as well as distributed tracing to be able to see where calls
          are going.

          A service mesh can now provide these services on a platform level and frees
          the application writers from those tasks. Routing decisions are done at the
          mesh level.

          Kiali works with Istio, in OpenShift or Kubernetes, to visualize the service
          mesh topology, to provide visibility into features like circuit breakers,
          request rates and more. It offers insights about the mesh components at
          different levels, from abstract Applications to Services and Workloads.

          See [https://www.kiali.io](https://www.kiali.io) to read more.

          ### Accessing the UI

          By default, the Kiali operator exposes the Kiali UI as a Route on OpenShift
          or Ingress on Kubernetes.

          On OpenShift, the default root context path is '/' and on Kubernetes it is
          '/kiali' though you can change this by configuring the 'web_root' setting in
          the Kiali CR.

          ## About this Operator

          ### Kiali Custom Resource Configuration Settings

          For quick descriptions of all the settings you can configure in the Kiali
          Custom Resource (CR), see the file
          [kiali_cr.yaml](https://github.com/kiali/kiali/blob/v1.13.0/operator/deploy/kiali/kiali_cr.yaml)

          Note that the Kiali operator can be told to restrict Kiali's access to
          specific namespaces, or can provide to Kiali cluster-wide access to all
          namespaces.

          ## Prerequisites for enabling this Operator

          Today Kiali works with Istio. So before you install Kiali, you must have
          already installed Istio. Note that Istio can come pre-bundled with Kiali
          (specifically if you installed the Istio demo helm profile or if you
          installed Istio with the helm option '--set kiali.enabled=true'). If you
          already have the pre-bundled Kiali in your Istio environment and you want to
          install Kiali via the Kiali Operator, uninstall the pre-bundled Kiali first.
          You can do this via this command,

              kubectl delete --ignore-not-found=true all,secrets,sa,templates,configmaps,deployments,clusterroles,clusterrolebindings,ingresses,customresourcedefinitions --selector="app=kiali" -n istio-system

          When you install Kiali in a non-OpenShift Kubernetes environment, the
          authentication strategy will default to `login`. When using the
          authentication strategy of `login`, you are required to create a Kubernetes
          Secret with a `username` and `passphrase` that you want users to provide in
          order to successfully log into Kiali. Here is an example command you can
          execute to create such a secret (with a username of `admin` and a passphrase
          of `admin`),

              kubectl create secret generic kiali -n istio-system --from-literal "username=admin" --from-literal "passphrase=admin"

          If you wish to use the "ldap" authentication strategy, you must have an LDAP
          server available and accessible to Kiali.
        displayName: Kiali Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Kiali
        version: 1.13.0
      name: alpha
    - currentCSV: kiali-operator.v1.13.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "kiali.io/v1alpha1",
                "kind": "Kiali",
                "metadata": {
                  "name": "kiali"
                },
                "spec": {
                  "installation_tag": "My Kiali",
                  "istio_namespace": "istio-system",
                  "deployment": {
                    "namespace": "istio-system",
                    "verbose_mode": "4",
                    "view_only_mode": false
                  },
                  "external_services": {
                    "grafana": {
                      "url": ""
                    },
                    "prometheus": {
                      "url": ""
                    },
                    "tracing": {
                      "url": ""
                    }
                  },
                  "server": {
                    "web_root": "/mykiali"
                  }
                }
              },
              {
                "apiVersion": "monitoring.kiali.io/v1alpha1",
                "kind": "MonitoringDashboard",
                "metadata": {
                  "name": "myappdashboard"
                },
                "spec": {
                  "title": "My App Dashboard",
                  "items": [
                    {
                      "chart": {
                        "name": "My App Processing Duration",
                        "unit": "seconds",
                        "spans": 6,
                        "metricName": "my_app_duration_seconds",
                        "dataType": "histogram",
                        "aggregations": [
                          {
                            "label": "id",
                            "displayName": "ID"
                          }
                        ]
                      }
                    }
                  ]
                }
              }
            ]
          capabilities: Deep Insights
          categories: Monitoring,Logging & Tracing
          certified: "false"
          containerImage: quay.io/kiali/kiali-operator:v1.13.0
          createdAt: "2020-01-21T12:39:22Z"
          description: 'Kiali project provides answers to the questions: What microservices
            are part of my Istio service mesh and how are they connected?'
          repository: https://github.com/kiali/kiali
          support: Kiali
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A configuration file for a Kiali installation.
            displayName: Kiali
            kind: Kiali
            name: kialis.kiali.io
            version: v1alpha1
          - description: A configuration file for defining an individual metric dashboard.
            displayName: Monitoring Dashboard
            kind: MonitoringDashboard
            name: monitoringdashboards.monitoring.kiali.io
            version: v1alpha1
        description: |-
          ## About the managed application

          A Microservice Architecture breaks up the monolith into many smaller pieces
          that are composed together. Patterns to secure the communication between
          services like fault tolerance (via timeout, retry, circuit breaking, etc.)
          have come up as well as distributed tracing to be able to see where calls
          are going.

          A service mesh can now provide these services on a platform level and frees
          the application writers from those tasks. Routing decisions are done at the
          mesh level.

          Kiali works with Istio, in OpenShift or Kubernetes, to visualize the service
          mesh topology, to provide visibility into features like circuit breakers,
          request rates and more. It offers insights about the mesh components at
          different levels, from abstract Applications to Services and Workloads.

          See [https://www.kiali.io](https://www.kiali.io) to read more.

          ### Accessing the UI

          By default, the Kiali operator exposes the Kiali UI as a Route on OpenShift
          or Ingress on Kubernetes.

          On OpenShift, the default root context path is '/' and on Kubernetes it is
          '/kiali' though you can change this by configuring the 'web_root' setting in
          the Kiali CR.

          ## About this Operator

          ### Kiali Custom Resource Configuration Settings

          For quick descriptions of all the settings you can configure in the Kiali
          Custom Resource (CR), see the file
          [kiali_cr.yaml](https://github.com/kiali/kiali/blob/v1.13.0/operator/deploy/kiali/kiali_cr.yaml)

          Note that the Kiali operator can be told to restrict Kiali's access to
          specific namespaces, or can provide to Kiali cluster-wide access to all
          namespaces.

          ## Prerequisites for enabling this Operator

          Today Kiali works with Istio. So before you install Kiali, you must have
          already installed Istio. Note that Istio can come pre-bundled with Kiali
          (specifically if you installed the Istio demo helm profile or if you
          installed Istio with the helm option '--set kiali.enabled=true'). If you
          already have the pre-bundled Kiali in your Istio environment and you want to
          install Kiali via the Kiali Operator, uninstall the pre-bundled Kiali first.
          You can do this via this command,

              kubectl delete --ignore-not-found=true all,secrets,sa,templates,configmaps,deployments,clusterroles,clusterrolebindings,ingresses,customresourcedefinitions --selector="app=kiali" -n istio-system

          When you install Kiali in a non-OpenShift Kubernetes environment, the
          authentication strategy will default to `login`. When using the
          authentication strategy of `login`, you are required to create a Kubernetes
          Secret with a `username` and `passphrase` that you want users to provide in
          order to successfully log into Kiali. Here is an example command you can
          execute to create such a secret (with a username of `admin` and a passphrase
          of `admin`),

              kubectl create secret generic kiali -n istio-system --from-literal "username=admin" --from-literal "passphrase=admin"

          If you wish to use the "ldap" authentication strategy, you must have an LDAP
          server available and accessible to Kiali.
        displayName: Kiali Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Kiali
        version: 1.13.0
      name: stable
    defaultChannel: stable
    packageName: kiali
    provider:
      name: Kiali
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: WSO2
      provider-url: ""
    name: api-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/api-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: api-operator.v1.0.1
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"wso2.com/v1alpha1","kind":"API","metadata":{"name":"petstore"},"spec":{"definition":{"configmapName":"petstore-swagger","type":"swagger"},"interceptorConfName":"","mode":"privateJet","override":true,"replicas":1,"updateTimeStamp":""}},{"apiVersion":"wso2.com/v1alpha1","kind":"API","metadata":{"name":"test-api-publisher","namespace":"wso2-system"},"spec":{"definition":{"configmapName":"test-publish","type":"swagger"},"replicas":1}},{"apiVersion":"wso2.com/v1alpha1","kind":"Security","metadata":{"name":"jwt-security"},"spec":{"type":"JWT","issuer":"https://localhost:9443/jwt/token","audience":"https://localhost:9443/jwt/token","certificate":"wso2am300-secret"}},{"apiVersion":"wso2.com/v1alpha1","kind":"RateLimiting","metadata":{"name":"app1"},"spec":{"type":"application","description":"Allow
            4 requests per minute","timeUnit":"min","unitTime":1,"requestCount":{"limit":4}}},{"apiVersion":"wso2.com/v1alpha1","kind":"TargetEndpoint","metadata":{"name":"helloworld-sidecar","labels":{"app":"app2"}},"spec":{"protocol":"http","port":8080,"deploy":{"name":"helloworldservice","dockerImage":"lakwarus/helloworld:v1","count":2},"mode":"sidecar"}}]'
          capabilities: Seamless Upgrades
          categories: Developer Tools,      Monitoring,    Cloud Provider
          certified: "false"
          containerImage: wso2/k8s-api-operator:1.0.1
          createdAt: "2019-12-18T03:00:00.000Z"
          description: API Operator provides a fully automated experience for cloud-native
            API management of microservices
          repository: https://github.com/wso2/k8s-apim-operator
          support: ""
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: 'API holds API-related information. You can see the API definition
              and data structure for API here. API takes the Swagger definition as
              a configMap along with replica count and micro-gateway deployment mode. '
            displayName: API
            kind: API
            name: apis.wso2.com
            version: v1alpha1
          - description: 'Security holds security-related information. You can see
              the API definition and data structure for Security` here. Security supports
              different security types: basic-auth, OAuth2, JWT, etc. The following
              YAML shows a sample payload for Security with JWT.'
            displayName: Security
            kind: Security
            name: securities.wso2.com
            version: v1alpha1
          - description: 'RateLimiting holds rate-limiting related information. You
              can see the API definition and data structure for RateLimiting here. '
            displayName: RateLimiting
            kind: RateLimiting
            name: ratelimitings.wso2.com
            version: v1alpha1
          - description: |-
              TargetEndpoint holds endpoint related information. You can see the API definition and data for TargetEndpoint here.

              API gateway can be deployed in three patterns: shared, private-jet, and sidecar.

              If your backend is already running and you need to expose it via a microgateway, you can define the target URL in the Swagger itself.

              If your backend service is not running, but you plan to run it in the same Kubernetes cluster, you can use TargetEndpoint with its relevant Docker image.

              Then APIM Operator will spin-up the corresponding Kubernetes deployment for the defined backend service itself with the microgateway.

              In shared and private-jet mode, the backend can be running in separate PODs, but in sidecar mode, the gateway will run in the same POD adjacent to the backend service.
            displayName: TargetEndpoint
            kind: TargetEndpoint
            name: targetendpoints.wso2.com
            version: v1alpha1
        description: "## Introduction\nAs microservices are increasingly being deployed
          on Kubernetes, the need to expose these microservices as well documented,
          easy to consume, managed APIs is becoming important to develop great applications.
          The API operator for Kubernetes makes APIs a first-class citizen in the
          Kubernetes ecosystem. Similar to deploying microservices, you can now use
          this operator to deploy APIs for individual microservices or compose several
          microservices into individual APIs. With this users will be able to expose
          their microservice as managed API in Kubernetes environment without any
          additional work.\n![](https://raw.githubusercontent.com/wso2/k8s-apim-operator/master/docs/images/API-K8s-Operator.png)\n##
          About this Operator\nAPI Operator provides a fully automated experience
          for cloud-native API management. A user can expose an already deployed microservice
          as an API using the API Operator by providing the API definition of the
          particular microservice.\n\nOnce the API is deployed, it will be deployed
          as a managed API. Also you will be able to add business plans and security
          schemes to your micro-services using the available CRDs.\n\nFurther, by
          integrating with an API portal it can provide additional features, such
          as distributed throttling, token generation, token revocation and monitor
          statistics using dashboard.\n![](https://raw.githubusercontent.com/wso2/k8s-apim-operator/master/docs/images/overview.png)\n##
          Prerequisites\n- [Kubectl](https://kubernetes.io/docs/tasks/tools/install-kubectl/)\n\n-
          [Kubernetes v1.12 or above](https://Kubernetes.io/docs/setup/)   \n - Minimum
          CPU and Memory for the K8s cluster: **2 vCPU, 8GB of Memory**\n\n- An account
          in DockerHub or private docker registry\n- [API controller](https://github.com/wso2/product-apim-tooling/releases/)
          \n- [api-k8s-crds-1.0.1.zip](https://github.com/wso2/k8s-apim-operator/releases/download/v1.0.1/api-k8s-crds-1.0.1.zip)\n\n\n##
          Using the API Operator\n\n### Step 1: Configure API Controller\n\n- Download
          API controller v3.0.0 for your operating system from the [website](https://wso2.com/api-management/tooling/)
          or from [GitHub]((https://github.com/wso2/product-apim-tooling/releases/)\n\n-
          Extract the API controller distribution and navigate inside the extracted
          folder using the command-line tool\n\n- Add the location of the extracted
          folder to your system's $PATH variable to be able to access the executable
          from anywhere.\n\nYou can find available operations using the below command.\n```\n>>
          apictl --help\n```\nBy default API controller does not support kubectl command.
          \nSet the API Controllerâ€™s mode to Kubernetes to be compatible with kubectl
          commands\n\n```\n>> apictl set --mode k8s \n```\n\n### Step 2: Install API
          Operator Configurations\n\n\n* Execute the below command to create wso2-system
          namespace\n```\nkubectl create ns wso2-system\n```\n* Extract [api-k8s-crds-1.0.1.zip](https://github.com/wso2/k8s-apim-operator/releases/download/v1.0.1/api-k8s-crds-1.0.1.zip)\n\n
          \   1. This zip contains the artifacts that required to deploy in Kubernetes.\n
          \   2. Extract api-k8s-crds-1.0.1.zip\n    \n    ```\n    cd api-k8s-crds-1.0.1\n
          \   ```\n \n\n* Deploy the controller level configurations **[IMPORTANT]**\n
          \n    When you deploy an API, this will create a docker image for the API
          and be pushed to Docker-Hub. For this, your Docker-Hub credentials are required.\n
          \   \n    1. Open **apim-operator/controller-configs/controller_conf.yaml**
          and navigate to docker registry section(mentioned below), and  update ***user's
          docker registry***.\n            \n        ```\n        #docker registry
          name which the mgw image to be pushed.  eg->  dockerRegistry: username\n
          \       dockerRegistry: <username-docker-registry>\n        ```\n        \n
          \   2. Open **apim-operator/controller-configs/docker_secret_template.yaml**
          and navigate to data section. <br>\n        Enter the base 64 encoded username
          and password of the Docker-Hub account \n        \n        ```\n        data:\n
          \        username: ENTER YOUR BASE64 ENCODED USERNAME\n         password:
          ENTER YOUR BASE64 ENCODED PASSWORD\n        ```\n        Once you done with
          the above configurations, execute the following command to deploy controller
          configurations.\n\n        ```\n        >> kubectl apply -f apim-operator/controller-configs/\n
          \       \n        configmap/controller-config created\n        configmap/apim-config
          created\n        security.wso2.com/default-security-jwt created\n        secret/wso2am300-secret
          created\n        configmap/docker-secret-mustache created\n        secret/docker-secret
          created\n        configmap/dockerfile-template created\n        configmap/mgw-conf-mustache
          created\n        ```\n\n### Step 3: Expose the sample microservice as a
          managed API\n\nLetâ€™s deploy an API for our microservice.\nThe Open API definition
          of an example API can be found [here](https://github.com/wso2/k8s-apim-operator/blob/v1.0.1/scenarios/scenario-2/swagger.yaml)\n\nThe
          endpoint of our microservice is referred in the API definition.\n\n- Deploy
          the API using the following command\n\n    ```\n    >> apictl add api -n
          \"api_name\" --from-file=\"location to the Open API definition\" \n    ```\n\t
          Optional Parameters:\n    --replicas=3          Number of replicas\n    --namespace=wso2
          \     Namespace to deploy the API\n    --overwrite=true\t  Overwrite the
          docker image creation for already created docker image\n    \n    **_Note:_**
          Namespace and replicas are optional parameters. If they are not provided,
          the default namespace will be used and 1 replica will be created. \n\t\t\n
          \   ```\n        apictl add api -n petstore-api --from-file=swagger.yaml\n
          \   ``` \n    - Output:\n    ```\n        creating configmap with swagger
          definition\n        configmap/petstore-api-swagger created\n        api.wso2.com/petstore-api
          created\n    ```\n    \n- Get available API <br /> \n    ```\n        apictl
          get apis\n    ```\n    - Output:\n    ```    \n        NAME          AGE\n
          \       petstore-api   5m\n    ```\n\n    When you deploy the API, it will
          first run the Kaniko job. This basically builds the docker image of the
          API and pushes it to Docker-Hub. \n\n    Once the Kaniko job is completed,
          it will deploy the managed API for your microservice.\n\n- Verify the API
          deployment\n\n    If you list down the pods immediately after the add API
          command you will only see the pod related to Kaniko job. Once it is completed
          you will see the deployed API. If you are on Minikube, this might take several
          minutes.\n\n    ```\n    >> kubectl get pods \n    \n    Output:\n    NAME
          \                                  READY   STATUS    RESTARTS   AGE\n    petstore-api-kaniko-fxvkt
          \             1/1     Running   0          45s\n    ```\n\n    If you execute
          the same command after sometime you will see the managed API has been deployed
          after the Kaniko job.\n\n    ```\n    >> kubectl get pods \n    \n    Output:\n
          \   NAME                                   READY   STATUS    RESTARTS   AGE\n
          \   petstore-api-6957fc89d6-kn9sp          1/1     Running   0          21s\n
          \   ```\n\n- Get service details to invoke the API. (Please wait until the
          external-IP is populated in the corresponding service)\n    ```\n    >>
          kubectl get services \n    \n    Output:\n    NAME               TYPE           CLUSTER-IP
          \    EXTERNAL-IP      PORT(S)                         AGE\n    online-store
          \      LoadBalancer   10.83.9.142    35.232.188.134   9095:31055/TCP,9090:32718/TCP
          \  57s\n    ```\n\n**You now have a microgateway deployed in Kubernetes
          that runs your API for the microservice.**\n- You can see petstore service
          has been exposed as a managed API.\n- Get the external IP of the managed
          API's service OR minikube ip and nodeport related to 9095 if you are using
          minikube\n\n- Invoking the API <br />\n    ```\n       TOKEN=eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsIng1dCI6IlpqUm1ZVE13TlRKak9XVTVNbUl6TWpnek5ESTNZMkl5TW1JeVkyRXpNamRoWmpWaU1qYzBaZz09In0.eyJhdWQiOiJodHRwOlwvXC9vcmcud3NvMi5hcGltZ3RcL2dhdGV3YXkiLCJzdWIiOiJhZG1pbkBjYXJib24uc3VwZXIiLCJhcHBsaWNhdGlvbiI6eyJvd25lciI6ImFkbWluIiwidGllciI6IlVubGltaXRlZCIsIm5hbWUiOiJzYW1wbGUtY3JkLWFwcGxpY2F0aW9uIiwiaWQiOjMsInV1aWQiOm51bGx9LCJzY29wZSI6ImFtX2FwcGxpY2F0aW9uX3Njb3BlIGRlZmF1bHQiLCJpc3MiOiJodHRwczpcL1wvd3NvMmFwaW06MzIwMDFcL29hdXRoMlwvdG9rZW4iLCJ0aWVySW5mbyI6e30sImtleXR5cGUiOiJQUk9EVUNUSU9OIiwic3Vic2NyaWJlZEFQSXMiOltdLCJjb25zdW1lcktleSI6IjNGSWlUM1R3MWZvTGFqUTVsZjVVdHVTTWpsUWEiLCJleHAiOjM3MTk3Mzk4MjYsImlhdCI6MTU3MjI1NjE3OSwianRpIjoiZDI3N2VhZmUtNTZlOS00MTU2LTk3NzUtNDQwNzA3YzFlZWFhIn0.W0N9wmCuW3dxz5nTHAhKQ-CyjysR-fZSEvoS26N9XQ9IOIlacB4R5x9NgXNLLE-EjzR5Si8ou83mbt0NuTwoOdOQVkGqrkdenO11qscpBGCZ-Br4Gnawsn3Yw4a7FHNrfzYnS7BZ_zWHPCLO_JqPNRizkWGIkCxvAg8foP7L1T4AGQofGLodBMtA9-ckuRHjx3T_sFOVGAHXcMVwpdqS_90DeAoT4jLQ3darDqSoE773mAyDIRz6CAvNzzsWQug-i5lH5xVty2kmZKPobSIziAYes-LPuR-sp61EIjwiKxnUlSsxtDCttKYHGZcvKF12y7VF4AqlTYmtwYSGLkXXXw\n
          \   ```\n   \n    ```\n        curl -X GET \"https://<external IP of LB
          service>:9095/petstore/v1/pet/55\" -H \"accept: application/xml\" -H \"Authorization:Bearer
          $TOKEN\" -k\n    ```\n    - Once you execute the above command, it will
          call to the managed API (petstore-api), which then call its endpoint(https://petstore.swagger.io/v2).
          If the request is success, you would be able to see the response as below.\n
          \   ```\n        <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?><Pet><category><id>55</id><name>string</name></category><id>55</id><name>SRC_TIME_SIZE</name><photoUrls><photoUrl>string</photoUrl></photoUrls><status>available</status><tags><tag><id>55</id><name>string</name></tag></tags></Pet>\n
          \   ```\n    \n\n- Delete the  API <br /> \n    - Following command will
          delete all the artifacts created with this API including pods, deployment
          and services.\n    ```\n        apictl delete api petstore-api\n    ```\n
          \   -  Output:\n    ```\n        api.wso2.com \"petstore-api\" deleted\n
          \   ```\n\t\t\n**Please refer the [documentation](https://github.com/wso2/k8s-apim-operator/tree/v1.0.1/docs)
          for further details**\n\n\n\n\n\n\n\n"
        displayName: API Operator for Kubernetes
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: WSO2
        version: 1.0.1
      name: stable
    defaultChannel: stable
    packageName: api-operator
    provider:
      name: WSO2
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Turbonomic, Inc.
      provider-url: ""
    name: kubeturbo
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kubeturbo
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kubeturbo-operator.v6.4.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"Kubeturbo","metadata":{"name":"kubeturbo-release"},"spec":{"restAPIConfig":{"opsManagerPassword":"Turbo_password","opsManagerUserName":"Turbo_username"},"serverMeta":{"turboServer":"https://Turbo_server_URL","version":6.4}}}]'
          capabilities: Basic Install
          categories: Monitoring
          certified: "false"
          containerImage: turbonomic/kubeturbo-operator:6.4
          createdAt: "2019-05-01 00:00:00"
          description: Turbonomic Workload Automation for Multicloud simultaneously
            optimizes performance, compliance, and cost in real-time. Workloads are
            precisely resourced, automatically, to perform while satisfying business
            constraints.
          repository: https://github.com/turbonomic/kubeturbo/tree/master/deploy/kubeturbo-operator
          support: Turbonomic, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Turbonomic Workload Automation for Multicloud simultaneously
              optimizes performance, compliance, and cost in real-time. Workloads
              are precisely resourced, automatically, to perform while satisfying
              business constraints.
            displayName: Kubeturbo Operator
            kind: Kubeturbo
            name: kubeturbos.charts.helm.k8s.io
            version: v1alpha1
        description: |-
          ### Decision Automation for Kubernetes
          Turbonomic makes workloads smartÃ¢â‚¬â€enabling them to self-manageÃ¢â‚¬â€and determines the specific actions that will drive continuous health:

          * Continuous placement for Pods (rescheduling)
          * Continuous scaling for applications and  the underlying cluster.

          It assures application performance by giving workloads the resources they need when they need them.

          ### How does it work?
          Turbonomic uses a container Ã¢â‚¬â€ KubeTurbo Ã¢â‚¬â€ that runs in your Kubernetes or Red Hat OpenShift cluster to discover and monitor your environment.
          KubeTurbo runs together with the default scheduler and sends this data back to the Turbonomic analytics engine.
          Turbonomic determines the right actions that drive continuous health, including continuous placement for Pods and continuous scaling for applications and the underlying cluster.
        displayName: Kubeturbo Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Turbonomic, Inc.
        version: 6.4.0
      name: alpha
    defaultChannel: alpha
    packageName: kubeturbo
    provider:
      name: Turbonomic, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: PlanetScale
      provider-url: ""
    name: planetscale
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/planetscale
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: planetscale-operator.v0.1.8
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "planetscale.com/v1alpha1",
                "kind": "PsCluster",
                "metadata": {
                  "name": "example"
                },
                "spec": {
                  "monitored": true,
                  "lockserver": {
                    "endpoint": "<LockServer IP and Port>",
                    "root_path": "/vitess/example/global"
                  },
                  "cells": [
                    {
                      "name": "example1",
                      "useGlobalLockserver": true,
                      "gateway": {
                        "count": 2
                      },
                      "vtctld": {
                        "count": 1
                      },
                      "keyspaces": [
                        {
                          "name": "messagedb",
                          "shards": [
                            {
                              "range": "-80",
                              "replicas": [
                                {
                                  "type": "replica"
                                },
                                {
                                  "type": "replica"
                                },
                                {
                                  "type": "rdonly"
                                },
                                {
                                  "type": "replica"
                                }
                              ]
                            },
                            {
                              "range": "80-",
                              "replicas": [
                                {
                                  "type": "replica"
                                },
                                {
                                  "type": "replica"
                                },
                                {
                                  "type": "rdonly"
                                },
                                {
                                  "type": "replica"
                                }
                              ]
                            }
                          ]
                        }
                      ]
                    }
                  ]
                }
              }
            ]
          capabilities: Deep Insights
          categories: Database
          certified: "False"
          containerImage: registry.connect.redhat.com/planetscale/operator:0.1.8
          createdAt: "2019-03-06 21:40:00"
          description: PlanetScale's operator for Vitess deploys and manages instances
            of MySQL with Vitess, a database clustering system for horizontal scaling
            of MySQL.
          support: PlanetScale
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Instance of a PlanetScale Vitess Cluster
            displayName: PsCluster
            kind: PsCluster
            name: psclusters.planetscale.com
            version: v1alpha1
        description: |-
          The Vitess Operator deploys and manages instances of MySQL with Vitess, a database clustering system for horizontal scaling of MySQL
          through generalized sharding.


          By encapsulating shard-routing logic, Vitess allows application code and
          database queries to remain agnostic to the distribution of data onto
          multiple shards. With Vitess, you can even split and merge shards as your
          needs grow, with an atomic cutover step that takes only a few seconds.


          Vitess has been a core component of YouTube's database infrastructure since
          2011, and has grown to encompass tens of thousands of MySQL nodes. For more
          information, visit [https://planetscale.com](https://planetscale.com)

          ### New Features

          We have support for Orchestrator, backups, and restores from GCS and S3. We also have seamless upgrades for all containers.

          ### Supported Features

          * **Dashboard** - The Operator deploys a dashboard for monitoring and introspecting your cluster.

          * **Scale Gateways** - Scale out the Gateway, which is the component that responds to queries and returns consolidated results from the MySQL shards.

          * **Configure Keyspaces** - Configure how your data is sharded with in your cluster.

          ### Before You Start

          1. Create a RedHat registry image pull secret called
          `planetscale-operator-pull-secret`, which is required to pull the operator
          image.

          2. Create an etcd cluster, which is used as a lock server by PlanetScale Vitess clusters. Try out the [etcd Operator](https://www.operatorhub.io/?keyword=etcd).

          ### Permissions

          This operator only supports running in the same namespace as the PsCluster resources it is watching.


          This operator should be deployed in an isolated namespace since the Pods it creates use the `default` service account and require the `use` permission on the `anyuid` Security Context Contraint (SCC) to run correctly. Running this operator in a shared namespace is not recommended since unrelated pods will have access to use the `anyuid` SCC.
        displayName: PlanetScale Operator for Vitess
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: PlanetScale
        version: 0.1.8
      name: beta
    defaultChannel: beta
    packageName: planetscale
    provider:
      name: PlanetScale
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: MemSQL
      provider-url: ""
    name: memql-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/memql-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: memsql-operator.v0.0.2
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "memsql.com/v1alpha1",
              "kind": "MemsqlCluster",
              "metadata": {
                "name": "memsql-cluster"
              },
              "spec": {
                "license": "BDVhNDg5OWFjNTc3OTQxODRhOTVlMWVlZWY1YTFlNTlkAAAAAAAAAAAEAAAAAAAAAAwwNgIZAPXaOk16AZMCIzdmkvvO0VH57SIhkshiMAIZAJYHyL5vkNE4Sohz8BDqMuikVj9lNLvy4w==",
                "adminHashedPassword": "*FABE5482D5AADF36D028AC443D117BE1180B9725",
                "releaseID": "722ce44d-6f95-4855-b093-9802a9ae7cc9",
                "redundancyLevel": 1,
                "aggregatorSpec": {
                  "count": 3,
                  "height": 0.5,
                  "storageGB": 256,
                  "storageClass": "standard"
                },
                "leafSpec": {
                  "count": 1,
                  "height": 1,
                  "storageGB": 1024,
                  "storageClass": "standard"
                }
              }
            }]
          capabilities: Seamless Upgrades
          categories: Database, OpenShift Optional
          certified: "true"
          containerImage: registry.connect.redhat.com/memsql/operator:ubi-v0.0.2
          createdAt: "2019-03-27 12:59:59"
          description: Install a MemSQL Cluster
          support: MemSQL
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: MemSQL
            displayName: MemSQL
            kind: MemsqlCluster
            name: memsqlclusters.memsql.com
            version: v1alpha1
        description: |
          This package contains everything you need to run the MemSQL Kubernetes Operator
          in your own Kubernetes cluster.

          # Getting Started

          * Install RBAC resources
              ```bash
              kubectl create -f rbac.yaml
              ```
          * Install the MemSQL Cluster CRD
              ```bash
              kubectl create -f memsql-cluster-crd.yaml
              ```
          * Deploy the Operator
              ```bash
              kubectl create -f deployment.yaml
              kubectl get deployments
              kubectl get pods
              ```

          # Creating a cluster

          * Edit the memsql-cluster.yaml file for your MemSQL cluster
              * name -> cluster name of your choice
              * license -> license key from portal.memsql.com
              * releaseID -> use "722ce44d-6f95-4855-b093-9802a9ae7cc9" (MemSQL v6.7.14)
              * adminHashedPassword -> `HASHED_PASSWORD`
                  * see below Python script to convert your password, include the leading `*`

              * Python function to hash your password:
                  ```python
                  from hashlib import sha1
                  print("*" + sha1(sha1('mypassword').digest()).hexdigest().upper())
                  ```

              * redundancyLevel -> 1 or 2
              * units -> number of nodes. Leaf nodes will automatically be doubled if redundancyLevel=2
              * height -> height=1 corresponds to one MemSQL License Unit (8vCPU/32GB RAM)
              * storageGB -> size of the persistent volume claim in GB
              * storageClass -> name of the Kubernetes storage class for the
                persistent volume claim
              * NOTE: the minimum production config should be:
                  * 1 leaf unit @ height 1
                  * 3 aggregator units @ height 0.5
                  * redundancyLevel = 2

          * Create the cluster.
              ```bash
              kubectl create -f memsql-cluster.yaml
              ```

          * To get logs for operator (or any MemSQL node), run the following after looking
            up the PODNAME using `kubectl get pods`:
              ```bash
              kubectl logs PODNAME
              ```

          * To get info about all MemSQL clusters running, use these commands. -oyaml will expose the DDL/DML endpoints for MemSQL.
              ```bash
              kubectl get memsql
              kubectl describe memsql
              ```

          * To see all the pods, services, stateful sets, & resources used by the MemSQL cluster, run:
              ```bash
              kubectl get all
              ```

          * Connect to the MemSQL cluster.
            Use the DDL endpoint for creating tables.
            DML endpoint (load balancer amongst other aggregators) can be used for read/write
            workload. Use the user called "admin" with the password you defined during
            cluster creation.
              ```bash
              mysql -u admin -h <hostname> -P <port> -p<password>
              ```

          * Notes on editing a MemSQL cluster:
              * The MemSQL operator supports scaling and changing certain parts of the spec. Currently you can change:
                  * units
                  * height
                  * releaseID
                  * adminHashedPassword
                  * license
                  * redundancyLevel

          * To delete the MemSQL cluster:
              ```bash
              kubectl delete -f memsql-cluster.yaml
              ```
        displayName: MemSQL
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: MemSQL
        version: 0.0.2
      name: alpha
    defaultChannel: alpha
    packageName: memql-certified
    provider:
      name: MemSQL
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Kubemq.io
      provider-url: ""
    name: kubemq-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kubemq-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kubemq-operator.v0.3.1
      currentCSVDesc:
        annotations:
          alm-examples: "[{\n \t\"apiVersion\": \"core.k8s.kubemq.io/v1alpha1\",\n
            \t\"kind\": \"KubemqCluster\",\n \t\"metadata\": {\n \t\t\"name\": \"kubemq-cluster\",\n
            \t\t\"labels\": {\n \t\t\t\"kubemq.cluster.name\": \"kubemq-cluster\"\n
            \t\t}\n \t},\n \t\"spec\": {\n \t\t\"replicas\": 3\n \t}\n }]"
          capabilities: Auto Pilot
          categories: ' Streaming & Messaging'
          certified: "false"
          containerImage: docker.io/kubemq/kubemq-operator:latest
          createdAt: "2020-02-16 18:00:00"
          description: KubeMQ is Kubernetes Message Queue Broker
          repository: https://github.com/kubemq-io/kubemq-operator
          support: support@kubemq.io
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Kubemq Cluster
            displayName: KubemqCluster
            kind: KubemqCluster
            name: kubemqclusters.core.k8s.kubemq.io
            version: v1alpha1
        description: |-
          ## What is KubeMQ?
          Enterprise-grade message queue broker native for Kubernetes. Delivered in a production-ready cluster, and designed for any type of workload.
          KubeMQ is provided as a small, lightweight Docker container, designed for any workload and architecture running in Kubernetes or any other container orchestration system which support Docker.

          ## Main Features
          - All-batteries included Messaging Broker for Kubernetes environment
          - Blazing fast (written in Go), small and lightweight Docker container
          - Asynchronous and Synchronous messaging with support for  `Exactly One Delivery`, `At Most Once Delivery` and `At Least Once Delivery` models
          - Supports durable FIFO based Queue, Publish-Subscribe Events, Publish-Subscribe with Persistence (Events Store), RPC Command and Query messaging patterns
          - Supports gRPC, Rest and WebSocket Transport protocols with TLS support (both RPC and Stream modes)
          - Authentication and authorisation
          - No Message broker configuration needed (i.e., queues, exchanges)
          - Built-in Caching, Metrics, and Tracing
          - .Net, Java, Python, Go and NodeJS SDKs

          ## Messaging Patterns

          ### Queues
          KubeMQ supports distributed durable FIFO based queues with the following core features:

          - **Exactly One Delivery** - Only one message guarantee will deliver to the subscriber
          - **Single and Batch Messages Send and Receive** - Single and multiple messages in one call
          - **Multicast and routing** - sending to multiple sources in one command
          - **RPC and Stream Flows** - RPC flow allows an insert and pull messages in one call. Stream flow allows single message consuming in transactional way
          - **Message Policy** - Each message can be configured with expiration and delay timers. In addition, each message can specify a dead-letter queue for un-processed messages attempts
          - **Long Polling** - Consumers can wait until a message available in the queue to consume
          - **Peak Messages** - Consumers can peek into a queue without removing them from the queue
          - **Ack All Queue Messages** - Any client can mark all the messages in a queue as discarded and will not be available anymore to consume
          - **Visibility timers** - Consumers can pull a message from the queue and set a timer which will cause the message not be visible to other consumers. This timer can be extended as needed.
          - **Resend Messages** - Consumers can send back a message they pulled to a new queue or send a modified message to the same queue for further processing.

          ### Pub/Sub

          KubeMQ supports Publish-Subscribe (a.k.a Pub/Sub) messages patterns with the following core features:

          - **Events** -  An asynchronous real-time Pub/Sub pattern.
          - **Events Store** -An asynchronous Pub/Sub pattern with persistence.
          - **Grouping** - Load balancing of events between subscribers
          - **Multicast and routing** - sending to multiple sources in one command

          ### RPC
          KubeMQ supports CQRS based RPC flows with the following core features:

          - **Commands** -  A synchronous two ways Command pattern for CQRS types of system architecture.
          - **Query** - A synchronous two ways Query pattern for CQRS types of system architecture.
          - **Response** - An answer for a Query type RPC call
          - **Timeout** - Timeout interval is set for each RPC call. Once no response is received within the Timeout interval, RPC call return an error
          - **Grouping** - Load balancing of RPC calls between receivers
          - **Caching** - RPC response can be cached for future requests without the need to process again by a receiver

          ## Interfaces
          - **gRPC** - High performance RPC and streaming framework that can run in any environment, Open source and Cloud Native.
          - **Rest** - Restful Api with WebSocket support for bi-directional streaming.

          ## SDK
          - **C#** - C# SDK based on gRPC
          - **Java** - Java SDK based on gRPC
          - **Go** - Go SDK based on gRPC
          - **Python** - Python SDK based on gRPC
          - **cURL** - cURL SDK based on Rest
          - **Node** - Node SDK based on gRPC and Rest
          - **PHP** - PHP SDK based on Rest
          - **Ruby** - Ruby SDK based on Rest
          - **jQuery** jQuery SDK based Rest

          ## Observability
          - **Metrics** - Metrics can be exported to variety of Backends
          | Spec               | Field                     | Type/Options                                                 | Description                                                         | Example              |
          |:-------------------|:--------------------------|:-------------------------------------------------------------|:--------------------------------------------------------------------|:---------------------|
          | **replicas**       |                           | int                                                          | Desired amount of pods in the cluster                               | 3                    |
          | **configData**     |                           | yaml/toml                                                    | Load configuration file                                             |                      |
          | **volume**         |                           |                                                              | **Setting for Persisted Volume**                                    |                      |
          |                    | size                      | string                                                       | Desired size of Persisted Volume Claim                              | "30Gi"               |
          | **license**        |                           |                                                              | **Setting for License**                                             |                      |
          |                    | data                      | string                                                       | Set Licence data key                                                |                      |
          |                    | token                     | string                                                       | Set License Token                                                   |                      |
          | **image**          |                           |                                                              | **Setting for Image**                                               |                      |
          |                    | registry                  | string                                                       | Set registry host                                                   | docker.io            |
          |                    | repository                | string                                                       | Set repository name                                                 | kubemq/kubemq-uni    |
          |                    | tag                       | string                                                       | Set image tag                                                       | latest               |
          |                    | pullPolicy                | Always/IfNotPresent/Never                                    | Set image pull policy                                               | Always               |
          | **api**            |                           |                                                              | **Setting for Api interface**                                       |                      |
          |                    | disable                   | bool                                                         | Disable Api interface                                               | false                |
          |                    | expose                    | ClusterIP/NodePort/LoadBalancer                              | Desired service type                                                | ClusterIP (Default)  |
          |                    | nodePort                  | int                                                          | Desired port number in NodePort expose type                         | 31000                |
          | **grpc**           |                           |                                                              | **Setting for gRPC interface**                                      |                      |
          |                    | disable                   | bool                                                         | Disable gRPC interface                                              |                      |
          |                    | expose                    | ClusterIP/NodePort/LoadBalancer                              | Desired service type                                                | ClusterIP (Default)  |
          |                    | nodePort                  | int                                                          | Desired port number in NodePort expose type                         | 31000                |
          |                    | bufferSize                | int                                                          | set subscribe message / requests buffer size to use on server       | 100                  |
          |                    | bodyLimit                 | int                                                          | Set Max size of payload in bytes                                    | 1,048,576 (1M bytes) |
          | **rest**           |                           |                                                              | **Setting for rest interface**                                      |                      |
          |                    | disable                   | bool                                                         | Disable rest interface                                              |                      |
          |                    | expose                    | ClusterIP/NodePort/LoadBalancer                              | Desired service type                                                | ClusterIP (Default)  |
          |                    | nodePort                  | int                                                          | Desired port number in NodePort expose type                         | 31000                |
          |                    | bufferSize                | int                                                          | set subscribe message / requests buffer size to use on server       | 100                  |
          |                    | bodyLimit                 | int                                                          | Set Max size of payload in bytes                                    | 1,048,576 (1M bytes) |
          | **tls**            |                           |                                                              | **Setting for TLS**                                                 |                      |
          |                    | cert                      | string                                                       | Set tls certification data                                          |                      |
          |                    | key                       | string                                                       | Set tls private key data                                            |                      |
          |                    | ca                        | string                                                       | Set tls certification authority  data                               |                      |
          | **authentication** |                           |                                                              | **Setting for Authentication**                                      |                      |
          |                    | key                       | string                                                       | Set JWT public key data                                             |                      |
          |                    | type                      | string HS256/HS384/HS512/RS256/RS384/RS512/ES256/ES384/ES512 | Set JWT public key signing method                                   |                      |
          | **authorization**  |                           |                                                              | **Setting for Authorization**                                       |                      |
          |                    | policy                    | string                                                       | Set Authorization policy data                                       |                      |
          |                    | url                       | string url                                                   | Set Optional authorization server url for policy data               |                      |
          |                    | autoReload                | int                                                          | Set auto reload policy data from url                                |                      |
          | **routing**        |                           |                                                              | **Setting for Routing**                                             |                      |
          |                    | data                      | string                                                       | Set Routing data                                                    |                      |
          |                    | url                       | string url                                                   | Set Optional routing server url for routing data                    |                      |
          |                    | autoReload                | int                                                          | Set auto reload data from url                                       |                      |
          | **store**          |                           |                                                              | **Setting for Store**                                               |                      |
          |                    | clean                     | bool                                                         | Set clear persistence data on start-up                              | false                |
          |                    | path                      | string                                                       | Set persistence file path                                           | ./store              |
          |                    | maxChannels               | int                                                          | Set limit number of persistence channels                            | 0                    |
          |                    | maxSubscribers            | int                                                          | Set limit of subscribers per channel                                | 0                    |
          |                    | maxMessages               | int                                                          | Set limit of messages per channel                                   | 0                    |
          |                    | maxChannelSize            | int                                                          | Set limit size of channel in bytes                                  | 0                    |
          |                    | messagesRetentionMinutes  | int                                                          | Set message retention time in minutes                               | 1440                 |
          |                    | purgeInactiveMinutes      | int                                                          | Set time in minutes of channel inactivity to delete                 | 1440                 |
          | **queue**          |                           |                                                              | **Setting for Queue**                                               |                      |
          |                    | maxReceiveMessagesRequest | int                                                          | Set max of sending / receiving batch of queue message               | 1024                 |
          |                    | maxWaitTimeoutSeconds     | int                                                          | Set max expiration allowed for message                              | 3600                 |
          |                    | maxExpirationSeconds      | int                                                          | Set max expiration allowed for message                              | 43200                |
          |                    | maxDelaySeconds           | int                                                          | set max delay seconds allowed for message                           | 43200                |
          |                    | maxReQueues               | int                                                          | Set max retires to receive message before discard                   | 1024                 |
          |                    | maxVisibilitySeconds      | int                                                          | Set max time of hold received message before returning to queue     | 43200                |
          |                    | defaultVisibilitySeconds  | int                                                          | Set default time of hold received message before returning to queue | 60                   |
          |                    | defaultWaitTimeoutSeconds | int                                                          | Set default time to wait for a message in a queue                   | 1                    |
          | **resource**       |                           |                                                              | **Setting for Resources**                                           |                      |
          |                    | limitsCpu                 | string                                                       | Set Limits CPU                                                      | 500m                 |
          |                    | limitsMemory              | string                                                       | Set Limits Memory                                                   | 2Gi                  |
          |                    | requestsCpu               | string                                                       | Set Requests CPU                                                    | 1                    |
          |                    | requestsMemory            | string                                                       | Set Requests Memory                                                 | 512M                 |
          | **nodeSelector**   |                           |                                                              | **Setting for Node Selector**                                       |                      |
          |                    | keys                      | key/value strings                                            | Set Key Value for node selector                                     |                      |
          | **health**         |                           |                                                              | **Setting for health prob**                                         |                      |
          |                    | enabled                   | bool                                                         | Set enable/disable health prob                                      |                      |
          |                    | initialDelaySeconds       | int                                                          | Set Initial Delay Seconds                                           |                      |
          |                    | periodSeconds             | int                                                          | Set Period Seconds Interval                                         |                      |
          |                    | timeoutSeconds            | int                                                          | Set Timeout Seconds                                                 |                      |
          |                    | successThreshold          | int                                                          | Set Success Threshold                                               |                      |
          |                    | failureThreshold          | int                                                          | Set Failure Threshold                                               |                      |
          | **log**            |                           |                                                              | **Setting for log**                                                 |                      |
          |                    | level                     | int                                                          | Set log level, 1 - debug , 2 - info, 3 - warn, 4 - error, 5 - fatal | 2                    |
          |                    | file                      | string                                                       | Set log file write path                                             | ./log                |
        displayName: Kubemq Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Kubemq.io
        version: 0.3.1
      name: alpha
    defaultChannel: alpha
    packageName: kubemq-operator
    provider:
      name: Kubemq.io
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat, Inc.
      provider-url: ""
    name: openshifttemplateservicebroker
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/openshifttemplateservicebroker
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: openshifttemplateservicebrokeroperator.4.2.18-202002031246
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"osb.openshift.io/v1","kind":"TemplateServiceBroker","metadata":{"name":"template-service-broker","namespace":"template-service-broker"},"spec":{}}]'
          capabilities: Seamless Upgrades
          containerImage: registry.redhat.io/openshift4/ose-template-service-broker@sha256:d043deed8a0496e0a42bf5304c151d4b277e53a3ea2a4ca3546514bc81f3ec21
          description: OpenShift Template Service Broker is an implementation of the
            [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Open Service Broker supporting management of OpenShift
              templates.
            displayName: Template Service Broker
            kind: TemplateServiceBroker
            name: templateservicebrokers.osb.openshift.io
            version: v1
        description: |
          The OpenShift Template Service Broker implements the [Open Service Broker
          API](https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md)
          endpoints:
          - *Catalog*: returns a list of available templates as OSB API
            *Service* objects (the templates are read from one or more
            namespaces configured in the master config).
          - *Provision*: provision a given template (referred by its UID) into a
            namespace.  Under the covers, this creates a non-namespaced
            **BrokerTemplateInstance** object for the template service broker to
            store state associated with the the instantiation, as well as the
            **Secret** and **TemplateInstance** objects which are picked up by
            the **TemplateInstance** controller.  *Provision* is an asynchronous
            operation: it may return before provisioning is completed, and the
            provision status can (must) be recovered via the *Last Operation*
            endpoint (see below).
          - *Bind*: for a given template, return "credentials" exposed in any
            created ConfigMap, Secret, Service or Route object (see
            ExposeAnnotationPrefix and Base64ExposeAnnotationPrefix
            documentation).  The *Bind* call records the fact that it took
            place in the appropriate **BrokerTemplateInstance** object.
          - *Unbind*: this simply removes the metadata previously placed in the
            **BrokerTemplateInstance** object by a *Bind* call.
          - *Deprovision*: removes the objects created by the *Provision* call.
            The garbage collector removes all additional objects created by the
            **TemplateInstance** controller, hopefully transitively, as
            documented above.
          - *Last Operation*: returns the status of the previously run
            asynchronous operation.  In the template service broker, *Provision*
            is the only asynchronous operation.
        displayName: OpenShift Template Service Broker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.2.18-202002031246
      name: "4.2"
    - currentCSV: openshifttemplateservicebrokeroperator.4.2.18-202002031246-s390x
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"osb.openshift.io/v1","kind":"TemplateServiceBroker","metadata":{"name":"template-service-broker","namespace":"template-service-broker"},"spec":{}}]'
          capabilities: Seamless Upgrades
          containerImage: registry.redhat.io/openshift4/ose-template-service-broker@sha256:56a99fefeb2ff3fcab12d158903693eca0eb4c0fa787978e90407651ce28a722
          description: OpenShift Template Service Broker is an implementation of the
            [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Open Service Broker supporting management of OpenShift
              templates.
            displayName: Template Service Broker
            kind: TemplateServiceBroker
            name: templateservicebrokers.osb.openshift.io
            version: v1
        description: |
          The OpenShift Template Service Broker implements the [Open Service Broker
          API](https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md)
          endpoints:
          - *Catalog*: returns a list of available templates as OSB API
            *Service* objects (the templates are read from one or more
            namespaces configured in the master config).
          - *Provision*: provision a given template (referred by its UID) into a
            namespace.  Under the covers, this creates a non-namespaced
            **BrokerTemplateInstance** object for the template service broker to
            store state associated with the the instantiation, as well as the
            **Secret** and **TemplateInstance** objects which are picked up by
            the **TemplateInstance** controller.  *Provision* is an asynchronous
            operation: it may return before provisioning is completed, and the
            provision status can (must) be recovered via the *Last Operation*
            endpoint (see below).
          - *Bind*: for a given template, return "credentials" exposed in any
            created ConfigMap, Secret, Service or Route object (see
            ExposeAnnotationPrefix and Base64ExposeAnnotationPrefix
            documentation).  The *Bind* call records the fact that it took
            place in the appropriate **BrokerTemplateInstance** object.
          - *Unbind*: this simply removes the metadata previously placed in the
            **BrokerTemplateInstance** object by a *Bind* call.
          - *Deprovision*: removes the objects created by the *Provision* call.
            The garbage collector removes all additional objects created by the
            **TemplateInstance** controller, hopefully transitively, as
            documented above.
          - *Last Operation*: returns the status of the previously run
            asynchronous operation.  In the template service broker, *Provision*
            is the only asynchronous operation.
        displayName: OpenShift Template Service Broker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.2.18-202002031246
      name: 4.2-s390x
    - currentCSV: openshifttemplateservicebrokeroperator.4.3.2-202002112006
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"osb.openshift.io/v1","kind":"TemplateServiceBroker","metadata":{"name":"template-service-broker","namespace":"template-service-broker"},"spec":{}}]'
          capabilities: Seamless Upgrades
          containerImage: registry.redhat.io/openshift4/ose-template-service-broker@sha256:8bc0f9a32a3be7a2fef62b811a223bf95a43af3cc81ea50b744f3fa6d204c12a
          description: OpenShift Template Service Broker is an implementation of the
            [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          olm.skipRange: '>=4.2.0 <4.3.2-202002112006'
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Open Service Broker supporting management of OpenShift
              templates.
            displayName: Template Service Broker
            kind: TemplateServiceBroker
            name: templateservicebrokers.osb.openshift.io
            version: v1
        description: |
          The OpenShift Template Service Broker implements the [Open Service Broker
          API](https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md)
          endpoints:
          - *Catalog*: returns a list of available templates as OSB API
            *Service* objects (the templates are read from one or more
            namespaces configured in the master config).
          - *Provision*: provision a given template (referred by its UID) into a
            namespace.  Under the covers, this creates a non-namespaced
            **BrokerTemplateInstance** object for the template service broker to
            store state associated with the the instantiation, as well as the
            **Secret** and **TemplateInstance** objects which are picked up by
            the **TemplateInstance** controller.  *Provision* is an asynchronous
            operation: it may return before provisioning is completed, and the
            provision status can (must) be recovered via the *Last Operation*
            endpoint (see below).
          - *Bind*: for a given template, return "credentials" exposed in any
            created ConfigMap, Secret, Service or Route object (see
            ExposeAnnotationPrefix and Base64ExposeAnnotationPrefix
            documentation).  The *Bind* call records the fact that it took
            place in the appropriate **BrokerTemplateInstance** object.
          - *Unbind*: this simply removes the metadata previously placed in the
            **BrokerTemplateInstance** object by a *Bind* call.
          - *Deprovision*: removes the objects created by the *Provision* call.
            The garbage collector removes all additional objects created by the
            **TemplateInstance** controller, hopefully transitively, as
            documented above.
          - *Last Operation*: returns the status of the previously run
            asynchronous operation.  In the template service broker, *Provision*
            is the only asynchronous operation.
        displayName: OpenShift Template Service Broker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.3.2-202002112006
      name: "4.3"
    - currentCSV: openshifttemplateservicebrokeroperator.4.1.34-202002061038
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"osb.openshift.io/v1","kind":"TemplateServiceBroker","metadata":{"name":"template-service-broker","namespace":"template-service-broker"},"spec":{}}]'
          capabilities: Seamless Upgrades
          containerImage: registry.redhat.io/openshift4/ose-template-service-broker@sha256:1a871b57fe1e14ab1123558f4356a28603e0f49406ec4825c2e95e826fe1cd0c
          description: OpenShift Template Service Broker is an implementation of the
            [Open Service Broker API](https://github.com/openservicebrokerapi/servicebroker)
          olm.skipRange: '>=4.1.0 <4.1.34-202002061038'
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An Open Service Broker supporting management of OpenShift
              templates.
            displayName: Template Service Broker
            kind: TemplateServiceBroker
            name: templateservicebrokers.osb.openshift.io
            version: v1
        description: |
          The OpenShift Template Service Broker implements the [Open Service Broker
          API](https://github.com/openservicebrokerapi/servicebroker/blob/master/spec.md)
          endpoints:
          - *Catalog*: returns a list of available templates as OSB API
            *Service* objects (the templates are read from one or more
            namespaces configured in the master config).
          - *Provision*: provision a given template (referred by its UID) into a
            namespace.  Under the covers, this creates a non-namespaced
            **BrokerTemplateInstance** object for the template service broker to
            store state associated with the the instantiation, as well as the
            **Secret** and **TemplateInstance** objects which are picked up by
            the **TemplateInstance** controller.  *Provision* is an asynchronous
            operation: it may return before provisioning is completed, and the
            provision status can (must) be recovered via the *Last Operation*
            endpoint (see below).
          - *Bind*: for a given template, return "credentials" exposed in any
            created ConfigMap, Secret, Service or Route object (see
            ExposeAnnotationPrefix and Base64ExposeAnnotationPrefix
            documentation).  The *Bind* call records the fact that it took
            place in the appropriate **BrokerTemplateInstance** object.
          - *Unbind*: this simply removes the metadata previously placed in the
            **BrokerTemplateInstance** object by a *Bind* call.
          - *Deprovision*: removes the objects created by the *Provision* call.
            The garbage collector removes all additional objects created by the
            **TemplateInstance** controller, hopefully transitively, as
            documented above.
          - *Last Operation*: returns the status of the previously run
            asynchronous operation.  In the template service broker, *Provision*
            is the only asynchronous operation.
        displayName: OpenShift Template Service Broker Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 4.1.34-202002061038
      name: stable
    defaultChannel: "4.3"
    packageName: openshifttemplateservicebroker
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Lightbend, Inc.
      provider-url: ""
    name: lightbend-console-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/lightbend-console-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: lightbend-console-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "app.lightbend.com/v1alpha1",
              "kind": "Console",
              "metadata": {
                "name": "example-console",
                "namespace": "placeholder"
              },
              "spec": {
                "alertManagers": null,
                "alpineImage": "alpine",
                "alpineVersion": "3.8",
                "apiGroupVersion": "rbac.authorization.k8s.io",
                "busyboxImage": "busybox",
                "busyboxVersion": "1.30",
                "configMapReloadImage": "jimmidyson/configmap-reload",
                "configMapReloadVersion": "v0.2.2",
                "consoleAPI": {
                  "defaultMonitorWarmup": "1m",
                  "defaultMonitorsConfigMap": "console-api-default-monitors",
                  "staticRulesConfigMap": "console-api-static-rules"
                },
                "consoleUIConfig": {
                  "isMonitorEditEnabled": false,
                  "logo": ""
                },
                "daemonSetApiVersion": "apps/v1beta2",
                "defaultCPURequest": "100m",
                "defaultMemoryRequest": "50Mi",
                "deploymentApiVersion": "apps/v1beta2",
                "elasticsearchImage": "elasticsearch",
                "elasticsearchMemoryRequest": "510Mi",
                "elasticsearchVersion": "7.2.0",
                "enableElasticsearch": false,
                "esConsoleExposePort": 30080,
                "esConsoleImage": "{{.Values.imageCredentials.registry}}/enterprise-suite/es-console",
                "esConsoleVersion": "v1.2.6",
                "esGrafanaEnvVars": null,
                "esGrafanaImage": "{{.Values.imageCredentials.registry}}/enterprise-suite/es-grafana",
                "esGrafanaVersion": "v0.3.0",
                "esGrafanaVolumeSize": "32Gi",
                "esMonitorImage": "{{.Values.imageCredentials.registry}}/enterprise-suite/console-api",
                "esMonitorVersion": "v1.2.3",
                "exposeServices": false,
                "goDnsmasqImage": "lightbend-docker-registry.bintray.io/lightbend/go-dnsmasq",
                "goDnsmasqVersion": "v0.1.7-1",
                "imageCredentials": {
                  "registry": "lightbend-docker-commercial-registry.bintray.io",
                  "username": "setme",
                  "password": "setme"
                },
                "imagePullPolicy": "IfNotPresent",
                "kubeStateMetricsImage": "gcr.io/google_containers/kube-state-metrics",
                "kubeStateMetricsVersion": "v1.2.0",
                "minikube": false,
                "podUID": null,
                "prometheusDomain": "prometheus.io",
                "prometheusImage": "prom/prometheus",
                "prometheusMemoryRequest": "250Mi",
                "prometheusVersion": "v2.9.2",
                "prometheusVolumeSize": "256Gi",
                "rbacApiVersion": "rbac.authorization.k8s.io/v1",
                "usePersistentVolumes": true
              }
            }]
          capabilities: Basic Install
          categories: Monitoring
          certified: "false"
          containerImage: lightbend-docker-registry.bintray.io/lightbend/console-operator:1.2.3
          createdAt: "2019-09-18T00:00:00Z"
          description: Lightbend Console provides visualizations for Akka, Play, and
            Lagom applications. See https://developer.lightbend.com/docs/console/current/
            for details.
          repository: https://github.com/lightbend/console-charts/tree/master/operator
          support: Lightbend, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Console
            displayName: Console
            kind: Console
            name: consoles.app.lightbend.com
            version: v1alpha1
        description: |
          ## About the managed application
          Lightbend Console provides visualization and basic monitoring for Akka, Play, and Lagom applications. See https://developer.lightbend.com/docs/console/current/ for further details.
          ## About this Operator
          The operator provides a simple installation of Lightbend Console. See https://github.com/lightbend/console-charts/blob/master/operator/README.md for details.
          ## Prerequisites for enabling this Operator
          None are required to install the operator.
          To install an instance of Console, you will need to add at minimum your credentials to the Console custom resource.
        displayName: Lightbend Console Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Lightbend, Inc.
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: lightbend-console-operator
    provider:
      name: Lightbend, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Aqua Security, Inc.
      provider-url: ""
    name: aqua
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/aqua
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: aqua-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaCsp",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "platform": "openshift",
                    "requirements": true
                  },
                  "registry": {
                    "url": "registry.aquasec.com",
                    "username": "example@gmail.com",
                    "password": "",
                    "email": "example@gmail.com"
                  },
                  "database": {
                    "replicas": 1,
                    "service": "ClusterIP"
                  },
                  "gateway": {
                    "replicas": 1,
                    "service": "ClusterIP"
                  },
                  "server": {
                    "replicas": 1,
                    "service": "LoadBalancer"
                  },
                  "adminPassword": "Password1",
                  "licenseToken": null
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaDatabase",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5",
                    "platform": "openshift"
                  },
                  "deploy": {
                    "replicas": 1,
                    "service": "ClusterIP"
                  },
                  "diskSize": 10
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaEnforcer",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5"
                  },
                  "gateway": {
                    "host": "aqua-gateway-svc",
                    "port": 3622
                  },
                  "token": "token"
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaGateway",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5"
                  },
                  "common": {
                    "databaseSecret": {
                      "name": "aqua-aqua-db",
                      "key": "password"
                    }
                  },
                  "externalDb": {
                    "host": "aqua-db",
                    "port": 5432,
                    "username": "postgres"
                  },
                  "deploy": {
                    "replicas": 1,
                    "service": "ClusterIP"
                  }
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaScanner",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5"
                  },
                  "deploy": {
                    "replicas": 1
                  },
                  "login": {
                    "username": "administrator",
                    "password": "Password1",
                    "host": "http://aqua-server:8080"
                  }
                }
              },
              {
                "apiVersion": "operator.aquasec.com/v1alpha1",
                "kind": "AquaServer",
                "metadata": {
                  "name": "aqua"
                },
                "spec": {
                  "infra": {
                    "serviceAccount": "aqua-sa",
                    "version": "4.5"
                  },
                  "common": {
                    "databaseSecret": {
                      "name": "aqua-aqua-db",
                      "key": "password"
                    }
                  },
                  "externalDb": {
                    "host": "aqua-db",
                    "port": 5432,
                    "username": "postgres"
                  },
                  "deploy": {
                    "replicas": 1,
                    "service": "LoadBalancer"
                    },
                  "adminPassword": "Password1",
                  "licenseToken": null
                }
              }
            ]
          capabilities: Basic Install
          categories: Security
          certified: "false"
          containerImage: aquasec/aqua-operator:1.0.0
          createdAt: "2019-12-30T08:00:00Z"
          description: The Aqua Security Operator runs within a Openshift cluster
            and provides a means to deploy and manage Aqua Security cluster and components.
          repository: https://github.com/aquasecurity/aqua-operator
          support: Aqua Security, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Aqua Security CSP Deployment with Aqua Operator
            displayName: AquaCsp
            kind: AquaCsp
            name: aquacsps.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Database Deployment with Aqua Operator
            displayName: AquaDatabase
            kind: AquaDatabase
            name: aquadatabases.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Enforcer Deployment with Aqua Operator
            displayName: AquaEnforcer
            kind: AquaEnforcer
            name: aquaenforcers.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Gateway Deployment with Aqua Operator
            displayName: AquaGateway
            kind: AquaGateway
            name: aquagateways.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Scanner Deployment with Aqua Operator
            displayName: AquaScanner
            kind: AquaScanner
            name: aquascanners.operator.aquasec.com
            version: v1alpha1
          - description: Aqua Security Server Deployment with Aqua Operator
            displayName: AquaServer
            kind: AquaServer
            name: aquaservers.operator.aquasec.com
            version: v1alpha1
        description: "The Aqua Security Operator runs within an OpenShift cluster,
          and provides a means to deploy and manage the Aqua Security cluster and
          components\n* Server (sometimes called â€œconsoleâ€)\n* Database (not recommended
          for production environments)\n* Gateway\n* Enforcer (sometimes called â€œagentâ€)\n*
          Scanner\n* CSP (package containing the Server, Database, and Gateway - not
          supported, and not for production environments)\nUse the aqua-operator to
          \n* Deploy Aqua Security components on OpenShift\n* Scale up Aqua Security
          components with extra replicas\n* Assign metadata tags to Aqua Security
          components\n* Automatically scale the number of Aqua scanners according
          to the number of images in the scan queue\n## Before You Begin Using the
          Operator CRDs\nObtain access to the Aqua registry - https://www.aquasec.com/about-us/contact-us/\nYou
          need to create\n* A secret for the Docker registry\n* A secret for the database\n```bash\noc
          create secret docker-registry aqua-registry --docker-server=registry.aquasec.com
          --docker-username=<AQUA_USERNAME> --docker-password=<AQUA_PASSWORD> --docker-email=<user
          email> -n aqua\noc create secret generic aqua-database-password --from-literal=db-password=<password>
          -n aqua\noc secrets add aqua-sa aqua-registry --for=pull -n aqua\n```\n##
          After the Installation\nOnce the operator is installed in the cluster, you
          now can use the CRDs to install the Aqua cluster and components."
        displayName: Aqua Security Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Aqua Security, Inc.
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: aqua
    provider:
      name: Aqua Security, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: prometheus
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/prometheus
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: prometheusoperator.0.32.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"monitoring.coreos.com/v1","kind":"Prometheus","metadata":{"name":"example","labels":{"prometheus":"k8s"}},"spec":{"replicas":2,"serviceAccountName":"prometheus-k8s","securityContext":
            {}, "serviceMonitorSelector":{},"ruleSelector":{},"alerting":{"alertmanagers":[{"namespace":"openshift-monitoring","name":"alertmanager-main","port":"web"}]}}},{"apiVersion":"monitoring.coreos.com/v1","kind":"ServiceMonitor","metadata":{"name":"example","labels":{"k8s-app":"prometheus"}},"spec":{"selector":{"matchLabels":{"k8s-app":"prometheus"}},"endpoints":[{"port":"web","interval":"30s"}]}},{"apiVersion":"monitoring.coreos.com/v1","kind":"PodMonitor","metadata":{"name":"example","labels":{"k8s-app":"prometheus"}},"spec":{"selector":{"matchLabels":{"k8s-app":"prometheus"}},"podMetricsEndpoints":[{"port":"web","interval":"30s"}]}},{"apiVersion":"monitoring.coreos.com/v1","kind":"Alertmanager","metadata":{"name":"alertmanager-main"},"spec":{"replicas":3,
            "securityContext": {}}},{"apiVersion":"monitoring.coreos.com/v1","kind":"PrometheusRule","metadata":{"creationTimestamp":null,"labels":{"prometheus":"example","role":"alert-rules"},"name":"prometheus-example-rules"},"spec":{"groups":[{"name":"./example.rules","rules":[{"alert":"ExampleAlert","expr":"vector(1)"}]}]}}]'
          capabilities: Deep Insights
          categories: Monitoring
          certified: "false"
          containerImage: quay.io/coreos/prometheus-operator:v0.32.0
          createdAt: "2019-09-04 12:00:00"
          description: Manage the full lifecycle of configuring and managing Prometheus
            and Alertmanager servers.
          repository: https://github.com/coreos/prometheus-operator
          support: Frederic Branczyk
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A running Prometheus instance
            displayName: Prometheus
            kind: Prometheus
            name: prometheuses.monitoring.coreos.com
            version: v1
          - description: A Prometheus Rule configures groups of sequentially evaluated
              recording and alerting rules.
            displayName: Prometheus Rule
            kind: PrometheusRule
            name: prometheusrules.monitoring.coreos.com
            version: v1
          - description: Configures prometheus to monitor a particular k8s service
            displayName: Service Monitor
            kind: ServiceMonitor
            name: servicemonitors.monitoring.coreos.com
            version: v1
          - description: Configures prometheus to monitor a particular pod
            displayName: Pod Monitor
            kind: PodMonitor
            name: podmonitors.monitoring.coreos.com
            version: v1
          - description: Configures an Alertmanager for the namespace
            displayName: Alertmanager
            kind: Alertmanager
            name: alertmanagers.monitoring.coreos.com
            version: v1
        description: |
          The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and deployment and management of Prometheus instances.

          Once installed, the Prometheus Operator provides the following features:

          * **Create/Destroy**: Easily launch a Prometheus instance for your Kubernetes namespace, a specific application or team easily using the Operator.

          * **Simple Configuration**: Configure the fundamentals of Prometheus like versions, persistence, retention policies, and replicas from a native Kubernetes resource.

          * **Target Services via Labels**: Automatically generate monitoring target configurations based on familiar Kubernetes label queries; no need to learn a Prometheus specific configuration language.

          ### Other Supported Features

          **High availability**

          Multiple instances are run across failure zones and data is replicated. This keeps your monitoring available during an outage, when you need it most.

          **Updates via automated operations**

          New Prometheus versions are deployed using a rolling update with no downtime, making it easy to stay up to date.

          **Handles the dynamic nature of containers**

          Alerting rules are attached to groups of containers instead of individual instances, which is ideal for the highly dynamic nature of container deployment.
        displayName: Prometheus Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.32.0
      name: beta
    defaultChannel: beta
    packageName: prometheus
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Broadcom, Inc.
      provider-url: ""
    name: uma-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/uma-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: uma-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"ca.broadcom.com/v1alpha1","kind":"UniversalMonitoringAgent","metadata":{"name":"uma-monitor"},"spec":{"agentManager":{"credential":null,"url":"localhost:5001"},"clusterName":"DevelopmentCluster","imageName":"registry.connect.redhat.com/ca/universalmonitoragent:v20.1.0","monitor":{"application":{"java":{"enabled":true,"filterType":"whitelist","propertiesOverride":null},"jmx":{"enabled":true}},"clusterPerformance":{"enabled":true},"container":{"dockerstats":{"enabled":true},"prometheus":{"backend":{"enabled":false,"endPoint":{"configFiles":null,"metricAlias":"container_name=container,pod_name=pod","password":null,"token":null,"url":null,"username":null},"filter":{"name":null,"value":null}},"exporter":{"enabled":true}}}},"type":"Kubernetes"}}]'
          capabilities: Basic Install
          categories: Application Runtime
          certified: "false"
          containerImage: caapm/universalmonitoragent:uma-operator
          createdAt: "2019-10-15T15:23:00Z"
          description: uma for monitoring microservices
          repository: https://github.gwd.broadcom.net/BROADCOM/uma-operator
          support: Broadcom, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Universal Monitoring Agent operator is a microservice that
              will let you monitor kubernetes,openshift,docker
            displayName: Uma Operator
            kind: UniversalMonitoringAgent
            name: universalmonitoringagents.ca.broadcom.com
            version: v1alpha1
        description: |
          Universal Monitoring Agent operator is a microservice that
          will let you monitor kubernetes,openshift,docker
        displayName: Uma Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Broadcom, Inc.
        version: 1.0.0
      name: alpha
    defaultChannel: alpha
    packageName: uma-operator
    provider:
      name: Broadcom, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat, Inc.
      provider-url: ""
    name: smartgateway-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/smartgateway-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: smartgateway-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "smartgateway.infra.watch/v1alpha1",
                "kind": "SmartGateway",
                "metadata": {
                  "name": "metrics"
                  },
                  "spec":{
                    "amqp_url": "qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/telemetry",
                    "container_image_path": "quay.io/redhat-service-assurance/smart-gateway:latest",
                    "debug": "false",
                    "size": 1
                    }
              },
              {
                "apiVersion": "smartgateway.infra.watch/v1alpha1",
                "kind": "SmartGateway",
                "metadata": {
                  "name": "events"
                  },
                  "spec": {
                    "amqp_url": "qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/telemetry",
                    "container_image_path": "quay.io/redhat-service-assurance/smart-gateway:latest",
                    "debug": "false",
                    "elastic_url": "https://elasticsearch.sa-telemetry.svc:9200",
                    "service_type": "events",
                    "size": 1
                    }
              }
            ]
          capabilities: Basic Install
          categories: Monitoring
          certified: "false"
          containerImage: quay.io/redhat-service-assurance/smart-gateway-operator.0.1.0
          createdAt: "2019-09-09T08:00:00Z"
          description: Smart gateway for service assurance. Includes applications
            for both metrics and events gathering. Provides middleware that connects
            to an AMQP 1.0 message bus, pulling data off the bus and exposing it as
            a scrape target for Prometheus. Metrics are provided via the OPNFV Barometer
            project (collectd). Events are provided by the various event plugins for
            collectd, including connectivity, procevent and sysevent.
          repository: https://github.com/redhat-service-assurance/smart-gateway-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: SmartGateway Events/Metrics Deployment with the Operator
            displayName: SmartGateway
            kind: SmartGateway
            name: smartgateways.smartgateway.infra.watch
            version: v1alpha1
        description: |-
          The Smart Gateway is middleware for OpenShift that takes metrics and events
          data streams from an AMQP 1.x message bus, and provides a method to having
          that data stream stored within Prometheus or ElasticSearch.

          ### Core capabilities

          The Smart Gateway provides two modes:

          * metrics: provides an HTTP scrape endpoint for Prometheus
          * events: writes events directly to an ElasticSearch endpoint

          ### Operator features

          * **Two Service Types** - Supports the `metrics` and `events` service types which defines the type of Smart Gateway to be deployed

          * **Configuration** - Configuration of the Smart Gateway to allow for it to be connected to an AMQP 1.x bus and pointed at the appropriate storage mechanism based on service type.

          ### Before getting started

          1. Ensure an AMQP 1.x bus has been setup (such as AMQ Interconnect)
          2. If the Smart Gateway is running in events mode, be sure the ElasticSearch Operator and ElasticSearch instance have been pre-deployed.

          ### Example configuration

          **Events (ElasticSearch storage)**

              apiVersion: smartgateway.infra.watch/v1alpha1
              kind: SmartGateway
              metadata:
                name: cloud1-notify
                namespace: sa-telemetry
              spec:
                amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/notify
                container_image_path: 172.30.1.1:5000/sa-telemetry/smart-gateway:latest
                debug: "false"
                elastic_url: https://elasticsearch.sa-telemetry.svc:9200
                reset_index: "true"
                service_type: events
                size: 1
                tls_ca_cert: /config/certs/admin-ca
                tls_client_cert: /config/certs/admin-cert
                tls_client_key: /config/certs/admin-key
                use_tls: "true"

          **Metrics (Prometheus storage)**

              apiVersion: smartgateway.infra.watch/v1alpha1
              kind: SmartGateway
              metadata:
                name: cloud1-telemetry
                namespace: sa-telemetry
              spec:
                amqp_url: qdr-white.sa-telemetry.svc.cluster.local:5672/collectd/telemetry
                container_image_path: 172.30.1.1:5000/sa-telemetry/smart-gateway:latest
                debug: "false"
                service_type: metrics
                size: 1
        displayName: Smart Gateway Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat, Inc.
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: smartgateway-operator
    provider:
      name: Red Hat, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: CNCF
      provider-url: ""
    name: etcd
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/etcd
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: etcdoperator.v0.9.4-clusterwide
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "etcd.database.coreos.com/v1beta2",
                "kind": "EtcdCluster",
                "metadata": {
                  "name": "example",
                  "annotations": {
                    "etcd.database.coreos.com/scope": "clusterwide"
                  }
                },
                "spec": {
                  "size": 3,
                  "version": "3.2.13"
                }
              },
              {
                "apiVersion": "etcd.database.coreos.com/v1beta2",
                "kind": "EtcdRestore",
                "metadata": {
                  "name": "example-etcd-cluster-restore"
                },
                "spec": {
                  "etcdCluster": {
                    "name": "example-etcd-cluster"
                  },
                  "backupStorageType": "S3",
                  "s3": {
                    "path": "<full-s3-path>",
                    "awsSecret": "<aws-secret>"
                  }
                }
              },
              {
                "apiVersion": "etcd.database.coreos.com/v1beta2",
                "kind": "EtcdBackup",
                "metadata": {
                  "name": "example-etcd-cluster-backup"
                },
                "spec": {
                  "etcdEndpoints": ["<etcd-cluster-endpoints>"],
                  "storageType":"S3",
                  "s3": {
                    "path": "<full-s3-path>",
                    "awsSecret": "<aws-secret>"
                  }
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Database
          containerImage: quay.io/coreos/etcd-operator@sha256:66a37fd61a06a43969854ee6d3e21087a98b93838e284a6086b13917f96b0d9b
          createdAt: "2019-02-28 01:03:00"
          description: Create and maintain highly-available etcd clusters on Kubernetes
          repository: https://github.com/coreos/etcd-operator
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a cluster of etcd nodes.
            displayName: etcd Cluster
            kind: EtcdCluster
            name: etcdclusters.etcd.database.coreos.com
            version: v1beta2
          - description: Represents the intent to backup an etcd cluster.
            displayName: etcd Backup
            kind: EtcdBackup
            name: etcdbackups.etcd.database.coreos.com
            version: v1beta2
          - description: Represents the intent to restore an etcd cluster from a backup.
            displayName: etcd Restore
            kind: EtcdRestore
            name: etcdrestores.etcd.database.coreos.com
            version: v1beta2
        description: |
          The etcd Operater creates and maintains highly-available etcd clusters on Kubernetes, allowing engineers to easily deploy and manage etcd clusters for their applications.

          etcd is a distributed key value store that provides a reliable way to store data across a cluster of machines. Itâ€™s open-source and available on GitHub. etcd gracefully handles leader elections during network partitions and will tolerate machine failure, including the leader.


          ### Reading and writing to etcd

          Communicate with etcd though its command line utility `etcdctl` via port forwarding:

              $ kubectl --namespace default port-forward service/example-client 2379:2379
              $ etcdctl --endpoints http://127.0.0.1:2379 get /

          Or directly to the API using the automatically generated Kubernetes Service:

              $ etcdctl --endpoints http://example-client.default.svc:2379 get /

          Be sure to secure your etcd cluster (see Common Configurations) before exposing it outside of the namespace or cluster.


          ### Supported Features

          * **High availability** - Multiple instances of etcd are networked together and secured. Individual failures or networking issues are transparently handled to keep your cluster up and running.

          * **Automated updates** - Rolling out a new etcd version works like all Kubernetes rolling updates. Simply declare the desired version, and the etcd service starts a safe rolling update to the new version automatically.

          * **Backups included** - Create etcd backups and restore them through the etcd Operator.

          ### Common Configurations

          * **Configure TLS** - Specify [static TLS certs](https://github.com/coreos/etcd-operator/blob/master/doc/user/cluster_tls.md) as Kubernetes secrets.

          * **Set Node Selector and Affinity** - [Spread your etcd Pods](https://github.com/coreos/etcd-operator/blob/master/doc/user/spec_examples.md#three-member-cluster-with-node-selector-and-anti-affinity-across-nodes) across Nodes and availability zones.

          * **Set Resource Limits** - [Set the Kubernetes limit and request](https://github.com/coreos/etcd-operator/blob/master/doc/user/spec_examples.md#three-member-cluster-with-resource-requirement) values for your etcd Pods.

          * **Customize Storage** - [Set a custom StorageClass](https://github.com/coreos/etcd-operator/blob/master/doc/user/spec_examples.md#custom-persistentvolumeclaim-definition) that you would like to use.
        displayName: etcd
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: CNCF
        version: 0.9.4-clusterwide
      name: clusterwide-alpha
    - currentCSV: etcdoperator.v0.9.4
      currentCSVDesc:
        annotations:
          alm-examples: |
            [
              {
                "apiVersion": "etcd.database.coreos.com/v1beta2",
                "kind": "EtcdCluster",
                "metadata": {
                  "name": "example"
                },
                "spec": {
                  "size": 3,
                  "version": "3.2.13"
                }
              },
              {
                "apiVersion": "etcd.database.coreos.com/v1beta2",
                "kind": "EtcdRestore",
                "metadata": {
                  "name": "example-etcd-cluster-restore"
                },
                "spec": {
                  "etcdCluster": {
                    "name": "example-etcd-cluster"
                  },
                  "backupStorageType": "S3",
                  "s3": {
                    "path": "<full-s3-path>",
                    "awsSecret": "<aws-secret>"
                  }
                }
              },
              {
                "apiVersion": "etcd.database.coreos.com/v1beta2",
                "kind": "EtcdBackup",
                "metadata": {
                  "name": "example-etcd-cluster-backup"
                },
                "spec": {
                  "etcdEndpoints": ["<etcd-cluster-endpoints>"],
                  "storageType":"S3",
                  "s3": {
                    "path": "<full-s3-path>",
                    "awsSecret": "<aws-secret>"
                  }
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Database
          containerImage: quay.io/coreos/etcd-operator@sha256:66a37fd61a06a43969854ee6d3e21087a98b93838e284a6086b13917f96b0d9b
          createdAt: "2019-02-28 01:03:00"
          description: Create and maintain highly-available etcd clusters on Kubernetes
          repository: https://github.com/coreos/etcd-operator
          tectonic-visibility: ocs
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a cluster of etcd nodes.
            displayName: etcd Cluster
            kind: EtcdCluster
            name: etcdclusters.etcd.database.coreos.com
            version: v1beta2
          - description: Represents the intent to backup an etcd cluster.
            displayName: etcd Backup
            kind: EtcdBackup
            name: etcdbackups.etcd.database.coreos.com
            version: v1beta2
          - description: Represents the intent to restore an etcd cluster from a backup.
            displayName: etcd Restore
            kind: EtcdRestore
            name: etcdrestores.etcd.database.coreos.com
            version: v1beta2
        description: |
          The etcd Operater creates and maintains highly-available etcd clusters on Kubernetes, allowing engineers to easily deploy and manage etcd clusters for their applications.

          etcd is a distributed key value store that provides a reliable way to store data across a cluster of machines. Itâ€™s open-source and available on GitHub. etcd gracefully handles leader elections during network partitions and will tolerate machine failure, including the leader.


          ### Reading and writing to etcd

          Communicate with etcd though its command line utility `etcdctl` via port forwarding:

              $ kubectl --namespace default port-forward service/example-client 2379:2379
              $ etcdctl --endpoints http://127.0.0.1:2379 get /

          Or directly to the API using the automatically generated Kubernetes Service:

              $ etcdctl --endpoints http://example-client.default.svc:2379 get /

          Be sure to secure your etcd cluster (see Common Configurations) before exposing it outside of the namespace or cluster.


          ### Supported Features

          * **High availability** - Multiple instances of etcd are networked together and secured. Individual failures or networking issues are transparently handled to keep your cluster up and running.

          * **Automated updates** - Rolling out a new etcd version works like all Kubernetes rolling updates. Simply declare the desired version, and the etcd service starts a safe rolling update to the new version automatically.

          * **Backups included** - Create etcd backups and restore them through the etcd Operator.

          ### Common Configurations

          * **Configure TLS** - Specify [static TLS certs](https://github.com/coreos/etcd-operator/blob/master/doc/user/cluster_tls.md) as Kubernetes secrets.

          * **Set Node Selector and Affinity** - [Spread your etcd Pods](https://github.com/coreos/etcd-operator/blob/master/doc/user/spec_examples.md#three-member-cluster-with-node-selector-and-anti-affinity-across-nodes) across Nodes and availability zones.

          * **Set Resource Limits** - [Set the Kubernetes limit and request](https://github.com/coreos/etcd-operator/blob/master/doc/user/spec_examples.md#three-member-cluster-with-resource-requirement) values for your etcd Pods.

          * **Customize Storage** - [Set a custom StorageClass](https://github.com/coreos/etcd-operator/blob/master/doc/user/spec_examples.md#custom-persistentvolumeclaim-definition) that you would like to use.
        displayName: etcd
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: CNCF
        version: 0.9.4
      name: singlenamespace-alpha
    defaultChannel: singlenamespace-alpha
    packageName: etcd
    provider:
      name: CNCF
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: KEDA Community
      provider-url: ""
    name: keda
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/keda
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: keda.v1.2.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"keda.k8s.io/v1alpha1","kind":"KedaController","metadata":{"name":"keda","namespace":"keda"},"spec":{"watchNamespace":"","logLevel":"info","logLevelMetrics":"0"}},{"apiVersion":"keda.k8s.io/v1alpha1","kind":"TriggerAuthentication","metadata":{"name":"example-triggerauth","namespace":"example-namespace"},"spec":{"secretTargetRef":[{"key":"example-role-key","name":"example-secret-name","parameter":"exmaple-secret-parameter"}]}},{"apiVersion":"keda.k8s.io/v1alpha1","kind":"ScaledObject","metadata":{"name":"example-scaledobject","labels":{"deploymentName":"example-deployment"}},"spec":{"scaleTargetRef":{"deploymentName":"example-deployment","containerName":"example-container"},"pollingInterval":30,"cooldownPeriod":300,"minReplicaCount":0,"maxReplicaCount":100,"triggers":[{"type":"example-trigger","metadata":{"namespace":"examle-namespace"}}]}}]'
          capabilities: Basic Install
          categories: Cloud Provider
          certified: "false"
          containerImage: docker.io/kedacore/keda-olm-operator:1.2.0
          createdAt: "2020-02-06T00:00:00.000Z"
          description: Operator that provides KEDA, a Kubernetes-based event driver
            autoscaler
          repository: https://github.com/kedacore/keda-olm-operator
          support: KEDA Community
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: |
              Represents an installation of a particular version of KEDA Controller.
            displayName: KedaController
            kind: KedaController
            name: kedacontrollers.keda.k8s.io
            version: v1alpha1
          - description: Describes authentication parameters
            displayName: TriggerAuthentication
            kind: TriggerAuthentication
            name: triggerauthentications.keda.k8s.io
            version: v1alpha1
          - description: Defines how KEDA should scale your application and what the
              triggers are
            displayName: ScaledObject
            kind: ScaledObject
            name: scaledobjects.keda.k8s.io
            version: v1alpha1
        description: "## About the managed application\nKEDA is a Kubernetes-based
          event driven autoscaler.  KEDA can monitor event sources like Kafka, RabbitMQ,
          or cloud event sources and feed the metrics from those sources into the
          Kubernetes horizontal pod autoscaler.  With KEDA, you can have event driven
          and serverless scale of deployments within any Kubernetes cluster.\n## About
          this Operator\nThe KEDA Operator deploys and manages installation of KEDA
          Controller in the cluster. Install this operator and follow installation
          instructions on how to install KEDA in you cluster.\n### How to install
          KEDA in the cluster\nThe installation of KEDA is triggered by the creation
          of `KedaController` resource. \nOnly resource named `keda` in namespace
          `keda` will trigger the installation, reconfiguration or removal of the
          KEDA Controller resource.\n\nThere could be only one KEDA Controller in
          the cluster. \n\n"
        displayName: KEDA
        installModes:
        - supported: false
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: KEDA Community
        version: 1.2.0
      name: alpha
    defaultChannel: alpha
    packageName: keda
    provider:
      name: KEDA Community
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Couchbase
      provider-url: ""
    name: couchbase-enterprise-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/couchbase-enterprise-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: couchbase-operator.v1.2.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "couchbase.com/v1",
                "kind": "CouchbaseCluster",
                "metadata": {
                  "name": "cb-example"
                },
                "spec": {
                  "adminConsoleServiceType": "NodePort",
                  "exposedFeatureServiceType": "NodePort",
                  "adminConsoleServices": [
                    "data"
                  ],
                  "authSecret": "cb-example-auth",
                  "baseImage": "registry.connect.redhat.com/couchbase/server",
                  "buckets": [
                    {
                      "conflictResolution": "seqno",
                      "enableFlush": true,
                      "enableIndexReplica": false,
                      "evictionPolicy": "fullEviction",
                      "ioPriority": "high",
                      "memoryQuota": 128,
                      "name": "default",
                      "replicas": 1,
                      "type": "couchbase",
                      "compressionMode": "passive"
                    }
                  ],
                  "cluster": {
                    "analyticsServiceMemoryQuota": 1024,
                    "autoFailoverMaxCount": 3,
                    "autoFailoverOnDataDiskIssues": true,
                    "autoFailoverOnDataDiskIssuesTimePeriod": 120,
                    "autoFailoverServerGroup": false,
                    "autoFailoverTimeout": 120,
                    "dataServiceMemoryQuota": 256,
                    "eventingServiceMemoryQuota": 256,
                    "indexServiceMemoryQuota": 256,
                    "indexStorageSetting": "memory_optimized",
                    "searchServiceMemoryQuota": 256
                  },
                  "exposeAdminConsole": true,
                  "servers": [
                    {
                      "name": "all_services",
                      "services": [
                        "data",
                        "index",
                        "query",
                        "search",
                        "eventing",
                        "analytics"
                      ],
                      "size": 3
                    }
                  ],
                  "version": "6.0.1-1"
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Database
          certified: "true"
          containerImage: registry.connect.redhat.com/couchbase/operator:1.2.1-1
          createdAt: 2019/08/30
          description: The Couchbase Autonomous Operator allows users to easily deploy,
            manage, and maintain Couchbase deployments
          support: Couchbase, Inc
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Manages Couchbase clusters
            displayName: Couchbase Cluster
            kind: CouchbaseCluster
            name: couchbaseclusters.couchbase.com
            version: v1
        description: |
          The Couchbase Autonomous Operator allows users to easily deploy, manage, and maintain Couchbase deployments on OpenShift. By installing this integration you will be able to deply Couchbase Server clusters with a single command.

          ## Supported Features

          * **Automated cluster provisioning** - Deploying a Couchbase Cluster has never been easier. Fill out a Couchbase specific configuration and let the Couchbase Operator take care of provisioning nodes and setting up cluster to your exact specification.

          * **On-demand scalability** - Automatically scale your cluster up or down by changing a simple configuration parameter and let the Couchbase Operator handle provisioning of new nodes and joining them into the cluster.

          * **Auto-recovery** - Detect Couchbase node failures, rebalance out bad nodes, and bring the cluster back up to the desired capacity. Auto-recovery is completely automated so you can sleep easy through the night knowing that the Couchbase Operator will handle any failures.

          * **Geo-distribution** - Replicate your data between datacenters to move data closer to the users who consume it and protect against disaster scenarios where an entire datacenter becomes unavailable.

          * **Persistent storage** - Define persistent network-attached storage for each node in your cluster to allow pods to be recovered even if the node they were running on is no longer available.

          * **Rack/zone awareness** - Tell the Couchbase Operator about availability zones in your datacenter and let the operator take care of ensuring that nodes in your cluster are deployed equally across each zone.

          * **Supportability** - When things go wrong, use the cbopinfo tool provided with the Couchbase Operator to collect relevant data about your Couchbase deployment so that you can quickly address issues.

          * **Centralized configuration management** - Manage your configuration centrally with OpenShift. Updates to the configuration are watched by the Couchbase Operator and actions are taken to make the target cluster match the desired configuration.
          ## Required Parameters
          * `authSecret` - provide the name of a secret that contains two keys for the `username` and `password` of the super user ([documentation](https://docs.couchbase.com/operator/1.2/couchbase-cluster-config.html))

          ## About Couchbase Server

          Built on the most powerful NoSQL technology, Couchbase Server delivers unparalleled performance at scale, in any cloud. With features like memory-first architecture, geo-distributed deployments, and workload isolation, Couchbase Server excels at supporting mission-critical applications at scale while maintaining submillisecond latencies and 99.999% availability. Plus, with the most comprehensive SQL-compatible query language (N1QL), migrating from RDBMS to Couchbase Server is easy with ANSI joins.
        displayName: Couchbase Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Couchbase
        version: 1.2.1
      name: stable
    defaultChannel: stable
    packageName: couchbase-enterprise-certified
    provider:
      name: Couchbase
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: radanalytics.io
      provider-url: ""
    name: radanalytics-spark
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/radanalytics-spark
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: sparkoperator.v1.0.2
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "radanalytics.io/v1",
                "kind": "SparkCluster",
                "metadata": {
                  "name": "my-spark-cluster"
                },
                "spec": {
                  "worker": {
                    "instances": "2"
                  },
                  "master": {
                    "instances": "1"
                  }
                }
              },
              {
                "apiVersion": "radanalytics.io/v1",
                "kind": "SparkApplication",
                "metadata": {
                  "name": "my-spark-app"
                },
                "spec": {
                  "mainApplicationFile": "local:///opt/spark/examples/jars/spark-examples_2.11-2.3.0.jar",
                  "mainClass": "org.apache.spark.examples.SparkPi",
                  "driver": {
                    "cores": 0.2,
                    "coreLimit": "500m"
                  },
                  "executor": {
                    "instances": 2,
                    "cores": 1,
                    "coreLimit": "1000m"
                  }
                }
              },
              {
                "apiVersion": "radanalytics.io/v1",
                "kind": "SparkHistoryServer",
                "metadata": {
                    "name": "my-history-server"
                },
                "spec": {
                    "type": "remoteStorage",
                    "expose": true,
                    "logDirectory": "s3a://my-history-server/",
                    "updateInterval": 10,
                    "retainedApplications": 50,
                    "customImage": "quay.io/jkremser/openshift-spark:2.4.0-aws",
                    "sparkConfiguration": [
                      {
                          "name": "spark.hadoop.fs.s3a.impl",
                          "value": "org.apache.hadoop.fs.s3a.S3AFileSystem"
                      },
                      {
                          "name": "spark.hadoop.fs.s3a.access.key",
                          "value": "foo"
                      },
                      {
                          "name": "spark.hadoop.fs.s3a.secret.key",
                          "value": "bar"
                      },
                      {
                          "name": "spark.hadoop.fs.s3a.endpoint",
                          "value": "http://ceph-nano-0:8000"
                      }
                    ]
                }
              }
            ]
          capabilities: Seamless Upgrades
          categories: Big Data
          certified: "false"
          containerImage: quay.io/radanalyticsio/spark-operator:1.0.2
          createdAt: "2019-01-17 12:00:00"
          description: An operator for managing the Apache Spark clusters and intelligent
            applications that spawn those clusters.
          repository: https://github.com/radanalyticsio/spark-operator
          support: jkremser@redhat.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Apache Spark cluster
            displayName: Spark Cluster
            kind: SparkCluster
            name: sparkclusters.radanalytics.io
            version: v1
          - description: Apache Spark application
            displayName: Spark Application
            kind: SparkApplication
            name: sparkapplications.radanalytics.io
            version: v1
          - description: Server that keeps track of finished Spark jobs
            displayName: Spark History Server
            kind: SparkHistoryServer
            name: sparkhistoryservers.radanalytics.io
            version: v1
        description: |
          **Apache Spark** is a unified analytics engine for large-scale data processing. Using this operator you can deploy and manage Spark clusters that run in standalone mode. You can expose the metrics for Prometheus, prepare data for Spark workers or add custom Maven dependencies for your cluster. Operator also supports `SparkApplications` that share the same API with the GCP Spark operator. These applications spawn their own ad-hoc clusters using K8s as the native scheduler.

          Usage:
          ```
          # create cluster
          cat <<EOF | kubectl create -f -
          apiVersion: radanalytics.io/v1
          kind: SparkCluster
          metadata:
            name: my-spark-cluster
          spec:
            worker:
              instances: 2
          EOF
          ```

          For more advanced examples please consult [examples](https://github.com/radanalyticsio/spark-operator/tree/master/examples).
        displayName: Apache Spark Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: radanalytics.io
        version: 1.0.2
      name: alpha
    defaultChannel: alpha
    packageName: radanalytics-spark
    provider:
      name: radanalytics.io
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: strimzi-kafka-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/strimzi-kafka-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: strimzi-cluster-operator.v0.15.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                  "apiVersion":"kafka.strimzi.io/v1beta1",
                  "kind":"Kafka",
                  "metadata":{
                    "name":"my-cluster"
                  },
                  "spec":{
                    "kafka":{
                        "version":"2.3.1",
                        "replicas":3,
                        "listeners":{
                          "plain":{
                           },
                          "tls":{
                           }
                        },
                        "config":{
                          "offsets.topic.replication.factor":3,
                          "transaction.state.log.replication.factor":3,
                          "transaction.state.log.min.isr":2,
                          "log.message.format.version":"2.3"
                        },
                        "storage":{
                          "type":"ephemeral"
                        }
                    },
                    "zookeeper":{
                        "replicas":3,
                        "storage":{
                          "type":"ephemeral"
                        }
                    },
                    "entityOperator":{
                        "topicOperator":{
                         },
                        "userOperator":{
                         }
                    }
                  }
              },
              {
                  "apiVersion":"kafka.strimzi.io/v1beta1",
                  "kind":"KafkaConnect",
                  "metadata":{
                    "name":"my-connect-cluster"
                  },
                  "spec":{
                    "version":"2.3.1",
                    "replicas":1,
                    "bootstrapServers":"my-cluster-kafka-bootstrap:9093",
                    "tls":{
                        "trustedCertificates":[
                          {
                              "secretName":"my-cluster-cluster-ca-cert",
                              "certificate":"ca.crt"
                          }
                        ]
                    }
                  }
              },
              {
                  "apiVersion":"kafka.strimzi.io/v1beta1",
                  "kind":"KafkaConnectS2I",
                  "metadata":{
                    "name":"my-connect-cluster"
                  },
                  "spec":{
                    "version":"2.3.1",
                    "replicas":1,
                    "bootstrapServers":"my-cluster-kafka-bootstrap:9093",
                    "tls":{
                        "trustedCertificates":[
                          {
                              "secretName":"my-cluster-cluster-ca-cert",
                              "certificate":"ca.crt"
                          }
                        ]
                    }
                  }
              },
              {
                  "apiVersion":"kafka.strimzi.io/v1beta1",
                  "kind":"KafkaMirrorMaker",
                  "metadata":{
                    "name":"my-mirror-maker"
                  },
                  "spec":{
                    "version":"2.3.1",
                    "replicas":1,
                    "consumer":{
                        "bootstrapServers":"my-source-cluster-kafka-bootstrap:9092",
                        "groupId":"my-source-group-id"
                    },
                    "producer":{
                        "bootstrapServers":"my-target-cluster-kafka-bootstrap:9092"
                    },
                    "whitelist":".*"
                  }
              },
              {
                  "apiVersion":"kafka.strimzi.io/v1alpha1",
                  "kind":"KafkaBridge",
                  "metadata":{
                    "name":"my-bridge"
                  },
                  "spec":{
                    "replicas":1,
                    "bootstrapServers":"my-cluster-kafka-bootstrap:9092",
                    "http":{
                        "port":8080
                    }
                  }
              },
              {
                  "apiVersion":"kafka.strimzi.io/v1beta1",
                  "kind":"KafkaTopic",
                  "metadata":{
                    "name":"my-topic",
                    "labels":{
                        "strimzi.io/cluster":"my-cluster"
                    }
                  },
                  "spec":{
                    "partitions":10,
                    "replicas":3,
                    "config":{
                        "retention.ms":604800000,
                        "segment.bytes":1073741824
                    }
                  }
              },
              {
                  "apiVersion":"kafka.strimzi.io/v1beta1",
                  "kind":"KafkaUser",
                  "metadata":{
                    "name":"my-user",
                    "labels":{
                        "strimzi.io/cluster":"my-cluster"
                    }
                  },
                  "spec":{
                    "authentication":{
                        "type":"tls"
                    },
                    "authorization":{
                        "type":"simple",
                        "acls":[
                          {
                              "resource":{
                                "type":"topic",
                                "name":"my-topic",
                                "patternType":"literal"
                              },
                              "operation":"Read",
                              "host":"*"
                          },
                          {
                              "resource":{
                                "type":"topic",
                                "name":"my-topic",
                                "patternType":"literal"
                              },
                              "operation":"Describe",
                              "host":"*"
                          },
                          {
                              "resource":{
                                "type":"group",
                                "name":"my-group",
                                "patternType":"literal"
                              },
                              "operation":"Read",
                              "host":"*"
                          },
                          {
                              "resource":{
                                "type":"topic",
                                "name":"my-topic",
                                "patternType":"literal"
                              },
                              "operation":"Write",
                              "host":"*"
                          },
                          {
                              "resource":{
                                "type":"topic",
                                "name":"my-topic",
                                "patternType":"literal"
                              },
                              "operation":"Create",
                              "host":"*"
                          },
                          {
                              "resource":{
                                "type":"topic",
                                "name":"my-topic",
                                "patternType":"literal"
                              },
                              "operation":"Describe",
                              "host":"*"
                          }
                        ]
                    }
                  }
              }
            ]
          capabilities: Deep Insights
          categories: Streaming & Messaging
          certified: "false"
          containerImage: docker.io/strimzi/operator:0.15.0
          createdAt: "2019-10-08 11:00:00"
          description: Strimzi provides a way to run an Apache Kafka cluster on Kubernetes
            or OpenShift in various deployment configurations.
          repository: https://github.com/strimzi/strimzi-kafka-operator
          support: Strimzi
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Kafka cluster
            displayName: Kafka
            kind: Kafka
            name: kafkas.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Kafka Connect cluster
            displayName: Kafka Connect
            kind: KafkaConnect
            name: kafkaconnects.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Kafka Connect cluster with Source 2 Image support
            displayName: Kafka Connect Source to Image
            kind: KafkaConnectS2I
            name: kafkaconnects2is.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Kafka MirrorMaker cluster
            displayName: Kafka Mirror Maker
            kind: KafkaMirrorMaker
            name: kafkamirrormakers.kafka.strimzi.io
            version: v1beta1
          - description: Represents a Kafka Bridge cluster
            displayName: Kafka Bridge
            kind: KafkaBridge
            name: kafkabridges.kafka.strimzi.io
            version: v1alpha1
          - description: Represents a topic inside a Kafka cluster
            displayName: Kafka Topic
            kind: KafkaTopic
            name: kafkatopics.kafka.strimzi.io
            version: v1beta1
          - description: Represents a user inside a Kafka cluster
            displayName: Kafka User
            kind: KafkaUser
            name: kafkausers.kafka.strimzi.io
            version: v1beta1
        description: |
          Strimzi provides a way to run an [Apache KafkaÂ®](https://kafka.apache.org) cluster on  [Kubernetes](https://kubernetes.io/) or [OpenShift](https://www.openshift.com/) in various deployment configurations. See our [website](https://strimzi.io) for more details about the project.
          ### Supported Features
          * **Manages the Kafka Cluster** - Deploys and manages all of the components of this complex application, including dependencies like Apache ZooKeeperÂ® that are traditionally hard to administer.
          * **Includes Kafka Connect** - Allows for configuration of common data sources and sinks to move data into and out of the Kafka cluster.
          * **Topic Management** - Creates and manages Kafka Topics within the cluster.
          * **User Management** - Creates and manages Kafka Users within the cluster.
          * **Includes Kafka Mirror Maker** - Allows for morroring data between different Apache KafkaÂ® clusters.
          * **Includes HTTP Kafka Bridge** - Allows clients to send and receive messages through an Apache KafkaÂ® cluster via HTTP protocol.
          ### Upgrading your Clusters
          The Strimzi Operator understands how to run and upgrade between a set of Kafka versions. When specifying a new version in your config, check to make sure you aren't using any features that may have been removed. See [the upgrade guide](https://strimzi.io/docs/latest/#assembly-upgrading-kafka-versions-str) for more information.
          ### Storage
          An efficient data storage infrastructure is essential to the optimal performance of Apache KafkaÂ®. Apache KafkaÂ® deployed via Strimzi requires block storage. The use of file storage (for example, NFS) is not recommended.
          The Strimzi Operator supports three types of data storage:
          * Ephemeral (Recommended for development only)
          * Persistent
          * JBOD (Just a Bunch of Disks, suitable for Kafka only. Not supported in Zookeeper.)
          Strimzi also supports advanced operations such as adding or removing disks in Apache KafkaÂ® brokers or resizing the persistent volumes (where supported by the infrastructure).
          ### Documentation
          Documentation to the current _master_ branch as well as all releases can be found on our [website](https://strimzi.io/documentation).
          ### Getting help
          If you encounter any issues while using Strimzi, you can get help using:
          * [Strimzi mailing list on CNCF](https://lists.cncf.io/g/cncf-strimzi-users/topics)
          * [Strimzi Slack channel on CNCF workspace](https://cloud-native.slack.com/messages/strimzi)
          ### Contributing
          You can contribute by:
          * Raising any issues you find using Strimzi
          * Fixing issues by opening Pull Requests
          * Improving documentation
          * Talking about Strimzi

          All bugs, tasks or enhancements are tracked as [GitHub issues](https://github.com/strimzi/strimzi-kafka-operator/issues). Issues which  might be a good start for new contributors are marked with ["good-start"](https://github.com/strimzi/strimzi-kafka-operator/labels/good-start) label.

          The [Hacking guide](https://github.com/strimzi/strimzi-kafka-operator/blob/master/HACKING.md) describes how to build Strimzi and how to  test your changes before submitting a patch or opening a PR.

          The [Documentation Contributor Guide](https://strimzi.io/contributing/guide/) describes how to contribute to Strimzi documentation.

          If you want to get in touch with us first before contributing, you can use:
          * [Strimzi mailing list on CNCF](https://lists.cncf.io/g/cncf-strimzi-users/topics)
          * [Strimzi Slack channel on CNCF workspace](https://cloud-native.slack.com/messages/strimzi)
          ### License
          Strimzi is licensed under the [Apache License, Version 2.0](https://github.com/strimzi/strimzi-kafka-operator/blob/master/LICENSE).
        displayName: Strimzi
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.15.0
      name: stable
    defaultChannel: stable
    packageName: strimzi-kafka-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: IBM
      provider-url: ""
    name: ibm-spectrum-scale-csi
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ibm-spectrum-scale-csi
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ibm-spectrum-scale-csi-operator.v1.0.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "csi.ibm.com/v1",
                "kind": "CSIScaleOperator",
                "metadata": {
                  "labels": {
                    "app.kubernetes.io/instance": "ibm-spectrum-scale-csi-operator",
                    "app.kubernetes.io/managed-by": "ibm-spectrum-scale-csi-operator",
                    "app.kubernetes.io/name": "ibm-spectrum-scale-csi-operator"
                  },
                  "name": "ibm-spectrum-scale-csi",
                  "release": "ibm-spectrum-scale-csi-operator"
                },
                "spec": {
                  "clusters": [
                    {
                      "id": "\u003c Primary Cluster ID - WARNING: THIS IS A STRING NEEDS YAML QUOTES!\u003e",
                      "primary": {
                        "primaryFs": "\u003c Primary Filesystem \u003e",
                        "primaryFset": "\u003c Fileset in Primary Filesystem \u003e"
                      },
                      "restApi": [
                        {
                          "guiHost": "\u003c Primary cluster GUI IP/Hostname \u003e"
                        }
                      ],
                      "secrets": "secret1",
                      "secureSslMode": false
                    }
                  ],
                  "scaleHostpath": "\u003c GPFS FileSystem Path \u003e"
                },
                "status": {}
              }
            ]
          capabilities: Basic Install
          categories: Storage
          certified: "false"
          containerImage: registry.connect.redhat.com/ibm/ibm-spectrum-scale-csi-operator
          createdAt: Wed Dec  4 06:41:31 EST 2019
          description: An operator for deploying and managing the IBM CSI Spectrum
            Scale Driver.
          repository: https://github.com/IBM/ibm-spectrum-scale-csi-operator/
          support: IBM
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a deployment of the IBM CSI Spectrum Scale driver.
            displayName: IBM CSI Spectrum Scale Driver
            kind: CSIScaleOperator
            name: csiscaleoperators.csi.ibm.com
            version: v1
        description: "IBM Spectrum Scale CSI Operator Quickstart\n==========================================\n\nThe
          IBM Spectrum Scale CSI Operator runs within a Kubernetes cluster providing
          a means to \ndeploy and manage the CSI plugin for spectrum scale. For more
          in depth documentation please refer\nto the [README](https://github.com/IBM/ibm-spectrum-scale-csi-operator/blob/1.0.0/README.md).\n\nThis
          operator should be used to deploy the CSI plugin.\n\nThe configuration process
          is as follows:\n\n1. [Spectrum Scale GUI Setup](#spectrum-scale-gui-setup)\n2.
          [Custom Resource Configuration](#custom-resource-configuration)\n\nSpectrum
          Scale GUI Setup \n------------------------\n> **NOTE:** This step only needs
          to be preformed once per GUI.\n\n> **WARNING:** If your daemonset pods (driver
          pods) do not come up, generally this means you have a  secret that  has
          not been defined in the correct namespace.\n\n1. Ensure the Spectrum Scale
          GUI is running by pointing your browser to the IP hosting the GUI:\n\n    ![](https://user-images.githubusercontent.com/1195452/67230992-6d2d9700-f40c-11e9-96d5-3f0e5bcb2d9a.png)\n\n
          \   > If you do not see a login follow on screen instructions, or review
          the [GUI Documentation](https://www.ibm.com/support/knowledgecenter/en/STXKQY_5.0.3/com.ibm.spectrum.scale.v5r03.doc/bl1ins_quickrefforgui.htm)\n\n\n2.
          Create a CsiAdmin group account on in the GUI (currently requires a CLI
          call):\n\n   ```\n\n   export USERNAME=\"SomeUser\"\n   export PASSWORD=\"SomePassword\"\n
          \  /usr/lpp/mmfs/gui/cli/mkuser ${USERNAME} -p ${PASSWORD} -g CsiAdmin\n\n
          \  ```\n\n3. Create a Kubernetes secret for the `CsiAdmin` user:\n\n  ```\n\n
          \ export USERNAME_B64=$(echo $USERNAME | base64)\n  export PASSWORD_B64=$(echo
          $PASSWORD | base64)\n  export OPERATOR_NAMESPACE=\"ibm-spectrum-scale-csi-driver\"
          \ # Set this to the namespace you deploy the operator in.\n  \n\n  cat <<
          EOF > /tmp/csisecret.yaml\n  apiVersion: v1\n  data:\n    password: ${PASSWORD_B64}\n
          \   username: ${USERNAME_B64}\n  kind: Secret\n  type: Opaque\n  metadata:\n
          \   name: csisecret    # This should be in your CSIScaleOperator definition\n
          \   namespace: ${OPERATOR_NAMESPACE} \n    labels:\n      app.kubernetes.io/name:
          ibm-spectrum-scale-csi-operator # Used by the operator to detect changes,
          set on load of CR change if secret matches name in CR and namespace.\n  EOF\n
          \ \n\n  kubectl create -f /tmp/csisecret.yaml\n  rm -f /tmp/csisecret.yaml\n
          \ \n  ```\n\nCustom Resource Configuration\n-----------------------------\n\nThe
          bundled Custom Resource example represents the minimum settings needed to
          run the operator.\nIf your environment needs more advanced settings (e.g.
          remote clusters, node mapping, etc.) please\nrefer to the sample [Custom
          Resource](https://github.com/IBM/ibm-spectrum-scale-csi-operator/blob/1.0.0/stable/ibm-spectrum-scale-csi-operator-bundle/operators/ibm-spectrum-scale-csi-operator/deploy/crds/ibm-spectrum-scale-csi-operator-cr.yaml).\n\n\n"
        displayName: IBM Spectrum Scale CSI Plugin Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: IBM
        version: 1.0.0
      name: stable
    defaultChannel: stable
    packageName: ibm-spectrum-scale-csi
    provider:
      name: IBM
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Instana
      provider-url: ""
    name: instana-agent
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/instana-agent
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: instana-agent-operator.v0.0.5
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
             "apiVersion": "instana.io/v1alpha1",
             "kind": "InstanaAgent",
             "metadata": {
                "name": "instana-agent",
                "namespace": "instana-agent"
             },
             "spec": {
                "agent.zone.name": "my-k8s-cluster",
                "agent.key": "_PUT_YOUR_AGENT_KEY_HERE_",
                "agent.endpoint.host": "saas-us-west-2.instana.io",
                "agent.endpoint.port": 443,
                "config.files": {
                  "configuration.yaml": "# See https://docs.instana.io/quick_start/agent_setup/container/kubernetes/"
                }
             }
            }]
          capabilities: Basic Install
          categories: Monitoring,OpenShift Optional
          certified: "false"
          containerImage: instana/instana-agent-operator:0.0.5
          createdAt: "2019-04-23 14:17:43"
          description: Fully automated Application Performance Monitoring (APM) for
            microservices.
          repository: https://github.com/instana/instana-agent-operator
          support: Instana
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Instana Agent
            displayName: Instana Agent
            kind: InstanaAgent
            name: agents.instana.io
            version: v1alpha1
        description: |-
          # Kubernetes Operator for the Instana APM Agent

          Instana is a fully automated Application Performance Monitoring (APM) tool for microservices.

          This is the Kubernetes Operator for installing the [Instana APM Agent](https://www.instana.com) on Kubernetes or OpenShift.

          ## Configuration and Installation

          First, install this operator from [OperatorHub.io](https://operatorhub.io/), [OpenShift Container Platform](https://www.openshift.com/), or [OKD](https://www.okd.io/).

          Second, create the target namespace where the Instana agent should be installed. The agent does not need to run in the same namespace as the operator. Most users create a new namespace `instana-agent` for running the agents.

          Third, create a custom resource with the agent configuration in the target namespace. The operator will pick up the custom resource and install the Instana agent accordingly.

          The following is a minimal template of the custom resource:

          ```yaml
          apiVersion: instana.io/v1alpha1
          kind: InstanaAgent
          metadata:
            name: instana-agent
            namespace: instana-agent
          spec:
            agent.zone.name: '{{ (optional) name of the zone of the host }}'
            agent.key: '{{ put your Instana agent key here }}'
            agent.endpoint.host: '{{ the monitoring ingress endpoint }}'
            agent.endpoint.port: '{{ the monitoring ingress endpoint port, wrapped in quotes }}'
          config.files:
            configuration.yaml: |
              # You can leave this empty, or use this to configure your instana agent.
              # See https://docs.instana.io/quick_start/agent_setup/container/kubernetes/
          ```

          Save the template in a file `instana-agent.yaml` and edit the following values:

          * If your target namespace is not `instana-agent`, replace the `namespace:` accordingly.
          * `agent.key` must be set with your Instana agent key.
          * `agent.endpoint` must be set with the monitoring ingress endpoint, generally either `saas-us-west-2.instana.io` or `saas-eu-west-1.instana.io`.
          * `agent.endpoint.port` must be set with the monitoring ingress port, generally "443" (wrapped in quotes).
          * `agent.zone.name` should be set with the name of the Kubernetes cluster that is be displayed in Instana.

          For advanced configuration, you can edit the contents of the `configuration.yaml` file. View documentation here: [https://docs.instana.io/quick_start/agent_setup/container/kubernetes/](https://docs.instana.io/quick_start/agent_setup/container/kubernetes/).

          Apply the custom resource with `kubectl apply -f instana-agent.yaml`. After some time, you should see `instana-agent` Pods being created on each node of your cluster, and your cluster should show on the infrastructure map on your Instana Web interface.

          ## Uninstalling

          In order to uninstall the Instana agent, simply remove the custom resource with `kubectl delete -f instana-agent.yaml`.

          ## Source Code

          The Instana agent operator is an open source project hosted on [https://github.com/instana/instana-agent-operator](https://github.com/instana/instana-agent-operator/).
        displayName: Instana Agent Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Instana
        version: 0.0.5
      name: alpha
    defaultChannel: alpha
    packageName: instana-agent
    provider:
      name: Instana
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Citrix
      provider-url: ""
    name: cic-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cic-operator
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: cic-operator.v1.7.6
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "charts.helm.k8s.io/v1alpha1",
                "kind": "CitrixIngressController",
                "metadata": {
                  "name": "cic"
                },
                "spec": {
                  "cic": {
                    "image": "registry.connect.redhat.com/citrix/citrix-ingress-controller:latest",
                    "pullPolicy": "IfNotPresent"
                  },
                  "defaultSSLCert": null,
                  "exporter": {
                    "image": "registry.connect.redhat.com/citrix/citrix-adc-metrics-exporter:latest",
                    "ports": {
                      "containerPort": 8888
                    },
                    "pullPolicy": "IfNotPresent",
                    "required": false
                  },
                  "ingressClass": null,
                  "kubernetesURL": null,
                  "license": {
                    "accept": "no"
                  },
                  "logLevel": "DEBUG",
                  "loginFileName": null,
                  "nodeWatch": false,
                  "nsIP": "x.x.x.x",
                  "nsNamespace": null,
                  "nsPort": 443,
                  "nsProtocol": "HTTPS",
                  "nsVIP": null,
                  "openshift": true
                }
              }
            ]
          capabilities: Basic Install
          categories: Networking
          certified: "false"
          containerImage: registry.connect.redhat.com/citrix/citrix-k8s-ingress-controller:latest
          createdAt: "2020-02-06"
          description: Citrix provides an ingress controller for Citrix ADC MPX (hardware),
            Citrix ADC VPX (virtualized), and Citrix ADC CPX (containerized) for on-prem
            and cloud deployments. It automatically configures one or more Citrix
            ADC based on the Ingress resource configuration. This operator can be
            used deploy Citrix Ingress Controller in an Openshift environment.
          repository: https://github.com/citrix/citrix-k8s-ingress-controller
          support: Citrix Ingress Controller
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Deploys Citrix-Ingress-Controller in the Cluster.
            displayName: Citrix-Ingress-Controller
            kind: CitrixIngressController
            name: citrixingresscontrollers.charts.helm.k8s.io
            version: v1alpha1
        description: Citrix provides an ingress controller for Citrix ADC MPX (hardware),
          Citrix ADC VPX (virtualized), and Citrix ADC CPX (containerized) for on-prem
          and cloud deployments. It automatically configures one or more Citrix ADC
          based on the Ingress resource configuration. This operator can be used deploy
          Citrix Ingress Controller in an Openshift environment.
        displayName: Citrix-Ingress-Controller
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Citrix
        version: 1.7.6
      name: alpha
    defaultChannel: alpha
    packageName: cic-operator
    provider:
      name: Citrix
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Turbonomic, Inc.
      provider-url: ""
    name: t8c-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/t8c-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: t8c-operator.v7.17.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"charts.helm.k8s.io/v1alpha1","kind":"Xl","metadata":{"name":"xl-release"},"spec":{"global":{"repository":"turbonomic","tag":"7.17.1"}}}]'
          capabilities: Basic Install
          categories: Monitoring
          certified: "false"
          containerImage: registry.connect.redhat.com/turbonomic/t8c-operator:7.17
          createdAt: "2019-06-01T00:00:00.000Z"
          description: Turbonomic Workload Automation for Multicloud simultaneously
            optimizes performance, compliance, and cost in real-time. Workloads are
            precisely resourced, automatically, to perform while satisfying business
            constraints.
          repository: https://github.com/turbonomic/t8c-install/tree/master/operator
          support: Turbonomic, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Turbonomic Workload Automation for Multicloud simultaneously
              optimizes performance, compliance, and cost in real-time. Workloads
              are precisely resourced, automatically, to perform while satisfying
              business constraints.
            displayName: Turbonomic Platform Operator
            kind: Xl
            name: xls.charts.helm.k8s.io
            version: v1alpha1
        description: |-
          ### Realtime Decision Automation for Multicloud Applications
          Turbonomic Workload Automation for Multicloud simultaneously optimizes performance, compliance, and cost in real-time. Workloads are precisely resourced, automatically, to perform while satisfying business constraints:
          * Continuous placement of workload across multiple clouds both on-prem and public clouds providers.
          * Continuous scaling for applications and the underlying infrastructure.

          It assures application performance by giving workloads the resources they need when they need them.

          ### How does it work?
          Turbonomic uses a public APIs already exposed by application and infrastructure instrumentation to discover and monitor your environment.
          Turbonomic determines the right actions that drive continuous health, including continuous placement and continuous scaling for applications and the underlying cluster.
          Turbonomic leverages the built-on orchestration provided by the application and infrastructure deployment tools and automates the execution of these actions to continiously meet the respective service level objective of each application service.
        displayName: Turbonomic Platform Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Turbonomic, Inc.
        version: 7.17.0
      name: alpha
    defaultChannel: alpha
    packageName: t8c-certified
    provider:
      name: Turbonomic, Inc.
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: fuse-camel-k
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/fuse-camel-k
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: fuse-camel-k-operator.v7.5.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
              "apiVersion": "camel.apache.org/v1alpha1",
              "kind": "IntegrationPlatform",
              "metadata": {
                "name": "example"
              }
            },
            {
              "apiVersion": "camel.apache.org/v1alpha1",
              "kind": "Integration",
              "metadata": {
                "name": "example"
              },
              "spec": {
                "source": {
                  "content": "// Add example Java code to create Integration",
                  "name": "Example.java"
                }
              }
            },
            {
              "apiVersion": "camel.apache.org/v1alpha1",
              "kind": "IntegrationContext",
              "metadata": {
                "name": "example"
              }
            },
            {
              "apiVersion": "camel.apache.org/v1alpha1",
              "kind": "CamelCatalog",
              "metadata": {
                "name": "example"
              }
            },
            {
              "apiVersion": "camel.apache.org/v1alpha1",
              "kind": "Build",
              "metadata": {
                "name": "example"
              }
            }]
          capabilities: Basic Install
          categories: Integration & Delivery
          certified: "false"
          containerImage: apache/camel-k:0.3.4
          createdAt: "2019-10-15T16:22:00Z"
          description: FuseK is a lightweight integration platform.
          repository: https://github.com/apache/camel-k
          support: Camel
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A Camel K build
            displayName: Build
            kind: Build
            name: builds.camel.apache.org
            version: v1alpha1
          - description: A Camel catalog
            displayName: Camel Catalog
            kind: CamelCatalog
            name: camelcatalogs.camel.apache.org
            version: v1alpha1
          - description: A Camel K integration
            displayName: Integration
            kind: Integration
            name: integrations.camel.apache.org
            version: v1alpha1
          - description: A Camel K integration context
            displayName: Integration Context
            kind: IntegrationContext
            name: integrationcontexts.camel.apache.org
            version: v1alpha1
          - description: A Camel K integration platform
            displayName: Integration Platform
            kind: IntegrationPlatform
            name: integrationplatforms.camel.apache.org
            version: v1alpha1
        description: |
          FuseK
          ==============

          FuseK is a lightweight integration platform, that builds on Apache Camel K.

          ## Installation

          To start using FuseK, install the operator and then create the following `IntegrationPlatform`:
          ```
          apiVersion: camel.apache.org/v1alpha1
          kind: IntegrationPlatform
          metadata:
            name: camel-k
            labels:
              app: "camel-k"
          ```

          ## Running an Integration

          After the initial setup, you can run a Camel integration on the cluster by creating an example `Integration`:
          ```
          apiVersion: camel.apache.org/v1alpha1
          kind: Integration
          metadata:
            name: example
          spec:
            sources:
            - content: |
              import org.apache.camel.builder.RouteBuilder;

              public class Example extends RouteBuilder {
                  @Override
                  public void configure() throws Exception {
                      from("timer:tick")
                          .setBody(constant("Hello World!"))
                      .to("log:info?skipBodyLineSeparator=false");
                  }
              }
            name: Example.java
          ```
        displayName: FuseK Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 7.5.0
      name: alpha
    defaultChannel: alpha
    packageName: fuse-camel-k
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Snowdrop Team
      provider-url: ""
    name: halkyon
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/halkyon
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: halkyon.v0.1.9
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion" : "halkyon.io/v1beta1",
                "kind" : "Capability",
                "metadata" : {
                  "name" : "postgres-db"
                },
                "spec" : {
                  "category" : "database",
                  "kind" : "postgres",
                  "version" : "10",
                  "parameters" : [
                    {
                      "name" : "DB_USER",
                      "value" : "admin"
                    }, {
                      "name" : "DB_PASSWORD",
                      "value" : "admin"
                    }, {
                      "name" : "DB_NAME",
                      "value" : "sample-db"
                    }
                  ]
                }
              },
              {
                "apiVersion" : "halkyon.io/v1beta1",
                "kind" : "Component",
                "metadata" : {
                  "labels" : {
                    "app" : "fruit-backend-sb"
                  },
                  "name" : "fruit-backend-sb"
                },
                "spec" : {
                  "deploymentMode" : "build",
                  "runtime" : "spring-boot",
                  "version" : "2.1.6.RELEASE",
                  "exposeService" : true,
                  "buildConfig" : {
                    "type" : "s2i",
                    "url" : "https://github.com/halkyonio/operator.git",
                    "ref" : "master",
                    "contextPath" : "demo/",
                    "moduleDirName" : "fruit-backend-sb"
                  },
                  "port" : 8080
                }
              },
              {
                "apiVersion" : "halkyon.io/v1beta1",
                "kind" : "Link",
                "metadata" : {
                  "name" : "link-to-database"
                },
                "spec" : {
                  "componentName" : "fruit-backend-sb",
                  "kind" : "Secret",
                  "ref" : "postgres-db-config"
                }
              }
            ]
          capabilities: Basic Install
          categories: Developer Tools
          certified: "false"
          containerImage: quay.io/halkyonio/operator:v0.1.9
          createdAt: "2019-08-21"
          description: 'Halkyon: To get back to the halcyon days of local development
            in a modern kubernetes setting of microservices!'
          repository: https://github.com/halkyonio/operator
          support: Halkyon community
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A component describing your microservice
            displayName: Component
            kind: Component
            name: components.halkyon.io
            version: v1beta1
          - description: A capability or service to be deployed
            displayName: Capability
            kind: Capability
            name: capabilities.halkyon.io
            version: v1beta1
          - description: To link the microservices or access secrets
            displayName: Link
            kind: Link
            name: links.halkyon.io
            version: v1beta1
        description: |-
          ### Introduction

          Deploying modern micro-services applications that comply with the [12-factor](https://12factor.net/) guidelines to Kubernetes is difficult, mainly due to the host
          of different and complex Kubernetes Resources involved. In such scenarios developer experience becomes very important.

          This projects aims to tackle said complexity and vastly **simplify** the process of deploying micro-service applications to Kubernetes and get back to
          the halcyon days of local development! :sunglasses:

          By providing several, easy-to-use Kubernetes [Custom Resources (CRs)](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/) and
          an [Operator](https://enterprisersproject.com/article/2019/2/kubernetes-operators-plain-english) to handle them, the Halkyon project provides the following features:
          - Install micro-services (`components` in Halkyon's parlance) utilizing `runtimes` such as Spring Boot, Vert.x, Thorntail, Quarkus or Nodejs, serving as base building blocks for your application
          - Manage the relations between the different components using `link` CR allowing one micro-service for example to consume a REST endpoint provided by another
          - Deploy various infrastructure services like a database which are bound to a `component` via the `capability` CR.

          The Halkyon Operator requires `Kubernetes >= 1.13` or `OpenShift >= 3.11`.

          ### Prerequisite

          In order to use the Halkyon Operator and the CRs, it is mandatory to install [Tekton Pipelines](https://tekton.dev/) and [KubeDB](http://kubedb.com) Operators
          and their Custom Resources. Otherwise, it will fail to start or to be deployed from the Operatorhub UI screen of the console.
          See [prerequisite](https://github.com/halkyonio/operator/blob/master/README.md#prerequisites) section of the Halkyon documentation.

          ### How to use it

          Deploy the operator as defined within the [Operator Doc](https://github.com/halkyonio/operator#installing-the-halkyon-operator)
          or use the operator bundle promoted on [operatorhub.io](https://operatorhub.io/operator/halkyon).

          First create a namespace:
          ```
          kubectl create ns demo
          ```
          Next, create a component yml file with the following information within your maven java project:
          ```
          apiVersion: halkyon.io/v1beta1
          kind: Component
          metadata:
            name: spring-boot
          spec:
            runtime: spring-boot
            version: 2.1.6.RELEASE
            deploymentMode: dev
          ```
          Deploy it:
          ```
          kubectl apply -n demo -f my-component.yml
          ```
          Verify if the component has been deployed properly:
          ```
          kubectl get components -n demo
          NAME          RUNTIME       VERSION           AGE     MODE   STATUS   MESSAGE   REVISION
          spring-boot   spring-boot   2.1.6.RELEASE   2m19s      dev   Ready
          ```

          Package your Java Application "mvn package" and push the uber java file.
          ```
          kubectl cp target/my-component-1.0-SNAPSHOT.jar POD_NAME:/deployments/my-component-1.0-SNAPSHOT -n demo
          ```

          Start your application within the pod
          ```
          kubectl exec POD_NAME -n demo /var/lib/supervisord/bin/supervisord ctl start run-cmd
          ```

          Enrich your application with additional `Component`, `Link` them or deploy a `Capability` database using the supported CRs for your different microservices.

          To simplify your life even more when developing Java applications, add [Dekorate]( https://dekorate.io) to your project to automatically generate the YAML resources for your favorite runtime !

          ### A Real demo

          To play with a more [real-world example](https://github.com/halkyonio/operator/#key-concepts) and discover the different features currently supported, use the following [demo](https://github.com/halkyonio/operator/tree/master/demo)
          project. So jump there to see in action how Halkyon enhances the Developer Experience on Kubernetes ;-)

          ### More info

          [Custom Resources and their fields](https://github.com/halkyonio/api)

          [Project documentation - https://github.com/halkyonio/operator](https://github.com/halkyonio/operator#how-to-play-with-it)

          [Zulip Community](https://snowdrop.zulipchat.com/#narrow/stream/207165-halkyon)

          Follow us on [https://twitter.com/halkyonio](https://twitter.com/halkyonio)

          ### Contributing

          If you've got some great features, use cases for the Halkyon operator and project, open a ticket on [GitHub](https://github.com/halkyonio/operator/issues) !

          ### License

          Halkyon is licensed under the [Apache License, Version 2.0](https://github.com/halkyonio/operator/blob/master/LICENSE) license.
        displayName: Halkyon Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Snowdrop Team
        version: 0.1.9
      name: alpha
    defaultChannel: alpha
    packageName: halkyon
    provider:
      name: Snowdrop Team
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Syndesis team
      provider-url: ""
    name: syndesis
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/syndesis
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: syndesisoperator.1.7.0
      currentCSVDesc:
        annotations:
          alm-examples: "[{\n    \"apiVersion\": \"syndesis.io/v1alpha1\",\n    \"kind\":
            \"Syndesis\",\n    \"metadata\": {\n    \t\"name\": \"app\"\n    },\n
            \   \"spec\": {\n    \t\"integration\": {\n    \t\t\"limit\": 0\n    \t}\n
            \   }\n}]\n"
          capabilities: Deep Insights
          categories: Integration & Delivery
          certified: "false"
          containerImage: syndesis/syndesis-operator
          createdAt: "2019-05-08 16:12:00"
          description: Manages the installation of Syndesis, a flexible and customizable
            open source platform that provides core integration capabilities as a
            service.
          repository: https://github.com/syndesisio/syndesis/
          support: Syndesis
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Syndesis CRD
            displayName: Syndesis CRD
            kind: Syndesis
            name: syndesises.syndesis.io
            version: v1alpha1
        description: |
          Syndesis is a flexible and customizable, open source platform that provides core integration capabilities as a service.

          This operator installs as well as configures the following syndesis components:
          - syndesis-server
          - syndesis-meta
          - syndesis-ui
          - syndesis-db
          - syndesis-prometheus
          - syndesis-proxy
          - syndesis-oauthproxy

          ### How to install
          When the operator is installed (you have created a subscription and the operator is running in the selected namespace) create a new CR of Kind Syndesis (click the Create New button). The CR spec contains all defaults (see below).

          ### CR Defaults
          The CR definition is pretty simple and an empy CR will trigger a base installation
        displayName: Syndesis Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Syndesis team
        version: 1.7.0
      name: alpha
    defaultChannel: alpha
    packageName: syndesis
    provider:
      name: Syndesis team
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: MayaData
      provider-url: ""
    name: openebs
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/openebs
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: openebsoperator.v1.5.0
      currentCSVDesc:
        annotations:
          alm-examples: "[\n  {\n    \"apiVersion\": \"openebs.io/v1alpha1\",\n    \"kind\":
            \"OpenEBSInstallTemplate\",\n    \"metadata\": {\n      \"name\": \"oebs\",\n
            \     \"namespace\": \"openebs\"\n    },\n    \"spec\": {\n      \"rbac\":
            {\n        \"create\": \"true\"\n      },\n      \"serviceAccount\": {\n
            \       \"create\": \"true\",\n        \"name\": \"openebs-maya-operator\"\n
            \     },\n      \"release\": {\n        \"version\": \"1.5.0\"\n      },\n
            \     \"image\": {\n        \"pullPolicy\": \"IfNotPresent\"\n      },\n
            \     \"apiserver\": {\n        \"image\": \"quay.io/openebs/m-apiserver\",\n
            \       \"imageTag\": \"1.5.0\",\n        \"replicas\": \"1\",\n        \"ports\":
            {\n           \"externalPort\": \"5656\",\n           \"internalPort\":
            \"5656\"\n        },\n        \"sparse\": {\n          \"enabled\": \"false\"\n
            \       },\n        \"healthCheck\": {\n           \"initialDelaySeconds\":
            \"30\",\n           \"periodSeconds\": \"60\"\n        }\n      },\n      \"defaultStorageConfig\":
            {\n         \"enabled\": \"true\"\n       },\n      \"provisioner\": {\n
            \       \"image\": \"quay.io/openebs/openebs-k8s-provisioner\",\n        \"imageTag\":
            \"1.5.0\",\n        \"replicas\": \"1\",\n        \"healthCheck\": {\n
            \         \"initialDelaySeconds\": \"30\",\n          \"periodSeconds\":
            \"60\"\n        }\n      },\n      \"localprovisioner\": {\n        \"image\":
            \"quay.io/openebs/provisioner-localpv\",\n        \"imageTag\": \"1.5.0\",\n
            \       \"helperImage\": \"quay.io/openebs/linux-utils\",\n        \"helperImageTag\":
            \"1.5.0\",\n        \"replicas\": \"1\",\n        \"basePath\": \"/var/openebs/local\",\n
            \       \"healthCheck\": {\n          \"initialDelaySeconds\": \"30\",\n
            \         \"periodSeconds\": \"60\"\n        }\n      },\n      \"snapshotOperator\":
            {\n         \"controller\": {\n           \"image\": \"quay.io/openebs/snapshot-controller\",\n
            \          \"imageTag\": \"1.5.0\"\n         },\n         \"provisioner\":
            {\n           \"image\": \"quay.io/openebs/snapshot-provisioner\",\n           \"imageTag\":
            \"1.5.0\"\n         },\n         \"replicas\": \"1\",\n         \"upgradeStrategy\":
            \"Recreate\",\n         \"healthCheck\": {\n           \"initialDelaySeconds\":
            \"30\",\n           \"periodSeconds\": \"60\"\n         }\n      },\n
            \     \"ndm\": {\n        \"image\": \"quay.io/openebs/node-disk-manager-amd64\",\n
            \       \"imageTag\": \"v0.4.5\",\n        \"sparse\": {\n          \"path\":
            \"/var/openebs/sparse\",\n          \"size\": \"10737418240\",\n          \"count\":
            \"0\"\n        },\n        \"filters\": {\n          \"excludeVendors\":
            \"CLOUDBYT,OpenEBS\",\n          \"includePaths\": \"\",\n          \"excludePaths\":
            \"loop,fd0,sr0,/dev/ram,/dev/dm-,/dev/md\"\n        },\n        \"probes\":
            {\n           \"enableSeachest\": \"false\"\n        },\n        \"healthCheck\":
            {\n          \"initialDelaySeconds\": \"30\",\n          \"periodSeconds\":
            \"60\"\n        }\n      },\n      \"ndmOperator\": {\n        \"image\":
            \"quay.io/openebs/node-disk-operator-amd64\",\n        \"imageTag\": \"v0.4.5\",
            \n        \"replicas\": \"1\", \n        \"upgradeStrategy\": \"Recreate\",\n
            \       \"readinessCheck\": {\n          \"initialDelaySeconds\": \"4\",\n
            \         \"periodSeconds\": \"10\",\n          \"failureThreshold\":
            \"1\"\n        },\n        \"cleanupImage\": \"quay.io/openebs/linux-utils\",\n
            \       \"cleanupImageTag\": \"1.5.0\"\n      },\n      \"webhook\": {\n
            \       \"image\": \"quay.io/openebs/admission-server\",\n        \"imageTag\":
            \"1.5.0\",\n        \"generateTLS\": \"true\",\n        \"replicas\":
            \"1\"\n      },\n      \"jiva\": {\n        \"image\": \"quay.io/openebs/jiva\",\n
            \       \"imageTag\": \"1.5.0\",\n        \"replicas\": \"3\",\n        \"defaultStoragePath\":
            \"/var/openebs\"\n      },\n      \"cstor\": {\n        \"pool\": {\n
            \         \"image\": \"quay.io/openebs/cstor-pool\",\n          \"imageTag\":
            \"1.5.0\"\n        },\n        \"poolMgmt\": {\n          \"image\": \"quay.io/openebs/cstor-pool-mgmt\",\n
            \         \"imageTag\": \"1.5.0\"\n        },\n        \"target\": {\n
            \         \"image\": \"quay.io/openebs/cstor-istgt\",\n          \"imageTag\":
            \"1.5.0\"\n        },\n        \"volumeMgmt\": {\n          \"image\":
            \"quay.io/openebs/cstor-volume-mgmt\",\n          \"imageTag\": \"1.5.0\"\n
            \       }\n      },\n      \"policies\": {\n        \"monitoring\": {\n
            \         \"enabled\": \"true\",\n          \"image\": \"quay.io/openebs/m-exporter\",\n
            \         \"imageTag\": \"1.5.0\"\n        }\n      },\n      \"analytics\":
            {\n        \"enabled\": \"true\",\n        \"pingInterval\": \"24h\"\n
            \     }\n    }\n  }\n]\n"
          capabilities: Basic Install
          categories: Storage
          certified: "false"
          containerImage: index.docker.io/openebs/helm-operator:v0.0.5
          createdAt: "2020-01-11T15:52:12Z"
          description: Creates and maintains OpenEBS Control Plane deployments
          repository: https://github.com/openebs/helm-operator
          support: https://slack.openebs.io/
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a OpenEBS Install Operator
            displayName: OpenEBS Install Template
            kind: OpenEBSInstallTemplate
            name: openebsinstalltemplates.openebs.io
            version: v1alpha1
        description: "**OpenEBS** is a leading container attached storage solution
          that enables the use  of containers for mission-critical, persistent workloads
          and for other stateful  workloads such as logging or Prometheus for example.\nOpenEBS
          itself is deployed as just another container on your host and enables  storage
          services that can be designated on a per pod, application, cluster  or container
          level, including: \n* Data persistence across nodes\n* Synchronization of
          data across availability zones and cloud providers\n* A common layer whether
          you are running on the cloud, or your bare metal\n* Integration with Kubernetes,
          so developer and application intent flows into OpenEBS\n* Management of
          tiering to and from S3 and other targets.\n## OpenEBS Operator\nOpenEBS
          primarily provides container attached block storage (iSCSI volumes) by leveraging/aggregating
          the  storage on the nodes, with the storage controller itself running as
          a container. Different storage engines  (Jiva & cStor) are supported, with
          tools available to dynamically provision Kubernetes Local PVs.  The volumes
          are dynamically provisioned via PersistentVolumeClaims and are managed by
          a control plane component  called \"maya\", which also runs as a deployment
          in the K8s cluster. In addition to maya, a typical OpenEBS  installation
          comprises several other resources, which aid with various functionalities,
          ranging from snapshotting  to disk management. All these components are
          described briefly below: \n* **Maya-API-Server** - A storage orchestrator
          which integrates into Kubernetes workflow to help provision \n  and manage
          OpenEBS Jiva & cStor (storage engine) volumes\n\n* **Dynamic-OpenEBS-Provisioner**
          - A Kubernetes external storage provisioner that utilizes APIs exposed by
          maya-apiserver \n  to perform provision & delete operations of Jiva & cStor
          volumes\n\n* **Dynamic-LocalPV-Provisioner** - A dynamic provisioner for
          Kubernetes Local PVs\n* **OpenEBS-Snapshot-Operator** - A Kubernetes snapshot
          controller that creates & restores OpenEBS volume snapshots\n* **Node-Disk-Manager**
          - A disk management controller which identifies available disks, maintains
          inventory, and \n  dynamically attaches/detaches disks to backend storage
          pods \n\nThe helm-based OpenEBS Operator eases the setup of all the above
          mentioned components, with a simple custom resource  provided to define
          the install options, thereby enabling applications to start using the OpenEBS
          storageclasses in  their PVCs. The OpenEBSInstallTemplate CR can be used
          to specify start-up parameters & also update/overwrite the  definitions
          post install.\n## Pre-Requisites\nBefore installing OpenEBS control plane,
          perform the following steps to ensure successful deployment of the Node-Disk-Manager
          & the OpenEBS volume replicas, respectively. \n* Configure the OpenEBS service
          account on the namespace/project selected to use the privileged security
          context constraint.\n\n  **Note**: The serviceaccount name is same as the
          one specified in the `spec.serviceAccount.name` field of the OpenEBSInstallTemplate
          CR.\n\n\n  ```\n  oc adm policy add-scc-to-user privileged system:serviceaccount:<project>:<serviceaccountname>
          \n  ```\n\n* Configure the default service account on the namespace/project
          in which the volume replicas are deployed to use privileged \n  security
          context constraint. \n\n\n  ```\n  oc adm policy add-scc-to-user privileged
          system:serviceaccount:<project>:default \n  ```\n\n## Getting Started\n*
          Try the quickstart [guide](https://github.com/openebs/helm-operator/blob/master/olm/README.md)\n*
          To learn how to contribute, please read the [contribution guide](https://github.com/openebs/helm-operator/blob/master/CONTRIBUTING.md)\n*
          OpenEBS welcomes your feedback and contributions in any form possible. [Join
          our Community](https://openebs.org/community)\n\n## License\nOpenEBS is
          distributed under the  [Apache License, Version 2.0](http://www.apache.org/licenses/LICENSE-2.0.txt)\n"
        displayName: OpenEBS
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: MayaData
        version: 1.5.0
      name: alpha
    defaultChannel: alpha
    packageName: openebs
    provider:
      name: MayaData
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Containers & PaaS CoP
      provider-url: ""
    name: must-gather-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/must-gather-operator
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: must-gather-operator.v0.1.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "redhatcop.redhat.io/v1alpha1",
                "kind": "MustGather",
                "metadata": {
                  "name": "example-mustgather"
                },
                "spec": {
                  "caseID": "02527285",
                  "caseManagementAccountSecretRef": {
                    "name": "case-management-creds"
                  }
                }
              },
              {
                "apiVersion": "redhatcop.redhat.io/v1alpha1",
                "kind": "MustGather",
                "metadata": {
                  "name": "full-mustgather"
                },
                "spec": {
                  "caseID": "02527285",
                  "caseManagementAccountSecretRef": {
                    "name": "case-management-creds"
                  },
                  "serviceAccountRef": {
                    "name": "must-gather-admin"
                  },
                  "mustGatherImages": [
                    "quay.io/kubevirt/must-gather:latest",
                    "quay.io/ocs-dev/ocs-must-gather"
                  ]
                }
              }
            ]
          capabilities: Deep Insights
          categories: Security, Monitoring
          certified: "false"
          containerImage: quay.io/redhat-cop/must-gather-operator:latest
          createdAt: 11/14/2019
          description: This operator provides a facility to easily upload must-gather
            reports to a Red Hat case.
          repository: https://github.com/redhat-cop/must-gather-operator
          support: Best Effort
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: represents the request to collect must-gather information
              and upload it to the specified case
            displayName: Must Gather
            kind: MustGather
            name: mustgathers.redhatcop.redhat.io
            version: v1alpha1
        description: |-
          The must gather operator helps collecting must gather information on a cluster and uploading it to a case.
          To use the operator a cluster administrator can create the following must gather CR:

          ```yaml
          apiVersion: redhatcop.redhat.io/v1alpha1
          kind: MustGather
          metadata:
            name: example-mustgather
          spec:
            caseID: '02527285'
            caseManagementAccountSecretRef:
              name: case-management-creds
          ```

          this request will collect the standard must gather info and upload it to case `#02527285` using the credentials found in the `caseManagementCreds` secret.

          A more complex example:

          ```yaml
          apiVersion: redhatcop.redhat.io/v1alpha1
          kind: MustGather
          metadata:
            name: full-mustgather
          spec:
            caseID: '02527285'
            caseManagementAccountSecretRef:
              name: case-management-creds
            serviceAccountRef:
              name: must-gather-admin
            mustGatherImages:
            - quay.io/kubevirt/must-gather:latest
            - quay.io/ocs-dev/ocs-must-gather
          ```

          in this example we are using a specific service account (which must have cluster admin permissions as per must-gather requirements) and we are specifying a couple of additional must gather images to be run for the `kubevirt` and `ocs` subsystem. If not specified serviceAccountRef.Name will default to `default`. Also the standard must gather image: `quay.io/openshift/origin-must-gather:latest` is always added by default.
        displayName: Must Gather Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Containers & PaaS CoP
        version: 0.1.1
      name: alpha
    defaultChannel: alpha
    packageName: must-gather-operator
    provider:
      name: Containers & PaaS CoP
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Helm Community
      provider-url: ""
    name: cockroachdb-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cockroachdb-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: cockroachdb.v19.2.2
      currentCSVDesc:
        annotations:
          alm-examples: '[{ "apiVersion": "charts.helm.k8s.io/v1alpha1", "kind": "Cockroachdb",
            "metadata": { "name": "example" }, "spec": { "Name": "cdb", "Image": "registry.connect.redhat.com/cockroachdb/cockroach",
            "ImageTag": "19.2.2-ubi", "ImagePullPolicy": "Always", "Replicas": 3,
            "MaxUnavailable": 1, "Component": "cockroachdb", "InternalGrpcPort": 26257,
            "ExternalGrpcPort": 26257, "InternalGrpcName": "grpc", "ExternalGrpcName":
            "grpc", "InternalHttpPort": 8080, "ExternalHttpPort": 8080, "HttpName":
            "http", "Resources": { "requests": { "cpu": "100m", "memory": "1024Mi"
            } }, "InitPodResources": { }, "Storage": "10Gi", "StorageClass": null,
            "CacheSize": "256Mi", "MaxSQLMemory": "256Mi", "ClusterDomain": "cluster.local",
            "NetworkPolicy": { "Enabled": false, "AllowExternal": true }, "Service":
            { "type": "ClusterIP", "annotations": { } }, "PodManagementPolicy": "Parallel",
            "UpdateStrategy": { "type": "RollingUpdate" }, "NodeSelector": { }, "Tolerations":
            { }, "Secure": { "Enabled": false, "RequestCertsImage": "registry.connect.redhat.com/cockroachdb/request-cert",
            "RequestCertsImageTag": "0.4-2-ubi", "ServiceAccount": { "Create": true
            } } } }]'
          capabilities: Basic Install
          categories: Database
          certified: "false"
          containerImage: registry.connect.redhat.com/cockroachdb/cockroachdb-operator:v19.2.2-0.1.28
          createdAt: 2019-01-24T15-33-43Z
          description: CockroachDB Operator based on the CockroachDB helm chart
          repository: https://github.com/keith-mcclellan/crdb-helm-operator
          support: a-robinson
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a CockroachDB cluster
            displayName: CockroachDB
            kind: Cockroachdb
            name: cockroachdbs.charts.helm.k8s.io
            version: v1alpha1
        description: |
          CockroachDB is a scalable, survivable, strongly-consistent SQL database.

          ## About this Operator

          This Operator is based on a Helm chart for CockroachDB. It supports reconfiguration for some parameters, but notably does not handle scale down of the replica count in a seamless manner. Scale up works great.

          ## Core capabilities
          * **StatefulSet** - Sets up a dynamically scalable CockroachDB cluster using a Kubernetes StatefulSet
          * **Expand Replicas** - Supports expanding the set of replicas by simply editing your object
          * **Dashboard** - Installs the CockroachDB user interface to administer your cluster. Easily expose it via an Ingress rule.

          Review all of the [confiuguration options](https://github.com/helm/charts/tree/master/stable/cockroachdb#configuration) to best run your database instance. The example configuration is derived from the chart's [`values.yaml`](https://github.com/helm/charts/blob/master/stable/cockroachdb/values.yaml).

          ## Using the cluster

          The resulting cluster endpoint can be consumed from a `Service` that follows the pattern: `<StatefulSet-name>-public`. For example to connect using the command line client, use something like the following to obtain the name of the service:

          ```
          kubectl get service -l chart=cockroachdb-2.0.11
          NAME                                           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)              AGE
          example-9f8ngwzrxbxrulxqmdqfhn51h-cdb          ClusterIP   None             <none>        26257/TCP,8080/TCP   24m
          example-9f8ngwzrxbxrulxqmdqfhn51h-cdb-public   ClusterIP   10.106.249.134   <none>        26257/TCP,8080/TCP   24m
          ```

          Then you can use the CockroachDB command line client to connect to the database cluster:

          ```
          kubectl run -it --rm cockroach-client --image=cockroachdb/cockroach --restart=Never --command -- ./cockroach sql --insecure --host example-9f8ngwzrxbxrulxqmdqfhn51h-cdb-public
          ```

          ## Before you start

          This Operator requires a cluster with PV support in order to run correctly.
        displayName: CockroachDB
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Helm Community
        version: 19.2.2
      name: stable
    defaultChannel: stable
    packageName: cockroachdb-certified
    provider:
      name: Helm Community
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat Performance
      provider-url: ""
    name: ripsaw
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ripsaw
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ripsaw.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [{
            "apiVersion": "ripsaw.cloudbulldozer.io/v1alpha1",
            "kind": "Benchmark",
            "metadata": {
            "name": "example-benchmark",
            "namespace": "my-ripsaw"
            },
            "spec": {
            "cleanup": false,
            "workload": {
               "name": "uperf",
               "args": {
                  "hostnetwork": false,
                  "serviceip": false,
                  "pin": false,
                  "pin_server": "node-0",
                  "pin_client": "node-1",
                  "samples": 2,
                  "pair": 1,
                  "test_types": [
                     "stream",
                     "rr"
                  ],
                  "protos": [
                     "tcp",
                     "udp"
                  ],
                  "sizes": [
                     16384,
                     512
                  ],
                  "runtime": 10
               }
            }
            }
            }]
          capabilities: Seamless Upgrades
          categories: Developer Tools
          certified: "false"
          containerImage: quay.io/repository/benchmark-operator/benchmark-operator
          createdAt: "2019-06-28 06:42:43"
          description: Ripsaw is a benchmark operator to benchmark k8s and certain
            applications.
          repository: https://github.com/cloud-bulldozer/ripsaw/
          support: Red Hat Performance
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: The type of benchmark for Ripsaw to be run
            displayName: Benchmark
            kind: Benchmark
            name: benchmarks.ripsaw.cloudbulldozer.io
            version: v1alpha1
        description: |
          [Ripsaw](https://github.com/cloud-bulldozer/ripsaw/) is a benchmarking tool to benchmark your kubernetes cluster. Ripsaw fully supports running following:

          * Micro-benchmarks:
              * [fio](https://github.com/cloud-bulldozer/ripsaw/tree/master/docs/fio_distributed.md)
              * [uperf](https://github.com/cloud-bulldozer/ripsaw/tree/master/docs/uperf.md)

          * Application-benchmarks:
              * [ycsb](https://github.com/cloud-bulldozer/ripsaw/tree/master/docs/ycsb.md)
              * [pgbench](https://github.com/cloud-bulldozer/ripsaw/tree/master/docs/pgbench.md)

          You can also run following benchmarks through Ripsaw, but it'll be missing certain functionalities such as indexing to Elasticsearch, or running multiple samples:
          * [iperf3](https://github.com/cloud-bulldozer/ripsaw/tree/master/docs/iperf.md)
          * [sysbench](https://github.com/cloud-bulldozer/ripsaw/blob/master/docs/sysbench.md)
          * [byowl](https://github.com/cloud-bulldozer/ripsaw/blob/master/docs/byowl.md)

          You can use Ripsaw for:
          * Getting benchmark data from your current cluster classification
          * Getting an estimate of expected network/ storage performance data
          * Making a performance driven decision from data Ripsaw provides, on choices such as:
              * Which distribution to use
              * Which platform to deploy the k8s cluster on
              * Which storage class to provision storage volumes

          Steps to follow to install ripsaw:
          * Create a project/namespace called `my-ripsaw`
          * Install ripsaw into the `my-ripsaw` project/namespace
          * Read through documentation above for a CR of Benchmark to apply
          * Get the results
        displayName: Ripsaw
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat Performance
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: ripsaw
    provider:
      name: Red Hat Performance
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Red Hat
      provider-url: ""
    name: kubefed
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/kubefed
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: kubefed-operator.v0.1.0
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "operator.kubefed.io/v1alpha1",
                "kind": "KubeFed",
                "metadata": {
                  "name": "kubefed-resource"
                  },
                "spec": {
                  "scope": "Cluster"
                  }
              },
              {
                "apiVersion": "operator.kubefed.io/v1alpha1",
                "kind": "KubeFedWebHook",
                "metadata": {
                  "name": "kubefedwebhook-resource"
                  },
                "spec": {}
              }
            ]
          capabilities: Basic Install
          categories: OpenShift Optional, Integration & Delivery
          certified: "false"
          containerImage: quay.io/openshift/kubefed-operator:v0.1.0-rc6
          createdAt: "2019-06-14T00:00:00Z"
          description: Gain Hybrid Cloud capabilities between your clusters with Kubernetes
            Federation.
          repository: https://github.com/openshift/kubefed-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: ClusterPropagatedVersion holds version information about
              the state propagated from kubefed APIs
            displayName: ClusterPropagatedVersion
            kind: ClusterPropagatedVersion
            name: clusterpropagatedversions.core.kubefed.io
            version: v1alpha1
          - description: DNSEndpoint is the CRD wrapper for Endpoint which is designed
              to act as a source of truth for external-dns.
            displayName: DNSEndpoint
            kind: DNSEndpoint
            name: dnsendpoints.multiclusterdns.kubefed.io
            version: v1alpha1
          - description: Domain is the DNS zone associated with the kubefed control
              plane
            displayName: Domain
            kind: Domain
            name: domains.multiclusterdns.kubefed.io
            version: v1alpha1
          - description: KubeFedCluster configures kubefed to be aware of a Kubernetes
              cluster and encapsulates the details necessary to communicate with the
              cluster.
            displayName: KubeFedCluster
            kind: KubeFedCluster
            name: kubefedclusters.core.kubefed.io
            version: v1beta1
          - description: FederatedServiceStatus is the observed status of the resource
              for a named cluster.
            displayName: FederatedServiceStatus
            kind: FederatedServiceStatus
            name: federatedservicestatuses.core.kubefed.io
            version: v1alpha1
          - description: FederatedTypeConfig programs kubefed to know about a single
              API type - the "target type" - that a user wants to federate.
            displayName: FederatedTypeConfig
            kind: FederatedTypeConfig
            name: federatedtypeconfigs.core.kubefed.io
            version: v1beta1
          - description: KubeFedConfig
            displayName: KubeFedConfig
            kind: KubeFedConfig
            name: kubefedconfigs.core.kubefed.io
            version: v1beta1
          - description: IngressDNSRecord associates one or more Kubernetes Ingress
              and how to access the Kubernetes Ingress resources, with a scheme for
              constructing Domain Name System (DNS) resource records for the Ingress.
            displayName: IngressDNSRecord
            kind: IngressDNSRecord
            name: ingressdnsrecords.multiclusterdns.kubefed.io
            version: v1alpha1
          - description: PropagatedVersion
            displayName: PropagatedVersion
            kind: PropagatedVersion
            name: propagatedversions.core.kubefed.io
            version: v1alpha1
          - description: ReplicaSchedulingPreference provides an automated mechanism
              of distributing and maintaining total number of replicas for deployment
              or replicaset based federated workloads into federated clusters.
            displayName: ReplicaSchedulingPreference
            kind: ReplicaSchedulingPreference
            name: replicaschedulingpreferences.scheduling.kubefed.io
            version: v1alpha1
          - description: ServiceDNSRecord associates one or more Kubernetes Service
              resources and how to access the Service, with a scheme for constructing
              Domain Name System (DNS) resource records for the Service.
            displayName: ServiceDNSRecord
            kind: ServiceDNSRecord
            name: servicednsrecords.multiclusterdns.kubefed.io
            version: v1alpha1
          - description: KubeFed represents an installation of a particular version
              of KubeFed
            displayName: KubeFed
            kind: KubeFed
            name: kubefeds.operator.kubefed.io
            version: v1alpha1
          - description: KubeFedWebHook represents an installation of a particular
              version of KubeFed Webhook
            displayName: KubeFedWebHook
            kind: KubeFedWebHook
            name: kubefedwebhooks.operator.kubefed.io
            version: v1alpha1
        description: "Kubernetes Federation is a tool to sync (aka \"federate\") a
          set of Kubernetes\nobjects from a \"source\" into a set of other clusters.
          Common use-cases\ninclude federating Namespaces across all of your clusters
          or rolling out an\napplication across several geographically distributed
          clusters. The\nKubernetes Federation Operator runs all of the components
          under the hood to\nquickly get up and running with this powerful concept.
          Federation is a key\npart of any Hybrid Cloud capability.\n\n**Support**:
          \n\nThe KubeFed operator is purely for prototyping purposes and not\nsupported
          by any upstream community at this time. Note that at the moment uninstalling
          the operator doesn't delete the custom resources it reconciles. Users may
          optionally want to delete the custom resources and the associated CRDs before
          reinstalling the upgraded version of the operator.\n\n\n## Start using the
          operator\n\nThe KubeFed operator supports deploying a KubeFed control plane
          in both\nnamespace-scoped and cluster-scoped modes. For example, if you
          subscribe to\nthis package in a namespace `my-namespace`, KubeFed will be
          deployed to\nmanage objects _only_ in `my-namespace` in the host cluster
          and target\nclusters.\n\nYou must create the `KubeFedWebHook` and `KubeFed`
          custom resources (CRs) for KubeFed deployment. \nThe operator will create
          the following CRDs automatically for deploying components of a `KubeFed`
          control plane in a given cluster:\n* KubeFedWebHook\n* KubeFed  \n\n###
          KubeFed deployment\nFollow these steps to deploy a KubeFed control plane:\n\n*
          Install the operator by following the instructions in the\nOpenShift Container
          Platform 4.1 documentation on [Installing operator](https://docs.openshift.com/container-platform/4.1/applications/operators/olm-adding-operators-to-cluster.html#olm-installing-operators-from-operatorhub_olm-adding-operators-to-a-cluster).\nThe
          KubeFed operator only needs to be installed in the host cluster for the\ndeployment.
          It does not need to be installed in each individual target cluster.\n\n**Important
          Notes**: \n\n* It's recommended to deploy KubeFed in either a namespace-scoped
          or\ncluster-scoped mode but not both ways as deploying a cluster-scoped
          and\nnamespace-scoped KubeFed control plane to the same cluster will result
          in\nboth control planes try to manage resources in the namespace of the\nnamespace-scoped
          control plane and the managed resources will never converge\non the desired
          state. For more information, visit\n[link](https://github.com/openshift/federation-dev/blob/master/docs/kubefed-scope.md)\n\n*
          A KubeFed control plane does not need to be installed on each cluster that\nit
          will manage. Clusters whose resources are intended to be managed by\nKubeFed
          need to be registered with the managing control plane, and the\ncontrol
          plane will interact with those clusters directly via their API.\n\n* The
          instructions in this guide refer to the namespace where KubeFed is\ninstalled
          as `<namespace>` in command examples. The upstream user guide uses\n`kube-federation-system`
          as the name of this namespace.\n\n```\n$ oc get po -n <namespace>\n  NAME
          \                              READY   STATUS    RESTARTS   AGE\n  kubefed-operator-db58d9b89-8694x
          \  1/1     Running   0          15s\n\n```\n* Create a `KubeFedWebHook`
          resource to instantiate an admission webhook\ncontroller for KubeFed.\n```\n$
          cat <<-EOF | oc apply -n <namespace> -f -\n---\napiVersion: operator.kubefed.io/v1alpha1\nkind:
          KubeFedWebHook\nmetadata:\n  name: kubefed-webhook-resource\nspec: \n---\nEOF\n```\n*
          Create a `KubeFed` resource to drive the installation of a KubeFed control\nplane
          setting the scope to be either `Namespaced` or `Cluster`. If you're\nplanning
          to federate a cluster-scoped resource type like, for example\n`StorageClass`,
          create this CR with `scope:Cluster`.\n\n```\n$ cat <<-EOF | oc apply -n
          <namespace> -f -\n---\napiVersion: operator.kubefed.io/v1alpha1\nkind: KubeFed\nmetadata:\n
          \ name: kubefed-resource\nspec: \n  scope: Cluster \n---\nEOF\n```\n```\n$
          oc get po -n <namespace> \nNAME                                          READY
          \  STATUS    RESTARTS   AGE\nkubefed-admission-webhook-77f7bd89dd-92kzj
          \   1/1     Running   0          25s\nkubefed-controller-manager-67d5d5cc99-5t2fk
          \  1/1     Running   0          25s\nkubefed-controller-manager-67d5d5cc99-b865z
          \  1/1     Running   0          25s\nkubefed-operator-db58d9b89-8694x              1/1
          \    Running   0          61s\n\n```\n### Installing the KubeFed CLI tool\n\nOnce
          the KubeFed control plane is up and running, a user can download the\n`kubefedctl`
          command-line tool to communicate with it. You can get the\nlatest release
          on\n[GitHub](https://github.com/kubernetes-sigs/kubefed/releases).\n\n```\n
          $ curl -Ls https://github.com/kubernetes-sigs/kubefed/releases/download/v0.1.0-rc6/kubefedctl-0.1.0-rc6-linux-amd64.tgz
          -o kubefedctl-0.1.0-rc6-linux-amd64.tgz\n $ tar -xvzf kubefedctl-0.1.0-rc6-linux-amd64.tgz\n
          $ chmod u+x kubefedctl\n $ sudo mv kubefedctl /usr/local/bin/ # make sure
          the location is in the PATH\n```\n### Joining Clusters\n\nUse the `kubefedctl
          join` command to connect clusters you want KubeFed to manage resources in.
          The `join` operation reads information \nabout how to connect to the joining
          cluster and the cluster hosting federation from your local KUBECONFIG.\n\n```\n$
          kubefedctl join <cluster-name> \\\n    --cluster-context mycluster \\            #
          name of a KUBECONFIG context for the cluster to join (required when the
          context name differs from the cluster name)\n    --host-cluster-context
          mycluster \\       # name of a KUBECONFIG context for the hosting cluster
          (required when the current-context isn't set correctly.)\n    --kubefed-namespace=<namespace>
          \\        # namespace where KubeFed is deployed (required when the kubefed-namespace
          isn't `kube-federation-system`)\n    -v 2\n``` \n**Note**:     \n* If you're
          using an openshift cluster for deploying the KubeFed operator then\n use
          the following command to rename the context to a consumable format.\n \n
          ```\n $ oc config rename-context $(oc config current-context) <cluster-name>\n
          \n ```\n\n `kubefedctl join` creates `KubeFedCluster` resource named `<cluster-name>`
          to represent the joined cluster.\n\n KubeFedCluster resources configure
          the API endpoint of a registered cluster and reference a local secret containing
          \n the authentication token used to connect to that cluster.\n\n  ```\n
          \ $ oc get kubefedclusters -n <namespace>\n    NAME            READY     AGE\n
          \ <cluster-name>    True      7s\n\n  ```\n  ``` \n  $ oc get kubefedclusters
          cluster-name -o yaml                             \n  \n  ```\n  ```                                                           \n
          \   apiVersion: v1\n    items:\n    - apiVersion: core.kubefed.io/v1beta1\n
          \     kind: KubeFedCluster\n      metadata:\n        creationTimestamp:
          \"2019-06-07T19:42:16Z\"\n        generation: 1\n        name: cluster1\n
          \       namespace: <namespace>\n        resourceVersion: \"1055\"\n        selfLink:
          /apis/core.kubefed.io/v1beta1/namespaces/federation-test/kubefedclusters/cluster1\n
          \       uid: 5ac7125d-895c-11e9-b736-0242ac110002\n      spec:\n        apiEndpoint:
          https://172.17.0.2:6443\n        caBundle: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRFNU1EWXdOekU1TXpZek1Gb1hEVEk1TURZd05ERTVNell6TUZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBS0M1Cjk0UDd3d2RaMjBEby8weGJYbVpGKzdweks0cEE5b1ZaVW4yRFNSckhGQldmYThrcnpyVmtkQmdjdnNIa044RGQKTndxdFRXbGd6Y2NMVEZjb1cwSGE3VjRHMWpGY2piU1JkNHhPaWp6SktqMTNCakh3RStJY2R5UnZJbmpaUVlQcgpta1ZUQWRocU1mL3IwY08wSHRTYkJweTJLV3FsRmsyajdTSUhkT3AvK0cwWGU0SS9BQXN3U0xxNTczdXo2V0xkCmFMSFBYRkFyaWh5cFlwWUxXSzZNZWRqUEp5R0MxL3NPMjh5T0RWRTFxSUhRL1Q3TUk1a0YyOGh4cHpCWEhpcE8KMWlYOVJNOGNXdktyVUozeU13NzNpQ3cxR2RhQk1tcDR0ZmpWdzJDQ3IwVU9BZmpUVGYzc2xkeUVJcmJTblUxZApCL0k1M0IrUXhQTHBzY3U4aCtFQ0F3RUFBYU1qTUNFd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFGVVNNcS9DbWlXSnhKNlVIY0RqQVc3eW1rUnMKWWh3VktUa3JDbWJVVDNRNUY5SVVGMDFtZGQ2TTdvWUkyWVJFM2J6MkJudU5jM095eGxKakNqNWRLN2FwaGUwSwpham92b3Y2VVhWd0RBQWFWTTNPb2pMODFRbzhHcStFTVpGQ2NITVlqcHlYUEE2c1duMEZFYnRFTTRKQTBYaXpSClgvdUpISWppOGtFdUlWdmNFZHQzYldaTG9rdTVzZ2RvYnBVYWI0WmMrRVpoNVJLazVwY1ByQnpQaUlUcTF0UzAKYTRlemMyRlFHbUEzNHlwUmxWb25jY2FmVWx4aWUwaklmMVlWdnZJVE41d1dKakc5VVpZdmpseGxNT2NjRzRxSwpscm1Wc2JJcytiQTRIWVFCdDEwOTkwR3VmakYybytLZDgwR3g1NkxGZEJvZzVrclZ3VzdOZUxEM0plOD0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo=\n
          \       secretRef:\n          name: cluster1-jk4hm\n      status:\n        conditions:\n
          \       - lastProbeTime: \"2019-06-07T19:42:32Z\"\n          lastTransitionTime:
          \"2019-06-07T19:42:32Z\"\n          message: /healthz responded with ok\n
          \         reason: ClusterReady\n          status: \"True\"\n          type:
          Ready\n    kind: List\n    metadata:\n      resourceVersion: \"\"\n      selfLink:
          \"\"\n  ```\nFor more detailed information on how to use KubeFed, see [KubeFed
          project documentation](https://github.com/kubernetes-sigs/kubefed/blob/master/docs/userguide.md#user-guide).\n\n###
          Useful links\n* [Kubecon EU-2019](https://www.youtube.com/watch?v=GOiN1R2vQos&t=2461s)\n*
          [Blog](https://blog.openshift.com/federation-v2-is-now-kubefed/)\n\n\n###
          Contribution \nIf you've got some ideas or use cases for KubeFed operator,
          we would love to hear them!\n* Raise issues on [GitHub](https://github.com/openshift/kubefed-operator/issues).\n*
          [Join our community](https://github.com/kubernetes/community/tree/master/sig-multicluster#contact)\n\n###
          License\nKubeFed is licensed under the [Apache License, Version 2.0](https://github.com/kubernetes-sigs/kubefed/blob/master/LICENSE)
          license.\n"
        displayName: Kubefed Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 0.1.0
      name: alpha
    defaultChannel: alpha
    packageName: kubefed
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:48Z"
    labels:
      catalog: community-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: community-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: community
      provider: Crunchy Data
      provider-url: ""
    name: postgresql
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/postgresql
  spec: {}
  status:
    catalogSource: community-operators
    catalogSourceDisplayName: Community Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: postgresoperator.v4.2.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgcluster",
                "metadata": {
                  "labels": {
                    "archive": "false"
                  },
                  "name": "example"
                },
                "spec": {
                  "PrimaryStorage": {
                    "accessmode": "ReadWriteOnce",
                    "size": "1G",
                    "storageclass": "standard",
                    "storagetype": "dynamic"
                  },
                  "ccpimage": "crunchy-postgres-ha",
                  "ccpimagetag": "centos7-12.1-4.2.1",
                  "clustername": "example",
                  "database": "example",
                  "exporterport": "9187",
                  "name": "example",
                  "pgbadgerport": "10000",
                  "port": "5432",
                  "primarysecretname": "example-primaryuser",
                  "rootsecretname": "example-postgresuser",
                  "userlabels": {
                    "archive": "false"
                  },
                  "usersecretname": "example-primaryuser"
                }
              },
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgreplica",
                "metadata": {
                  "name": "example"
                },
                "spec": {},
                "status": {}
              },
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgpolicy",
                "metadata": {
                  "name": "example"
                },
                "spec": {},
                "status": {}
              },
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgtask",
                "metadata": {
                  "name": "example"
                },
                "spec": {}
              },
              {
                "apiVersion": "crunchydata.com/v1",
                "kind": "Pgbackup",
                "metadata": {
                  "name": "example"
                },
                "spec": {},
                "status": {}
              }
            ]
          capabilities: Auto Pilot
          categories: Database
          certified: "false"
          containerImage: crunchydata/postgres-operator:centos7-4.2.1-3
          createdAt: 2019-12-31 19:40Z
          description: Enterprise open source PostgreSQL-as-a-Service
          repository: https://github.com/CrunchyData/postgres-operator
          support: crunchydata.com
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: Represents a Postgres primary cluster member
            displayName: Postgres Primary Cluster Member
            kind: Pgcluster
            name: pgclusters.crunchydata.com
            version: v1
          - description: Represents a Postgres replica cluster member
            displayName: Postgres Replica Cluster Member
            kind: Pgreplica
            name: pgreplicas.crunchydata.com
            version: v1
          - description: Represents a Postgres sql policy
            displayName: Postgres SQL Policy
            kind: Pgpolicy
            name: pgpolicies.crunchydata.com
            version: v1
          - description: Represents a Postgres workflow task
            displayName: Postgres workflow task
            kind: Pgtask
            name: pgtasks.crunchydata.com
            version: v1
          - description: Represents a Postgres backup task
            displayName: Postgres backup task
            kind: Pgbackup
            name: pgbackups.crunchydata.com
            version: v1
        description: |-
          Crunchy PostgreSQL for OpenShift lets you run your own production-grade PostgreSQL-as-a-Service on OpenShift!

          Powered by the Crunchy [PostgreSQL Operator](https://github.com/CrunchyData/postgres-operator), Crunchy PostgreSQL
          for OpenShift automates and simplifies deploying and managing open source PostgreSQL clusters on OpenShift by providing the
          essential features you need to keep your PostgreSQL clusters up and running, including:

          - **PostgreSQL Cluster Provisioning**: [Create, Scale, & Delete PostgreSQL clusters with ease][provisioning],
          while fully customizing your Pods and PostgreSQL configuration!
          - **High-Availability**: Safe, automated failover backed by a [distributed consensus based high-availability solution][high-availability].
          Uses [Pod Anti-Affinity][anti-affinity] to help resiliency; you can configure how aggressive this can be!
          Failed primaries automatically heal, allowing for faster recovery time. You can even create regularly scheduled
          backups as well and set your backup retention policy
          - **Disaster Recovery**: Backups and restores leverage the open source [pgBackRest][] utility
          and [includes support for full, incremental, and differential backups as well as efficient delta restores][disaster-recovery].
          Set how long you want your backups retained for. Works great with very large databases!
          - **Monitoring**: Track the health of your PostgreSQL clusters using the open source [pgMonitor][] library.
          - **Clone**: Create new clusters from your existing clusters with a simple [`pgo clone`][pgo-clone] command.
          - **Full Customizability**: Crunchy PostgreSQL for OpenShift makes it easy to get your own PostgreSQL-as-a-Service up and running on
          and lets make further enhancements to customize your deployments, including:
            - Selecting different storage classes for your primary, replica, and backup storage
            - Select your own container resources class for each PostgreSQL cluster deployment; differentiate between resources applied for primary and replica clusters!
            - Use your own container image repository, including support `imagePullSecrets` and private repositories
            - Bring your own trusted certificate authority (CA) for use with the Operator API server
            - Override your PostgreSQL configuration for each cluster

          and much more!

          [anti-affinity]: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#inter-pod-affinity-and-anti-affinity
          [disaster-recovery]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/disaster-recovery/
          [high-availability]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/high-availability/
          [pgo-clone]: https://access.crunchydata.com/documentation/postgres-operator/latest/pgo-client/reference/pgo_clone/
          [provisioning]: https://access.crunchydata.com/documentation/postgres-operator/latest/architecture/provisioning/

          [pgBackRest]: https://www.pgbackrest.org
          [pgMonitor]: https://github.com/CrunchyData/pgmonitor

          ## Before You Begin

          There are several manual steps that the cluster administrator must perform prior to installing the operator. The
          operator must be provided with an initial configuration to run in the cluster, as well as certificates and
          credentials that need to be generated.

          Start by cloning the operator repository locally.

          ```
          git clone -b v4.2.1 https://github.com/CrunchyData/postgres-operator.git
          cd postgres-operator
          ```

          ### PostgreSQL Operator Configuration

          Edit `conf/postgres-operator/pgo.yaml` to configure the operator deployment. Look over all of the options and make any
          changes necessary for your environment.

          #### Image

          Update the `CCPImageTag` tag to configure the PostgreSQL image being used, updating for the version of PostgreSQL as needed.

          ```
          CCPImageTag:  centos7-12.1-4.2.1
          ```

          #### Storage

          Configure the backend storage for the Persistent Volumes used by each PostgreSQL cluster. Depending on the type of persistent
          storage you wish to make available, adjust the `StorageClass` as necessary. For example, to deploy on AWS using `gp2`, you
          would set the following:

          ```
          storageos:
            AccessMode:  ReadWriteOnce
            Size:  1G
            StorageType:  dynamic
            StorageClass:  gp2
            Fsgroup:  26
          ```

          Once the storage backend is defined, enable the new storage option as needed.

          ```
          PrimaryStorage: storageos
          ReplicaStorage: storageos
          BackrestStorage: storageos
          ```

          ### Certificates

          You will need to either generate new TLS certificates or use existing certificates for the operator API.

          You can generate new self-signed certificates using scripts in the operator repository.

          ```
          export PGOROOT=$(pwd)
          cd $PGOROOT/deploy
          $PGOROOT/deploy/gen-api-keys.sh
          $PGOROOT/deploy/gen-sshd-keys.sh
          cd $PGOROOT
          ```

          ### Configuration and Secrets

          Once the configuration changes have been updated and certificates are in place, we can save the information to the cluster.

          Create the pgo namespace if it does not exist already. This single namespace is where the operator should be deployed to. PostgreSQL clusters will also be deployed here.

          ```
          oc create namespace pgo
          ```

          Create the `pgo-backrest-repo-config` Secret that is used by the operator.

          ```
          oc create secret generic -n pgo pgo-backrest-repo-config \
            --from-file=config=$PGOROOT/conf/pgo-backrest-repo/config \
            --from-file=sshd_config=$PGOROOT/conf/pgo-backrest-repo/sshd_config \
            --from-file=aws-s3-credentials.yaml=$PGOROOT/conf/pgo-backrest-repo/aws-s3-credentials.yaml \
            --from-file=aws-s3-ca.crt=$PGOROOT/conf/pgo-backrest-repo/aws-s3-ca.crt
          ```

          Create the `pgo-auth-secret` Secret that is used by the operator.

          ```
          oc create secret generic -n pgo pgo-auth-secret \
            --from-file=server.crt=$PGOROOT/conf/postgres-operator/server.crt \
            --from-file=server.key=$PGOROOT/conf/postgres-operator/server.key
          ```

          Install the bootstrap credentials:

          ```
          $PGOROOT/deploy/install-bootstrap-creds.sh
          ```

          Install the security context constraint for OpenShift:

          ```
          oc create -f $PGOROOT/deploy/pgo-scc.yaml
          ```

          Remove existing credentials for pgo-apiserver TLS REST API, if they exist.

          ```
          oc delete secret -n pgo tls pgo.tls
          ```

          Create credentials for pgo-apiserver TLS REST API
          ```
          oc create secret -n pgo tls pgo.tls \
            --key=$PGOROOT/conf/postgres-operator/server.key \
            --cert=$PGOROOT/conf/postgres-operator/server.crt
          ```

          Create the `pgo-config` ConfigMap that is used by the operator.

          ```
          oc create configmap -n pgo pgo-config \
            --from-file=$PGOROOT/conf/postgres-operator
          ```

          Once these resources are in place, the operator can be installed into the cluster.

          ## After You Install

          Once the operator is installed in the cluster, you will need to perform several steps to enable usage.

          ### Service

          ```
          oc expose deployment -n pgo postgres-operator --type=LoadBalancer
          ```

          For the pgo client to communicate with the operator, it needs to know where to connect.
          Export the service URL as `PGO_APISERVER_URL` in the shell.

          ```
          export PGO_APISERVER_URL=https://<url of exposed service>:8443
          ```

          ### Security

          When postgres operator deploys, it creates a set of certificates the pgo client will need to communicate.

          ### Client Certificates

          Copy the client certificates from the apiserver to the local environment - we use /tmp for this example.

          ```
          oc cp <pgo-namespace>/<postgres-operator-pod>:/tmp/server.key /tmp/server.key -c apiserver
          oc cp <pgo-namespace>/<postgres-operator-pod>:/tmp/server.crt /tmp/server.crt -c apiserver
          ```

          Configure the shell for the pgo command line to use the certificates

          ```
          export PGO_CA_CERT=/tmp/server.crt
          export PGO_CLIENT_CERT=/tmp/server.crt
          export PGO_CLIENT_KEY=/tmp/server.key
          ```
        displayName: Crunchy PostgreSQL for OpenShift
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: true
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Crunchy Data
        version: 4.2.1
      name: stable
    defaultChannel: stable
    packageName: postgresql
    provider:
      name: Crunchy Data
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:47Z"
    labels:
      catalog: certified-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: certified-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: certified
      provider: Vacava
      provider-url: ""
    name: rapidbiz-operator-certified
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/rapidbiz-operator-certified
  spec: {}
  status:
    catalogSource: certified-operators
    catalogSourceDisplayName: Certified Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: rapidbiz-operator.v0.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                "apiVersion": "api.rapidbiz.com/v1",
                "kind": "Rapidbiz",
                "metadata": {
                  "name": "example-rapidbiz",
                  "labels": {
                    "name": "rapidbiz-operator",
                    "app.kubernetes.io/name": "rapidbiz",
                    "app.kubernetes.io/instance": "rapidbiz-operator",
                    "app.kubernetes.io/component": "server",
                    "app.kubernetes.io/managed-by": "ansible"
                  }
                },
                "spec": {
                  "replicas": 1,
                  "databaseuser": "",
                  "databasepassword": "",
                  "databasehost": "",
                  "databasetype": "MySQL",
                  "databaseport": "3306",
                  "databasename": "",
                  "adminuser": "admin@unknown.com",
                  "adminpassword": "adminpwd",
                  "serverlicense": "",
                  "company": "",
                  "pullpolicy": "IfNotPresent",
                  "imagetag": "latest",
                  "scalestate": false,
                  "scalepodmin": 1,
                  "scalepodmax": 10,
                  "scalepodcpu": 75,
                  "resourcerequirement": {
                    "requests": {
                      "memory": "1000Mi",
                      "cpu": "1000m"
                    },
                    "limits": {
                      "memory": "6400Mi",
                      "cpu": "8"
                    }
                  },
                  "pvc": true,
                  "serviceroute": true
                }
              }
            ]
          capabilities: Full Lifecycle
          categories: Application Runtime, Developer Tools
          certified: Red Hat certified
          containerImage: registry.connect.redhat.com/vacava/rapidbiz-operator:latest
          createdAt: "2020-02-04T16:00:00Z"
          description: RapidBIZ is a simple to use, integrated cloud development and
            delivery environment that enables significant productivity advantages.
          repository: https://access.redhat.com/containers/#/registry.connect.redhat.com/vacava/rapidbiz-operator
          support: Vacava Inc. Mark Plunkett
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: API to create a pod with RapidBIZ running in Open Liberty
              as an application requiring a database.
            displayName: RapidBIZ
            kind: Rapidbiz
            name: rapidbizs.api.rapidbiz.com
            version: v1
        description: |-
          RapidBIZ Ansible Operator OLM is designed to work on Open Shift version 4.2 or greater with Kuberutes version 1.14 or greater.

          RapidBIZ is a simple to use, integrated cloud development, and delivery environment that enables significant productivity and cost advantages - leveraging Docker Images provides enterprise level deployment management.

          Requires a database either external to the cluster or one installed via a database operator or docker image.

          See our website [documention](https://www.vacava.com/rapidbiz/deploy) for more detailed information.
        displayName: RapidBIZ Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Vacava
        version: 0.0.1
      name: alpha
    defaultChannel: alpha
    packageName: rapidbiz-operator-certified
    provider:
      name: Vacava
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: amq7-interconnect-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/amq7-interconnect-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: amq7-interconnect-operator.v1.1.0
      currentCSVDesc:
        annotations:
          alm-examples: '[{"apiVersion":"interconnectedcloud.github.io/v1alpha1","kind":"Interconnect","metadata":{"name":"amq-interconnect"},"spec":{"deploymentPlan":{"size":2,"role":"interior","placement":"Any"}}}]'
          capabilities: Basic Install
          categories: Networking, Streaming & Messaging
          certified: "false"
          containerImage: registry.redhat.io/amq7/amq-interconnect-operator:1.1
          createdAt: "2019-06-28 22:00:00"
          description: Layer 7 Networking
          repository: https://github.com/interconnectedcloud/qdr-operator
          support: Red Hat, Inc.
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: An instance of AMQ Interconnect
            displayName: AMQ Interconnect
            kind: Interconnect
            name: interconnects.interconnectedcloud.github.io
            version: v1alpha1
        description: |2

          AMQ Interconnect is a lightweight [AMQP 1.0](https://www.amqp.org/) message router for building large, highly resilient messaging networks for hybrid cloud and IoT/edge deployments. AMQ Interconnect transparently learns the addresses of messaging endpoints (such as clients, servers, and message brokers) and flexibly routes messages between them.

          ### Core Capabilities

          * High throughput, low latency, shortest path message forwarding based on Layer 7 address routing mechanisms

          * `Interior` mode deployments for any arbitrary topology of geographically-distributed and interconnected routers

          * `Edge` mode deployments for extremely large scale device endpoint connectivity

          * Automatic message traffic rerouting when the network topology changes (resiliency without restrictions)

          * Flexible addressing schemes and delivery semantics (anycast, multicast, closest, balanced)

          * Integrated management with full support for the draft AMQP management specification

          * Full-featured security capabilities for authentication, authorization, and policy-based resource access control

          ### Operator Features

          * **Flexible deployment plans** - Configurable deployment plans are available for `interior` and `edge` mode scenarios. These plans include all dependent resources

          * **Placement directives** - Directives are provided to control how the pods should be scheduled

          * **Connectivity configuration defaults** - Configuration defaults are automatically generated for listeners, connectors, and SSL/TLS setup

          * **Exposes the service** - Integrated management of OpenShift Routes for exposed listener services for client, inter-router, and edge communications

          * **Security certificate management** - Certificates are created and managed through integration with AMQ Certificate Manager

          ### Troubleshooting

          After deploying Interconnect, check any of the following to verify that it is operating correctly:

          * The Interconnect instance

          * The Deployment (or DaemonSet) instance

          * An individual pod for the Deployment (or DaemonSet)

          * A Route created for exposed services

          In addition, use `qdstat` commands to verify connectivity. This can be done using `oc exec` against an interconnect pod, for example `oc exec -it <podname> -- qdstat -c`
          to list connections.
        displayName: AMQ Interconnect
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.1.0
      name: 1.1.0
    defaultChannel: 1.1.0
    packageName: amq7-interconnect-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: cam-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/cam-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: cam-operator.v1.0.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigrationController",
                  "metadata": {
                    "name": "migration-controller",
                    "namespace": "openshift-migration"
                  },
                  "spec": {
                    "azure_resource_group": "",
                    "cluster_name": "host",
                    "migration_velero": true,
                    "migration_controller": true,
                    "migration_ui": true,
                    "olm_managed": true,
                    "restic_timeout": "1h",
                    "mig_pv_limit": "100",
                    "mig_pod_limit": "100",
                    "mig_namespace_limit": "10"
                  }
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "Backup",
                  "metadata": {
                    "name": "backup",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "BackupStorageLocation",
                  "metadata": {
                    "name": "backupstoragelocation",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "DeleteBackupRequest",
                  "metadata": {
                    "name": "deletebackuprequest",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "DownloadRequest",
                  "metadata": {
                    "name": "downloadrequest",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "PodVolumeBackup",
                  "metadata": {
                    "name": "podvolumebackup",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "PodVolumeRestore",
                  "metadata": {
                    "name": "podvolumerestore",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "ResticRepository",
                  "metadata": {
                    "name": "resticrepository",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "Restore",
                  "metadata": {
                    "name": "restore",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "Schedule",
                  "metadata": {
                    "name": "schedule",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "ServerStatusRequest",
                  "metadata": {
                    "name": "serverstatusrequest",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "VolumeSnapshotLocation",
                  "metadata": {
                    "name": "volumesnapshotlocation",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigCluster",
                  "metadata": {
                    "name": "host",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigPlan",
                  "metadata": {
                    "name": "migplan",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigMigration",
                  "metadata": {
                    "name": "migmigration",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigStorage",
                  "metadata": {
                    "name": "migstorage",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional
          certified: "false"
          containerImage: registry.redhat.io/rhcam-1-0/openshift-migration-rhel7-operator@sha256:c27f7293d019a8c033b3944f31e4812ca3eee53bd129561656ba79f557e0b7a8
          createdAt: "2019-07-25T10:21:00Z"
          description: Facilitates migration of workloads from OpenShift 3.x to OpenShift
            4.x
          repository: https://github.com/fusor/mig-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A velero backup storage location
            displayName: BackupStorageLocation
            kind: BackupStorageLocation
            name: backupstoragelocations.velero.io
            version: v1
          - description: A velero backup
            displayName: Backup
            kind: Backup
            name: backups.velero.io
            version: v1
          - description: A request to delete a velero backup
            displayName: DeleteBackupRequest
            kind: DeleteBackupRequest
            name: deletebackuprequests.velero.io
            version: v1
          - description: A download request for velero
            displayName: DownloadRequest
            kind: DownloadRequest
            name: downloadrequests.velero.io
            version: v1
          - description: A cluster defined for migration
            displayName: MigCluster
            kind: MigCluster
            name: migclusters.migration.openshift.io
            version: v1alpha1
          - description: A migration process
            displayName: MigMigration
            kind: MigMigration
            name: migmigrations.migration.openshift.io
            version: v1alpha1
          - description: A migration plan
            displayName: MigPlan
            kind: MigPlan
            name: migplans.migration.openshift.io
            version: v1alpha1
          - description: A migration contoller deployment
            displayName: MigrationController
            kind: MigrationController
            name: migrationcontrollers.migration.openshift.io
            version: v1alpha1
          - description: A migration storage location
            displayName: MigStorage
            kind: MigStorage
            name: migstorages.migration.openshift.io
            version: v1alpha1
          - description: A velero pod volume backup
            displayName: PodVolumeBackup
            kind: PodVolumeBackup
            name: podvolumebackups.velero.io
            version: v1
          - description: A velero pod volume restore
            displayName: PodVolumeRestore
            kind: PodVolumeRestore
            name: podvolumerestores.velero.io
            version: v1
          - description: A restic repository for velero
            displayName: ResticRepository
            kind: ResticRepository
            name: resticrepositories.velero.io
            version: v1
          - description: A velero restore
            displayName: Restore
            kind: Restore
            name: restores.velero.io
            version: v1
          - description: A velero schedule
            displayName: Schedule
            kind: Schedule
            name: schedules.velero.io
            version: v1
          - description: A velero server status request
            displayName: ServerStatusRequest
            kind: ServerStatusRequest
            name: serverstatusrequests.velero.io
            version: v1
          - description: A velero volume snapshot location
            displayName: VolumeSnapshotLocation
            kind: VolumeSnapshotLocation
            name: volumesnapshotlocations.velero.io
            version: v1
        description: |
          The Cluster Application Migration Operator for OpenShift enables installation of the OpenShift application migration tool.
        displayName: Cluster Application Migration Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.0.1
      name: release-v1
    - currentCSV: cam-operator.v1.1.1
      currentCSVDesc:
        annotations:
          alm-examples: |-
            [
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigrationController",
                  "metadata": {
                    "name": "migration-controller",
                    "namespace": "openshift-migration"
                  },
                  "spec": {
                    "azure_resource_group": "",
                    "cluster_name": "host",
                    "migration_velero": true,
                    "migration_controller": true,
                    "migration_ui": true,
                    "olm_managed": true,
                    "restic_timeout": "1h",
                    "mig_pv_limit": "100",
                    "mig_pod_limit": "100",
                    "mig_namespace_limit": "10"
                  }
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "Backup",
                  "metadata": {
                    "name": "backup",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "BackupStorageLocation",
                  "metadata": {
                    "name": "backupstoragelocation",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "DeleteBackupRequest",
                  "metadata": {
                    "name": "deletebackuprequest",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "DownloadRequest",
                  "metadata": {
                    "name": "downloadrequest",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "PodVolumeBackup",
                  "metadata": {
                    "name": "podvolumebackup",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "PodVolumeRestore",
                  "metadata": {
                    "name": "podvolumerestore",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "ResticRepository",
                  "metadata": {
                    "name": "resticrepository",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "Restore",
                  "metadata": {
                    "name": "restore",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "Schedule",
                  "metadata": {
                    "name": "schedule",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "ServerStatusRequest",
                  "metadata": {
                    "name": "serverstatusrequest",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "velero.io/v1",
                  "kind": "VolumeSnapshotLocation",
                  "metadata": {
                    "name": "volumesnapshotlocation",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigCluster",
                  "metadata": {
                    "name": "host",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigPlan",
                  "metadata": {
                    "name": "migplan",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigMigration",
                  "metadata": {
                    "name": "migmigration",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              },
              {
                  "apiVersion": "migration.openshift.io/v1alpha1",
                  "kind": "MigStorage",
                  "metadata": {
                    "name": "migstorage",
                    "namespace": "openshift-migration"
                  },
                  "spec": {}
              }
            ]
          capabilities: Seamless Upgrades
          categories: OpenShift Optional
          certified: "false"
          containerImage: registry.redhat.io/rhcam-1-1/openshift-migration-rhel7-operator@sha256:468a6126f73b1ee12085ca53a312d1f96ef5a2ca03442bcb63724af5e2614e8a
          createdAt: "2019-07-25T10:21:00Z"
          description: Facilitates migration of workloads from OpenShift 3.x to OpenShift
            4.x
          olm.skipRange: '>=0.0.0 <1.1.1'
          repository: https://github.com/fusor/mig-operator
          support: Red Hat
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: A velero backup storage location
            displayName: BackupStorageLocation
            kind: BackupStorageLocation
            name: backupstoragelocations.velero.io
            version: v1
          - description: A velero backup
            displayName: Backup
            kind: Backup
            name: backups.velero.io
            version: v1
          - description: A request to delete a velero backup
            displayName: DeleteBackupRequest
            kind: DeleteBackupRequest
            name: deletebackuprequests.velero.io
            version: v1
          - description: A download request for velero
            displayName: DownloadRequest
            kind: DownloadRequest
            name: downloadrequests.velero.io
            version: v1
          - description: A cluster defined for migration
            displayName: MigCluster
            kind: MigCluster
            name: migclusters.migration.openshift.io
            version: v1alpha1
          - description: A migration process
            displayName: MigMigration
            kind: MigMigration
            name: migmigrations.migration.openshift.io
            version: v1alpha1
          - description: A migration plan
            displayName: MigPlan
            kind: MigPlan
            name: migplans.migration.openshift.io
            version: v1alpha1
          - description: A migration contoller deployment
            displayName: MigrationController
            kind: MigrationController
            name: migrationcontrollers.migration.openshift.io
            version: v1alpha1
          - description: A migration storage location
            displayName: MigStorage
            kind: MigStorage
            name: migstorages.migration.openshift.io
            version: v1alpha1
          - description: A velero pod volume backup
            displayName: PodVolumeBackup
            kind: PodVolumeBackup
            name: podvolumebackups.velero.io
            version: v1
          - description: A velero pod volume restore
            displayName: PodVolumeRestore
            kind: PodVolumeRestore
            name: podvolumerestores.velero.io
            version: v1
          - description: A restic repository for velero
            displayName: ResticRepository
            kind: ResticRepository
            name: resticrepositories.velero.io
            version: v1
          - description: A velero restore
            displayName: Restore
            kind: Restore
            name: restores.velero.io
            version: v1
          - description: A velero schedule
            displayName: Schedule
            kind: Schedule
            name: schedules.velero.io
            version: v1
          - description: A velero server status request
            displayName: ServerStatusRequest
            kind: ServerStatusRequest
            name: serverstatusrequests.velero.io
            version: v1
          - description: A velero volume snapshot location
            displayName: VolumeSnapshotLocation
            kind: VolumeSnapshotLocation
            name: volumesnapshotlocations.velero.io
            version: v1
        description: |
          The Cluster Application Migration Operator for OpenShift enables installation of the OpenShift application migration tool.
        displayName: Cluster Application Migration Operator
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: false
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: false
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 1.1.1
      name: release-v1.1
    defaultChannel: release-v1.1
    packageName: cam-operator
    provider:
      name: Red Hat
- apiVersion: packages.operators.coreos.com/v1
  kind: PackageManifest
  metadata:
    creationTimestamp: "2020-02-16T21:47:45Z"
    labels:
      catalog: redhat-operators
      catalog-namespace: openshift-marketplace
      olm-visibility: hidden
      openshift-marketplace: "true"
      opsrc-datastore: "true"
      opsrc-owner-name: redhat-operators
      opsrc-owner-namespace: openshift-marketplace
      opsrc-provider: redhat
      provider: Red Hat
      provider-url: ""
    name: ocs-operator
    namespace: default
    selfLink: /apis/packages.operators.coreos.com/v1/namespaces/default/packagemanifests/ocs-operator
  spec: {}
  status:
    catalogSource: redhat-operators
    catalogSourceDisplayName: Red Hat Operators
    catalogSourceNamespace: openshift-marketplace
    catalogSourcePublisher: Red Hat
    channels:
    - currentCSV: ocs-operator.v4.2.1
      currentCSVDesc:
        annotations:
          alm-examples: |2-

            [
                {
                    "apiVersion": "ocs.openshift.io/v1",
                    "kind": "StorageCluster",
                    "metadata": {
                        "name": "example-storagecluster",
                        "namespace": "openshift-storage"
                    },
                    "spec": {
                        "manageNodes": false,
                        "monPVCTemplate": {
                            "spec": {
                                "accessModes": [
                                    "ReadWriteOnce"
                                ],
                                "resources": {
                                    "requests": {
                                        "storage": "10Gi"
                                    }
                                },
                                "storageClassName": "gp2"
                            }
                        },
                        "storageDeviceSets": [
                            {
                                "count": 3,
                                "dataPVCTemplate": {
                                    "spec": {
                                        "accessModes": [
                                            "ReadWriteOnce"
                                        ],
                                        "resources": {
                                            "requests": {
                                                "storage": "1Ti"
                                            }
                                        },
                                        "storageClassName": "gp2",
                                        "volumeMode": "Block"
                                    }
                                },
                                "name": "example-deviceset",
                                "placement": {},
                                "portable": true,
                                "resources": {}
                            }
                        ]
                    }
                },
                {
                    "apiVersion": "ocs.openshift.io/v1",
                    "kind": "OCSInitialization",
                    "metadata": {
                        "name": "example-ocsinitialization"
                    },
                    "spec": {}
                },
                {
                    "apiVersion": "ocs.openshift.io/v1",
                    "kind": "StorageClusterInitialization",
                    "metadata": {
                        "name": "example-storageclusterinitialization"
                    },
                    "spec": {}
                }
            ]
          capabilities: Full Lifecycle
          categories: Storage
          olm.skipRange: '>=4.2.0 <4.2.1'
        apiservicedefinitions: {}
        customresourcedefinitions:
          owned:
          - description: |-
              [This resource is not intended to be created or managed by users.]


              Represents a Ceph cluster.
            displayName: '[Internal] Ceph Cluster'
            kind: CephCluster
            name: cephclusters.ceph.rook.io
            version: v1
          - description: |-
              [This resource is not intended to be created or managed by users.]


              Represents a Ceph Block Pool.
            displayName: '[Internal] Ceph Block Pool'
            kind: CephBlockPool
            name: cephblockpools.ceph.rook.io
            version: v1
          - description: |-
              [This resource is not intended to be created or managed by users.]


              Represents a Ceph Object Store.
            displayName: '[Internal] Ceph Object Store'
            kind: CephObjectStore
            name: cephobjectstores.ceph.rook.io
            version: v1
          - description: |-
              [This resource is not intended to be created or managed by users.]


              Represents a Ceph Object Store User.
            displayName: '[Internal] Ceph Object Store User'
            kind: CephObjectStoreUser
            name: cephobjectstoreusers.ceph.rook.io
            version: v1
          - description: |-
              [This resource is not intended to be created or managed by users.]


              Represents a cluster of Ceph NFS ganesha gateways.
            displayName: '[Internal] Ceph NFS'
            kind: CephNFS
            name: cephnfses.ceph.rook.io
            version: v1
          - description: |-
              [This resource is not intended to be created or managed by users.]


              A NooBaa system - Create this to start
            displayName: '[Internal] NooBaa'
            kind: NooBaa
            name: noobaas.noobaa.io
            version: v1alpha1
          - description: Storage target spec such as aws-s3, s3-compatible, PV's and
              more. Used in BacketClass to construct data placement policies.
            displayName: BackingStore
            kind: BackingStore
            name: backingstores.noobaa.io
            version: v1alpha1
          - description: Storage policy spec  tiering, mirroring, spreading. Combines
              BackingStores. Referenced by ObjectBucketClaims.
            displayName: BucketClass
            kind: BucketClass
            name: bucketclasses.noobaa.io
            version: v1alpha1
          - description: |-
              [This resource is not intended to be created or managed by users.]


              OCS Initialization represents the initial data to be created when the OCS operator is installed.
            displayName: '[Internal] OCS Initialization'
            kind: OCSInitialization
            name: ocsinitializations.ocs.openshift.io
            version: v1
          - description: Storage Cluster represents a Openshift Container Storage
              Cluster including Ceph Cluster, NooBaa and all the storage and compute
              resources required.
            displayName: Storage Cluster
            kind: StorageCluster
            name: storageclusters.ocs.openshift.io
            version: v1
          - description: |-
              [This resource is not intended to be created or managed by users.]


              StorageCluster Initialization represents a set of tasks the OCS operator wants to implement for every StorageCluster it encounters.
            displayName: '[Internal] StorageCluster Initialization'
            kind: StorageClusterInitialization
            name: storageclusterinitializations.ocs.openshift.io
            version: v1
          required:
          - description: Claim a bucket just like claiming a PV. Automate you app
              bucket provisioning by creating OBC with your app deployment. A secret
              and configmap (name=claim) will be created with access details for the
              app pods.
            displayName: ObjectBucketClaim
            kind: ObjectBucketClaim
            name: objectbucketclaims.objectbucket.io
            version: v1alpha1
          - description: Used under-the-hood. Created per ObjectBucketClaim and keeps
              provisioning information.
            displayName: ObjectBucket
            kind: ObjectBucket
            name: objectbuckets.objectbucket.io
            version: v1alpha1
        description: |2

          Red Hat OpenShift Container Storage provides hyperconverged storage for applications within an OpenShift cluster.

          ## Components

          OpenShift Container Storage deploys three operators.

          ### OpenShift Container Storage operator

          The OpenShift Container Storage operator is the primary operator for OpenShift Container Storage. It serves to facilitate the other operators in OpenShift Container Storage by performing administrative tasks outside their scope as well as watching and configuring their CustomResources.

          ### Rook

          [Rook][1] deploys and manages Ceph on OpenShift, which provides block and file storage.

          ### NooBaa operator

          The NooBaa operator deploys and manages the [NooBaa][2] Multi-Cloud Gateway on OpenShift, which provides object storage.

          ## Before Subscription

          Before subscribing to OpenShift Container Storage, there are two pre-requisites that need to be satisfied.

          ### Namespace

          OpenShift Container Storage runs only in the openshift-storage namespace, which needs to be created before subscription. The following manifest can be used to create the namespace.

          ```
          apiVersion: v1
          kind: Namespace
          metadata:
            labels:
              openshift.io/cluster-monitoring: "true"
            name: openshift-storage
          spec: {}
          ```

          Save the above as rhocs-namespace.yaml, and create the Namespace with,

          ```
          $ oc create -f rhocs-namespace.yaml
          ```

          ### OperatorGroup
          An OperatorGroup targetting the openshift-storage namespace also needs to be created. The following manifest can be used to create the OperatorGroup.

          ```
          apiVersion: operators.coreos.com/v1
          kind: OperatorGroup
          metadata:
            name: openshift-storage-operatorgroup
            namespace: openshift-storage
          spec:
            serviceAccount:
              metadata:
                creationTimestamp: null
            targetNamespaces:
            - openshift-storage
          ```

          Save the above as rhocs-operatorgroup.yaml, and create the OperatorGroup with,

          ```
          $ oc create -f rhocs-operatorgroup.yaml
          ```

          ## After subscription

          After the three operators have been deployed into the openshift-storage namespace, a StorageCluster can be created. Note that the StorageCluster resource is the only resource that a user should be creating. OpenShift Container Storage includes many other custom resources which are internal and not meant for direct usage by users.

          [1]: https://rook.io
          [2]: https://noobaa.io
        displayName: OpenShift Container Storage
        installModes:
        - supported: true
          type: OwnNamespace
        - supported: true
          type: SingleNamespace
        - supported: false
          type: MultiNamespace
        - supported: true
          type: AllNamespaces
        provider:
          name: Red Hat
        version: 4.2.1
      name: stable-4.2
    defaultChannel: stable-4.2
    packageName: ocs-operator
    provider:
      name: Red Hat
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""
